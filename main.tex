%We want to remove gamma \leq 1/L_F condition to 2/L_F. 
%Comparisons for linear case or with 2 steps method case. We demonstrate the new method is better in the convergence. 

\documentclass{article}
\newcommand{\Reals}[1]{{\rm I\! R}^{#1}}
\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage{amsmath,amsfonts}
\usepackage{graphicx} 
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{wrapfig}
\usepackage{algpseudocode}
\usepackage{algorithm,bm}
\usepackage{subeqnarray} 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm, amssymb, amsfonts, amscd, latexsym}
\usepackage{changepage}
\usepackage{txfonts}
\usepackage{textcomp,marvosym}
\usepackage{dsfont}
\usepackage{graphicx}
\graphicspath{{./Figs/}}
\usepackage{color}
\usepackage{kotex}
\usepackage[right]{lineno}
\usepackage{microtype}
\usepackage{tabu,tabularx,multirow,booktabs}
\usepackage{cuted}
\usepackage[capitalize, nameinlink]{cleveref}
\usepackage{stackengine}
\usepackage{subeqnarray} 
\usepackage{soul}
\stackMath
\newcommand\tenq[2][1]{%
 \def\useanchorwidth{T}%
  \ifnum#1>1%
    \stackunder[0pt]{\tenq[\numexpr#1-1\relax]{#2}}{\scriptscriptstyle\sim}%
  \else%
    \stackunder[1pt]{#2}{\scriptscriptstyle\sim}%
  \fi%
}
\usepackage{titlesec}
\usepackage{lscape}
\usepackage{indentfirst}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage{caption}
 \usepackage{cancel}
\usepackage{subcaption}
\usepackage{algorithm,algpseudocode}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{amsmath}
\usepackage{xcolor}
\def\tbar{|\hspace*{-0.15em}|\hspace*{-0.15em}|}
\def\Ahatstar{{}^{*}\!\!\hat{A}}
\def\bold#1{{\bf #1}}
\def\text#1{{\rm #1}}
\def\calg#1{{\cal #1}}
\def\bbbb#1{{\mathbb #1}}
\def\PLTMG{{\sc PLTMG}}
\def\MC{{\sc MC}}
\def\MCX{{\sc MCX}}
\def\MALOC{{\sc MALOC}}
\def\SG{{\sc SG}}
\def\MCLAB{{\sc MCLab}}
\def\APBS{{\sc APBS}}
\def\FETK{{\sc FE}{\small tk}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\Lapse}{N}                               %Lapse function
\newcommand{\hme}{\hat g}                           %background metric
\newcommand{\hnabla}{\hat \nabla}                %background covariant derivative
\newcommand{\hGamma}{\hat \Gamma}        %background Christoffel
\newcommand{\Shift}{X}                                  %Shift Vectorfield
\newcommand{\Lie}{{\mathcal L}}                   %Lie derivative
\newcommand{\del}{\delta}                             %gauge quantity
\newcommand{\tr}{{\text{\rm tr}}}                     %trace
\newcommand{\jump}[1]{\ensuremath{[\![#1]\!]} } % jump command

\newtheorem{lemma}{Lemma} 
\newtheorem{remark}{Remark} 
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
%\newtheorem*{definition}{Definition}
\newcommand{\Label}[1]{\label{#1}\mbox{\fbox{\bf #1}\quad}}
\renewcommand{\Label}[1]{\label{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
\newcommand{\ott}[1]{\underset{\widetilde{\widetilde{}}}{#1}}
\newcommand{\ot}[1]{\underset{\widetilde{}}{#1}}
\newcommand{\sss}{\ott S}
\newcommand{\pps}{\ot T^*}
\newcommand{\pp}{\ot{T}}
\newcommand{\vu}{{\bf{u}}}
\newcommand{\vv}{{\bf{v}}}
\newcommand{\vn}{{\bf{n}}}
\newcommand{\vV}{{\bf{V}}}
\newcommand{\vF}{{\bf{F}}}
\newcommand{\vS}{{\bf{S}}}
\newcommand{\vE}{{\bf{E}}}
\newcommand{\vB}{{\bf{B}}}
\newcommand{\vA}{{\bf{A}}}
%\newcommand{\vH}{{\bf{H}}}
\newcommand{\vL}{{\bf{L}}}
\newcommand{\vf}{{\bf{f}}}
\newcommand{\vg}{{\bf{g}}}
%\newcommand{\vh}{{\bf{h}}}
\newcommand{\vI}{{\bf{I}}}
\newcommand{\vC}{{\bf{C}}}
\newcommand{\mC}{\mathcal{C}}
\newcommand{\vq}{\bf{q}}
\newcommand{\vr}{\bf{r}}
\newcommand{\vb}{\bf{b}}
\newcommand{\vX}{{X}}
\newcommand{\vx}{{\bf{x}}}
\newcommand{\vp}{{\phi}}
\theoremstyle{definition}
\newtheorem*{definition}{Definition} 

\usepackage[rightcaption]{sidecap}
\def\R{\mathbb{R}}
\newtheorem{thm}{Theorem}[section]
  
\newtheorem{prop}[thm]{Proposition}

\newtheorem{defn}[thm]{Definition}

\newtheorem{cor}[thm]{Corollary}

\newtheorem{assump}[thm]{Assumption}
\usepackage{lineno}
\linenumbers

\title{On the linear convergence analysis of the Determinstic Federated Learning algorithm} 
%\author{Authors} 
\date{January 24, 2023} 

\begin{document}

\maketitle
\begin{abstract}
In this note, we discuss a mathematical proof for the linear convergence of the Deterministic Federated Learning algorithm for the quadratic objective functionals, under appropriately chosen parameters. Our analytical tool is to recast the algorithm as the inexact Uzawa framework and use an appropriate norm for obtaining the linear convergence rate. We provide the linear convergence for federated learning algorithm, which is based on both exact local solve, i.e., Gauss-Seidel method, and inexact local solve, i.e., $n-$step Gradient descent methods. Our discussion is in the following order 
\begin{itemize}
    \item Convergence analysis for the quadratic functional $F(X) = \frac{1}{2} X^T A X - g^T X.$ 
    \item Convergence analysis for nonlinear convex functional $F$ 
    \item Still unresolved open problem and outlook. 
\end{itemize}
\end{abstract}

\tableofcontents 

\section{Introduction}

Data becomes increasingly decentralized and the privacy of individual data is an utmost importance in the digital age \cite{house2012consumer, cai2021deepstroke,chen2020ai,luo2020arbee,wang2020panel}. Unlike standard machine learning approaches, \textit{Federated learning} (FL) encourages each client to have a local training data set, which will not be stored to the server and  to update the local correction of the current global model maintained by the main server via the local data and local gradient descent method. Federated learning has been used successfully in many different areas, which include Internet of Things (IoT) applications \cite{hwang2015iot, ferrag2021federated}. Federated learning can be formulated as the optimization problem with a certain consensus constraint of distributed local optimization problem as discussed in \cite{mishchenko2022proxskip}. More precisely, let $N$ be the number of local clusters or clients, then FL can be given as follows:  
\begin{equation}\label{main:prob} 
\min_{ \substack{ X \in \Reals{N_x} \\X^1 = X^2 =\cdots = X^{N} } } \left \{ F(X) := \frac{1}{N} \sum_{i=1}^{N} f_i(X^i) \right \}, \qquad  X = (X^1,\cdots,X^{N})^T, 
\end{equation}
where $N_x = N \times d$ and $f_i : \Reals{d} \rightarrow \Reals{}$ is the objective functional obtained based on the data owned on $i^{\rm th}$ client for 
$i \in [N] := \{1,2,\cdots,N\}$. The problem is stated in the consensus formulation. Namely, the parameter $X$ stores the cloned parameters $X^k \in \Reals{d}$ for each client, $k=1,\cdots,N$ such that $X^1 = \cdots = X^N$ or the problem \eqref{main:prob} obtains the consensus, namely, the optimizer denoted by $X_* \in \Reals{N_x}$ such that $X_* = (z_*,\cdots,z_*)^T$ for some $z_* \in \Reals{d}$. Note that the local objective function $f_k$ depends on the data of the $k^{\rm th}$ worker, but not on those of the other clients. The standard Federated Avgerage ({\textit{FebAvg}}) algorithm consists of the following three steps: 
\begin{enumerate}
\item  the central server broadcasts the latest model $X_k$, to all the clients;
\item every worker, say $i^{\rm th}$ worker, lets $X_k^i = X_k$ and then performs one or few local updates with learning rate $\gamma$ 
\begin{equation}\label{gdstep2} 
X_{k+1}^i \leftarrow X_k^i - \gamma \nabla f_i(X_k^i),   
\end{equation}
\item the server then aggregates the local models, $X_{k+1}^1, \cdots X_{k+1}^{N}$, to produce the new global model $X_{k+1}$ \cite{konevcny2016federated}.
\end{enumerate}
The main bottleneck in the Federated learning algorithm lies in step 3, which is generally orders of magnitude more expensive than the local computations. Furthermore, communications make the algorithm vulnerable to cybersecurity. Recent methods, therefore, aim at enhancing the privacy of FL by using a reduced model or even sacrificing system efficiency. However, providing privacy has to be carefully balanced with system efficiency \cite{li2020federated}. One recent algorithm, called Scaffnew or ProxSkip, is shown to achieve the best communication efficiency, without sacrificing the convergence property, until today \cite{mishchenko2022proxskip}. Among others, Scaffnew reformulates the step 2, \eqref{gdstep2} as follows: for $\ell = 0, 1,\cdots, n-1$, 
\begin{equation}\label{main:scaffnew}
X_{k+\frac{\ell+1}{n}}^i \leftarrow X_{k+\frac{\ell}{n}}^i - \gamma \left (\nabla f_i \left ( X_{k+\frac{\ell}{n}}^i \right ) - H_k^i \right ),   
\end{equation} 
where a certain shift, $H_k^i$ is introduced for each client, and finite steps of the gradient descent method are used. Let $K = {\textbf{1}} \otimes I \in \Reals{N_x \times d}$, where $I$ is the $d\times d$ identity matrix and $H_k = (H_k^1,\cdots,H_k^{N})^T$, the complete algorithm is then given as in the following Algorithm \ref{alg:SCAFFOLD}. Note that ProxSkip introduces the probability $p$ to choose to apply the local step. Thus, the Determinstic ProxSkip can be understood that $p = 1/n$, where $n$ is the number of GD steps. This is summarized as the following algorithm \ref{alg:SCAFFOLD}. Note that the speed can be optimized if $p = 1/\sqrt{\kappa}$, where $\kappa$ is the condition number of $F$. 

\begin{algorithm}[H]
\caption{Deterministic ProxSkip or SCAFFOLD \cite{karimireddy2020scaffold}}\label{alg:SCAFFOLD}
Given a stepsize $\gamma > 0$, $n$, the number of GD steps, a stepsize for $H$ update, 
$\omega = \frac{1}{n \gamma}$, initial iterate $X_0 = (X^1_0, \dots, X^{N}_0)$, we perform the following until the convergence:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots$}
    \State{$X_k = Z_k$}
    \For{$\ell = 0, 1,\dots, n-1$}
    \State{$X_{k + \frac{\ell+1}{n}} = X_{k + \frac{\ell}{n}} - \gamma ( \nabla F(X_{k + \frac{\ell}{n}} ) - H_k)$ }
    \EndFor    
    \State{$Z_{k+1} = Kz_{k+1}, \mbox{ with } z_{k+1} = \frac{1}{N}\sum_{i = 1}^{N} X^i_{k+1}$} 
    %, \cdots, \sum_{i = 1}^{N_c} X^i_{t+1})$}
    \State{$H_{k+1} = H_k + \frac{1}{n \gamma }(Z_{k+1} - X_{k+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}
This method is demonstrated to be convergent and popularly used in the Federated learning community. However, its convergence analysis is open until today. Note that the stochastic version of Federated algorithm, for example, ProxSkip algorithm is shown to converge linearly in \cite{mishchenko2022proxskip}. The main result in this paper is to provide a mathematical proof of the linear convergence of the Algorithm \ref{alg:SCAFFOLD}. Furthermore, we shall also establish the linear convergence when the multiple GD scheme is replaced by exact solver. The rest of the paper is organized as follows: \S \ref{prob} discusses the formulation of the Federated learning algorithm and a couple of discussion on convex functional. 

Throughout the paper, we shall denote $\|\cdot\|$ and $\langle\cdot,\cdot\rangle$ by the standard Euclidean norm and inner product, respectively. In particular, we shall use for $\alpha > 0$, the following scaled norm: 
\begin{equation} 
\|x\|_{\alpha} = \alpha \|x\|. 
\end{equation}
We also use the standard tensor product $\otimes$. For any convex functional $F$, which is twice continuously differential, the Hessian of $F$ can be well-defined and it will be denoted by $\mathcal{H}_F$. By $\sigma(\mathcal{H}_F)$, we mean the spectrum of $\mathcal{H}_F$. We shall also denote $\rho(A)$ by the spectral radius of the matrix $A$. The notation ${\rm{Null}}(A)$ denotes the null space of the matrix $A$. We shall denote $\kappa(A)$ by the condition number of $A$.

%\end{document} 

\begin{comment}
\subsection{Scaffold}
Using our notation, we now introduce the ProxSkip algorithm. See algorithm~\ref{alg:ProxSkip}. 

\begin{algorithm}[H]
\caption{ProxSkip}\label{alg:ProxSkip}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, $h_0$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_{t+1} = Z_{t} - \gamma (\nabla F(Z_{t}) -h_t ) $}
    \State{Flip a coin $\theta_t$, $P(\theta_t = 1) = p $}
    \If{$\theta_t = 1$} 
    \State{$Z_{t+1} = {\rm prox}_{\frac{\gamma}{p}\psi } 
    \left ( X_{t+1} - \frac{\gamma}{p} h_t \right )$} 
    \Else
        \State{$Z_{t+1} = X_{t+1}$}
    \EndIf 
    \State{$h_{t+1} = h_t + \frac{p}{\gamma} (Z_{t+1} - X_{t+1})$} 
\EndFor
\end{algorithmic}
\end{algorithm}

A deterministic version of ProxSkip is given in algorithm \ref{alg:ProxSkip deterministic}. 

\begin{algorithm}[H]
\caption{ProxSkip Deterministic (SCAFFOLD)}\label{alg:ProxSkip deterministic}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_t = Z_t$}
    \For{$k = 0, 1,\dots, N-1$}
    \State{$X_{t + \frac{k+1}{N}} = X_{t + \frac{k}{N}} - \gamma ( \nabla F(X_{t + \frac{k}{N}} ) - H_t)$ }
    % \State{$Z_{t + \frac{k+1}{N}} =X_{t + \frac{k+1}{N}} $}
    \EndFor    
    \State{$Z_{t+1} = \text{prox}_{N\gamma \psi}(X_{t+1} - N\gamma H_t)$}
    \State{$H_{t+1} = H_t + \frac{1}{N \gamma }(Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}
\end{comment} 

\section{Federated Learning for $F$ being a quadratic functional}\label{prob}  

In this section, we describe the federated learning algorithm for the case when the objective functional is quadratic and thus, the Hessian exists and it is a constant matrix, namely, 
\begin{equation}
F(X) = \frac{1}{2} X^T AX - X^T g,
\end{equation}
where $g$ is given. More precisely, with the following notation:
\begin{equation}
K = \textbf{1} \otimes I, \quad \mathcal{A} = \begin{pmatrix} 
A & 0 \\ 
0 & 0 
\end{pmatrix} 
,\quad \mathcal{B} = \begin{pmatrix} -I & K \end{pmatrix}, 
\quad f = \begin{pmatrix} g \\ 0 \end{pmatrix}, \quad \mbox{ and } \quad U = \begin{pmatrix} X \\ z \end{pmatrix}. 
\end{equation}
The consensus optimization problem \eqref{main:prob} can be interpreted to find $(X_*,z_*,H_*)$ such that the following equation holds: 
\begin{subeqnarray}
(\mathcal{A} + r \mathcal{B}^T \mathcal{B}) U_* + \mathcal{B}^T H_* &=& f \\ 
\mathcal{B} U_* &=& 0,
\end{subeqnarray}
where $r$ is the penalty parameter. We shall denote $\mathcal{A}_r = \mathcal{A} + r \mathcal{B}^T \mathcal{B}$ and $A_r = A + rI$. With this notation, the full matrix system can be written as follows: 
\begin{equation}
\begin{pmatrix}
A_r & - r K & -I \\
-r K^T& r K^TK & K^T\\
-I& K & 0\\
\end{pmatrix}
\begin{pmatrix}
X_*\\
z_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
g \\
0 \\
0
\end{pmatrix}.
\end{equation}
Furthermore, the consensus algorithm can be read as follows: 
\begin{algorithm}
\caption{Augmented Lagrangian Uzawa formulation of FL}\label{algADMM1} 
Given $H_0$ such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,K-1$}
    \State{$X_{k+1}$ update:),  
    \begin{equation} \label{Xupdatexx}
    X_{k+1} = G(H_k + r Kz_k),
\end{equation} }
    \State{$z_{t+1}$ update: 
        \begin{equation} \label{zupdatexx}
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} \label{Hupdate2xx}
        H_{k+1} = H_k + \omega (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}
We also note that for $r \rightarrow 0$, we arrive at the following system: 
\begin{subeqnarray}
\mathcal{A} U_* + \mathcal{B}^T H_* &=& f, \\ 
\mathcal{B} U_* &=& 0.
\end{subeqnarray}
which can be written as the following system. 
\begin{equation}
\begin{pmatrix}
A & 0 & -I \\ 
0 & 0 & K^T \\
-I & K & 0 
\end{pmatrix} 
\begin{pmatrix}
X_* \\ 
z_*  \\ 
H_* 
\end{pmatrix} 
= \begin{pmatrix}
g \\ 
0 \\ 
0 
\end{pmatrix}.
\end{equation} 
However, we shall assume $r > 0$ throughout the paper. Our discussion shall investigate the convergence of Augmented Lagrangian Uzawa when the block $U$ is solved exactly to obtain some insight about the problem and then, we discuss the inexact solver for the block $U$, which will correspond to the Federated Learning Algorithm. 

\subsection{Convergence of Augmented Lagrangian Uzawa method for Exact solver for $U$ block} 

In this section, we shall discuss the convergence of the Augmented Lagrangian Uzawa method. First, we observe that by removing $U$, we get the following system for $H$: 
\begin{equation}
\mathcal{S}_r H_* = \mathcal{B} \mathcal{A}_r^{-1} \mathcal{B}^T H_* = \mathcal{B} \mathcal{A}_r^{-1} f. 
\end{equation} 
Therefore, the system we want to solve can be given as follows: 
\begin{subeqnarray}
(\mathcal{A} + r \mathcal{B}^T \mathcal{B}) U_* + \mathcal{B}^T H_* &=& f \\ 
\mathcal{S}_r H_* &=& \mathcal{B} \mathcal{A}_r^{-1} f. 
\end{subeqnarray}
We can apply the Richardson method to update $H$, whose parameter will be denoted by $\omega$. Thus, we arrive at the following Augmented Lagrangian Uzawa method: 
\begin{subeqnarray*} 
(\mathcal{A} + r \mathcal{B}^T \mathcal{B} ) U_{k+1} &=& f - \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + \omega ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + \omega \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
This is summarized as an algorithm. 

\begin{algorithm}\label{aglu}
\begin{algorithmic}
\For{$k=0, 1,2,\cdots $}
\State{Update of $U_{k+1}$: 
\begin{equation}\label{Srsolv}
(\mathcal{A} + r \mathcal{B}^T \mathcal{B} ) U_{k+1} = f - \mathcal{B}^T H_k 
\end{equation}}
\State{Update of $H_{k+1}$: 
\begin{equation} 
H_{k+1} = H_k + \omega \mathcal{B} U_{k+1}.  
\end{equation}}
\EndFor
\end{algorithmic}\caption{Augmented Lagrangian Uzawa}\label{algo1}
\end{algorithm}

\begin{theorem}
Let $\sigma(A) \in [\lambda_F, L_F]$. Then, the Algorithm \ref{algo1} converges with the convergence rate given as follows:
\begin{enumerate}
\item For $0 < \omega < \frac{2}{\rho(\mathcal{S}_r)}$, we have the following convergence:
\begin{equation}
\|H_* - H_k\| \leq \rho(I - \omega \mathcal{S}_r)^k \|H_* - H_0\|. 
\end{equation}
Furthermore, we have that 
\begin{equation}
\|X_* - X_k\|_A = |U_* - U_k|_{\mathcal{A}} \leq \sqrt{\frac{1}{r}}\rho(I - \omega \mathcal{S}_r)^k \|H_* - H_0\|. 
\end{equation} 
\item For $\omega = r$, we have the convergence rate given as follows: 
\begin{equation}
\|H_* - H_k\| \leq \left ( \frac{1}{1 + r/L_F}\right )^k \|H_* - H_0\| 
\end{equation} 
and 
\begin{equation}
\|X_* - X_k\|_A = |U_* - U_k|_{\mathcal{A}} \leq \sqrt{1/r} \left ( \frac{1}{1 + r/ L_F}\right )^k \|H_* - H_0\|,  
\end{equation} 
where $\mu_0 = 1/L_F$ is the smallest eigenvalue of $\mathcal{B} \mathcal{A}^{\dag} \mathcal{B}^T$.
\item For $\omega = \frac{2}{ \frac{\lambda_F}{1 + \lambda_F r} + \frac{L_F}{1 + L_F r}}$, we have 
\begin{equation}
\|H_* - H_k\| \leq \left ( \frac{L_F - \lambda_F}{L_F + \lambda_F + 2L_F \lambda_F r} \right )^k \|H_* - H_0\|. 
\end{equation}
Furthermore, we have that 
\begin{equation}
\|X_* - X_k\|_A = |U_* - U_k|_{\mathcal{A}} \leq \sqrt{1/r} \left ( \frac{L_F - \lambda_F}{L_F + \lambda_F + 2L_F \lambda_F r} \right )^k \|H_* - H_0\|. 
\end{equation} 
\end{enumerate} 
\end{theorem} 
\begin{proof}
The convergence of the Augmented Lagrangian Uzawa relies on the spectrum of the Schur complement operator. This leads to the choice of paramters $\omega$. The Schur compliment operator is given as follows: 
\begin{equation}
\mathcal{S}_r = \mathcal{B}( \mathcal{A} + r \mathcal{B}^T \mathcal{B})^{-1} \mathcal{B}^T. 
\end{equation} 
We note that 
\begin{equation}
{\rm Null}(\mathcal{A}) \cap {\rm Null}(\mathcal{B}) = \{ 0\}. 
\end{equation}
Further, we can show that $\mathcal{B} \mathcal{A}^\dag \mathcal{B}^T$ is symmetric positive definite. We note that 
\begin{equation}
\mathcal{A}^\dag = \begin{pmatrix} A^{-1} & 0 \\ 0 & 0 \end{pmatrix} 
\end{equation} 
and thus 
\begin{equation}
\mathcal{B}\mathcal{A}^\dag \mathcal{B}^T = \begin{pmatrix} -I & K \end{pmatrix}  \begin{pmatrix} A^{-1} & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} -I \\ K^T \end{pmatrix} = \begin{pmatrix} -I & K \end{pmatrix} \begin{pmatrix} -A^{-1} \\ 0 \end{pmatrix} = A^{-1}.
\end{equation} 
This means, that 
\begin{equation}
\frac{1}{L_F} I \leq \mathcal{B} \mathcal{A}^\dag \mathcal{B}^T \leq \frac{1}{\lambda_F} I.   
\end{equation} 
By applying the Sherman-Morrison-Woodbury formula, we have  
\begin{equation}
\mathcal{S}_r^{-1} = rI + (\mathcal{B} \mathcal{A}^\dag \mathcal{B}^T)^{-1} = rI + A = A_r.    
\end{equation}
Thus, the spectrum of $\mathcal{S}_r$ is given by 
\begin{equation}
\sigma(\mathcal{S}_r) = \left \{\frac{\eta}{1 + r \eta} : \eta \in \sigma(\mathcal{B}\mathcal{A}^\dag \mathcal{B}^T) \right \}.
\end{equation}
Thus, the spectral radius of $\mathcal{S}_r$ has the upper bound i.e., $\rho(\mathcal{S}_r) < 1/r$ for any $r > 0$. Therefore, since the convergence of Richardson method will be guaranteed if $0 < \omega < 2/\rho(\mathcal{S}_r) = 2r$, a simple choice could be $\omega = r$ for the convergence. While it is not the optimal choice, the convergence can be shown as follows: The Augmented Lagrangian Uzawa can be shown to behave as follows: 
\begin{equation}
\|H_* - H_k\| \leq \left ( \frac{1}{1 + r/L_F}\right )^k \|H_* - H_0\| 
\end{equation} 
and 
\begin{equation}
|U_* - U_k|_{\mathcal{A}} \leq \sqrt{1/r} \left ( \frac{1}{1 + r/ L_F}\right )^k \|H_* - H_0\|,  
\end{equation} 
where $\mu_0 = 1/L_F$ is the smallest eigenvalue of $\mathcal{B} \mathcal{A}^{\dag} \mathcal{B}^T$. Now, we shall consider more detailed discussion on optimal choice of $\omega$. We note that 
\begin{equation}
\sigma(\mathcal{S}_r) \in \left [ \frac{1}{\frac{1}{\lambda_F} + r},\frac{1}{\frac{1}{L_F} + r}\right ]
\end{equation} 
Then, the optimum convergence rate is given as follows for $\omega = \frac{2}{\lambda_{min}(\mathcal{S}_r) + \lambda_{max}(\mathcal{S}_r)}$: 
\begin{equation}
\frac{\kappa(\mathcal{S}_r) - 1}{\kappa(\mathcal{S}_r) + 1} = \frac{L - \lambda}{L + \lambda + 2 L \lambda r}.
\end{equation} 
This completes the proof. 
\end{proof} 
\begin{comment} 
To clarify the discussion, we assume that $N_c = 1$ and the original $3 \times 3$ system can be written as 
\begin{equation}
\begin{pmatrix}
\nabla^2 G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
b \\
0
\end{pmatrix}, \qquad B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
\begin{lemma}
The matrix $\nabla^2G(X,z)$ is symmetric positive definite and the spectrum is given by 
\begin{equation}
\sigma(H(G)) \subset \left \{ \min \left \{r + \lambda_F, \frac{r\lambda_F}{r + \lambda_F} \right \}, \max \left \{r + L_F, \frac{rL_F}{r + L_F} \right \} \right \}.  
\end{equation}
\end{lemma}
\begin{proof}
We begin with the spectrally equivalent matrix to $H(G)$, given as follows: 
\begin{equation*}
\begin{pmatrix}
A_r & 0 \\
0   & r K^T K - rK^T A_r^{-1} r K  
\end{pmatrix} = \begin{pmatrix}
A_r & 0 \\
0   & r K^T ( I - r A_r^{-1}) K  
\end{pmatrix} 
\end{equation*}
Due to the $\lambda_F$-strong convexity and $L_F$-smoothness of $F$, it is easy to see 
\begin{subeqnarray*}
&&\sigma(A_r) \subset \left \{ r + \lambda_F, L + \lambda_F \right \} \\
&&\sigma \left (I - r A_r^{-1} \right ) \subset \left \{ 1 - \frac{r}{r+\lambda_F}, 1 - \frac{r}{r + L_F} \right \} = \left \{ \frac{\lambda_F}{r+\lambda_F}, \frac{L_F}{r + L_F} \right \}
\end{subeqnarray*}
Thus, we have that
\begin{subeqnarray*}
\sigma \left (rI - r^2 A_r^{-1} \right ) \subset \left \{ \frac{r\lambda_F}{r+\lambda_F}, \frac{r L_F}{r + L_F} \right \}
\end{subeqnarray*}
and 
\begin{equation}
\sigma(H(G)) \subset \left \{ \min \left \{r + \lambda_F, \frac{r\lambda_F}{r + \lambda_F} \right \}, \max \left \{r + L_F, \frac{rL_F}{r + L_F} \right \} \right \}.  
\end{equation}
This completes the proof. 
%We also note that $K^TK = N_c I$, where $N_c$ is the number of clients and further note that for any $v$, we have 
%\begin{equation}
%\frac{(K^T (I - r A_r^{-1}) K v , v)}{(v,v)} = n \frac{((I - r A_r^{-1}) K v , Kv)}{(K v, Kv)}.   
%\end{equation}
%Thus, we have that 
%\begin{equation}\label{lambdaG}
%\lambda_G = n \left ( 1 - \frac{r}{r + \lambda_F} \right )\leq n \frac{((I - r A_r^{-1}) K v , Kv)}{(K v, Kv)} \leq n \left ( 1 - \frac{r}{r + L_F} \right ) = L_G.       
%\end{equation}
%    This completes the proof.
\end{proof}
\end{comment} 


\begin{comment} 
We shall use the standard notation that for all $k \geq 0$ to discuss the convergence:  
\begin{eqnarray*}
E_k^X &=& X_* - X_k \\
E_k^Z &=& Kz_* - Kz_k \\ 
E_k^H &=& H_* - H_k. 
\end{eqnarray*}

In the following two sections, we shall present convergence analysis. The first section will deal with the Gauss-Seidel method in which $D_r = A_r$. The second section shall deal with $D_r^*$ being the $N-$step Gradient Descent method. Algorithmic details are presented in each subsection.  

In passing to each subsection, we shall present an important result from the non-expansiveness: 
\begin{lemma}\label{main:lem1} 
The Algorithm \ref{algADMM1} produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \right \|^2. 
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\end{lemma}
\begin{proof} 
The Algorithm \ref{algADMM1} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& D_r^* (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $D_r^*$ is an approximate of $A_r^*$, the Fenchel-dual conjugate of $A_r$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& D_r^{*} (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T D_r^* (H_k + r K z_k)) = P_Z [D_r^* (H_k + r K z_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{*} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{*}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH}, and using the orthogonality, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2. 
\end{eqnarray*}
This completes the proof. 

% It is important to observe that 
% \begin{equation}
% X_* = A_r^*(H_* + rKz_*) = D_r^*(H_* + rKz_*). 
% \end{equation}

% Therefore, we see that
% \begin{eqnarray*}
% && \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% && \qquad = \|H_* - H_k - \omega (D_r^* (H_* + rKz_*) - D_r^*(H_k + rKz_*))\|. 
% \end{eqnarray*}
% The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
% \begin{eqnarray*}
% -\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] \right ). 
% \end{eqnarray*}
% Lastly, for $H$, we have 
% \begin{eqnarray*}
% E_{k+1}^H &=& E_k^H + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
% &=& E_k^H - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
% &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
% && \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
% &=& E_k^H - \omega Q_Z (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \\ 
% &=& Q_Z [E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))] 
% \end{eqnarray*}
% First, we shall observe that   
% \begin{eqnarray*}
% \omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left( P_Z [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right )\| \\
% &=& \|P_Z [(H_* - H_k) - \omega \left( [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right ) ] \|
% \end{eqnarray*}
% Next, we observe that there is a close relationship between $E_k^Z$ and $E_k^H$: 
% \begin{eqnarray*}
% \|H_{*} - H_{k+1}\| = \|Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))]\|. 
% \end{eqnarray*}
% Therefore, we have that by the nonexpansiveness, 
% \begin{eqnarray*}
% \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 %\\   
% %&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% %&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
% \end{eqnarray*}
% This completes the proof. 
\end{proof}

\begin{theorem}\label{thm:GS}
The Algorithm \ref{algADMM1} with $D_r = A_r$ and $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ has the convergence rate given as follows: 
\begin{eqnarray}\label{gsrate}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \rho^2_{GS}(r,L_F,\lambda_F) \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ), 
\end{eqnarray}
where with $\kappa(G) = \frac{r+L_F}{r + \lambda_F}$, 
\begin{equation} 
\rho^2_{GS}(r,L_F,\lambda_F) =  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2. 
\end{equation} 
We also have that 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ). 
\end{eqnarray*}
Furthermore, there exists a single optimal $r > 0$ which gives the optimal convergence rate and the convergence factor is always smaller than one for all $r \geq 0$. 
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM1} produces iterates given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^* (H_k + r K z_{\textcolor{red}{k+1}}) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $A_r^*$ is the Fenchel-dual conjugate of $A_r$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. Now due to Lemma \ref{main:lem1}, we have that with $D_r = A_r$,  
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_{\textcolor{red}{k+1}}))\|^2 %\\   
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\begin{comment} 
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{*} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{*}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ]. 
\end{eqnarray*}
The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] \right ). 
\end{eqnarray*}
Lastly, for $H$, we have 
\begin{eqnarray*}
H_{*} - H_{k+1} &=& H_* - H_k + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
&=& H_* - H_k - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
&=& H_* - H_k - \omega [ A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
&& - P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] ] \\
&=& H_* - H_k - \omega Q_Z (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)) \\ 
&=& Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))] 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
H_{*} - H_{k+1} - \omega (Kz_* - K z_{k+1}) = H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)). 
\end{eqnarray*}
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^*(H_k + rKz_*) - A_r^*(H_k + rKz_{\textcolor{red}{k+1}}). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_{\textcolor{red}{k+1}}))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^*} = 1/(r + L_F)$ and $L_{G^*} = 1/(r + \lambda_F)$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{r + L_F} + \frac{1}{r + \lambda_F}} = \frac{2 (r + \lambda_F)(r + L_F)}{2r + L_F + \lambda_F}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F} \|E_k^H\|\|E_{\textcolor{red}{k+1}}^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{r + \lambda_F} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^*} + L_{G^*})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^*(H_k + r Kz_*) - A_r^{*} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r + \lambda_F} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}
This provides the convergence rate. Finally, we notice that for all $r \geq 0$, 
\begin{eqnarray*}
\frac{r^2}{\omega^2} = \frac{r^2 (2r + L_F + \lambda_F)^2}{4(r + \lambda_F)^2(r + L_F)^2} < 1. 
\end{eqnarray*}
Thus, we obtain that due to the orthogonality, 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 &=& \|A_r^*(H_* + rKz_*) - A_r^*(H_k + r K z_k) \|^2 \\
&\leq& \frac{1}{(r + \lambda_F)^2}\|E_k^H - r E_k^Z\|^2 = \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + r^2 \|E_k^Z\|^2 \right ) \\
&=& \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \frac{r^2}{\omega^2} \|E_k^Z\|_\omega ^2 \right ) \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) 
\end{eqnarray*}
We now discuss the convergence factor denoted by $\rho_{GS}^2$ and given by 
\begin{equation}
f(r) = \rho^2_{GS}(r, L_F, \lambda_F) = \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 = \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 
\end{equation}
We note that the simple calculation shows that the derivative of $f(r)$ is given as follows: 
\begin{equation}
f'(r) = \frac{-4(L-\lambda)^2}{(2r + L_F + \lambda_F)^3} + \frac{2r \lambda_F}{(r + \lambda_F)^3}. 
\end{equation}
Therefore, for small $r$, it takes the negative sign, but it changes its sign for larger $r$ and continues to be positive. Thus, there exists a single critical point, which gives the optimal $r_{\rm opt}$. On the other hand, it is continuously increasing (see Figure \ref{exam}). Since $\lim_{r \rightarrow \infty} f(r) = 1$ and thus, for any $r \geq 0$, the convergence factor is smaller than one. Thus, it converges linearly for all fixed $r \geq 0$. This completes the proof.
%\end{proof} 
\end{comment} 

\subsection{Convergence of Exact and Inexact Block Gauss-Seidel Method for the $U$ system}
In this section, we discuss the solution by Gauss-Seidel for the following subsystem of the full system for Federated Learning: 
\begin{equation}
\mathcal{A}_r \begin{pmatrix} X_* \\ z_* \end{pmatrix} = \begin{pmatrix} 
A + rI & - r K \\ - r K^T & r K^T K   
\end{pmatrix} \begin{pmatrix} X_* \\ z_* \end{pmatrix} = \begin{pmatrix} f_1 \\ f_2 \end{pmatrix}. 
\end{equation} 
We shall consider both exact block Gauss-Seidel method and inexact block Gauss-Seidel method. To handle these at the same time, it would be good to introduce a general framework. We denote $R$ by the modification of $D^{-1}$. Then, the modified block Gauss-Seidel method is given as follows: 
\begin{equation}\label{gsmethod} 
U_{k+1} = U_k + (R^{-1} + L)^{-1} (f - \mathcal{A}_r U_{k}), \quad k = 1,2,\cdots. 
\end{equation}
Note that 
\begin{equation} 
D = \begin{pmatrix} A + r I & 0 \\ 0 & rN I \end{pmatrix} \quad \mbox{ and } \quad L = \begin{pmatrix} 0 & 0 \\ -r K^T & 0 \end{pmatrix}. 
\end{equation} 
We note that the modified block Gauss-Seidel method converges if 
\begin{equation}
\overline{R} = R^T + R - R^T D R > 0. 
\end{equation} 
Furthermore, the convergence rate can be obtained as follows: 
\begin{equation}
\|I - (R^{-1} + L)^{-1} \mathcal{A}_r\|_{\mathcal{A}_r}^2 = 1 - \frac{1}{1 + c_0(r)}, 
\end{equation} 
where 
\begin{equation}
c_0(r) = \sup_{\|v\|_{\mathcal{A}_r}=1} \langle \overline{R}^{-1} R^T ( D+ U - R^{-1}) v, R^T (D + U - R^{-1}) v \rangle. 
\end{equation}

\begin{remark}
The above framework can handle the case when $n-$step Gradient descent method is used to handle $A_r = A + rI$ block.
\end{remark} 
In the next two sections, we shall consider the case $R = D^{-1}$ and $R$ is an approximate block solve for $A_r = A + rI$. We begin with the first case. 

\subsubsection{Exact Block Gauss-Seidel Method for $U$ system} 

In this section, we shall establish the convergence rate for the case when $R = D^{-1}$. Here is the main theorem:

\begin{theorem} 
The block Gauss-Seidel method defined by the Equation \eqref{gsmethod}, i.e., $R = D^{-1}$, converges with the rate given as follows: 
\begin{equation}
\|I - (D + L)^{-1} \mathcal{A}_r\|_{\mathcal{A}_r}^2 \leq \frac{r}{r+\lambda_F}.  
\end{equation}
\end{theorem}
\begin{proof} 
By the abstract convergence estimate, we have that 
\begin{subeqnarray}
c_0(r) &=& \sup_{v = (X,z)} \frac{\langle D^{-1} U v, Uv \rangle}{\|v\|_{\mathcal{A}_r}^2} \\ 
&=& \sup_{v = (X,z)} \frac{\langle A_r^{-1}(rK z), (rKz) \rangle}{(A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)}  \\
&=& \sup_{z} \frac{\langle A_r^{-1}(rK z), (rKz) \rangle}{\inf_{X} (A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)}.
\end{subeqnarray}
For any given $z$, we are confronted to solve the following minimization problem over $X$: 
\begin{eqnarray*}
&& \inf_{X} (AX,X) + r(X,X) - 2r (Kz,X) + r (Kz,Kz) \\
&& \quad \geq \inf_{X} (\lambda_F X,X) + r(X,X) - 2r (Kz,X) + r(Kz,Kz) \\ 
&& \quad = \inf_{\alpha \geq 0} ((r + \lambda_F) \alpha^2 - 2r \alpha + r) \|Kz\|^2 \\
&& \quad = \inf_{\alpha \geq 0} \left ( (r+\lambda_F)\left (\alpha - \frac{r}{(r+\lambda_F)} \right )^2  - \frac{r^2}{r+\lambda_F} + r \right ) \|Kz\|^2 \\ 
&& \quad = \frac{\lambda_F r}{r+\lambda_F} \|Kz\|^2. 
\end{eqnarray*} 
Therefore, we have that 
\begin{equation}
c_0(r) \leq \sup_{z} \frac{\frac{r^2}{r+\lambda_F} \|Kz\|^2}{\frac{\lambda_F r}{r+\lambda_F}\|Kz\|^2} = \frac{r}{\lambda_F}. 
\end{equation} 
This means that the convergence rate behaves like 
\begin{equation}
1 - \frac{1}{1 + \frac{r}{\lambda_F}} = 1 - \frac{\lambda_F}{r + \lambda_F} = \frac{r}{r+\lambda_F}. 
\end{equation}
This completes the proof. 
\end{proof}
\begin{remark} 
Note that this result shows that the convergence rate can deteriorate for large $r \gg 1$. Thus, the advantage we see at large $r$ for the exact Augmented Lagrangian Uzawa may not be fully exploited, due to this effect. 
\end{remark} 

\subsubsection{Inexact Block Gauss-Seidel Method for $U$ system : $n-$ step Gradient Descent for the first block} 
In this section, we consider the standard block solve for which the first block is given as $n-$ step Gradient Descent method. To describe the approximate $A_r^{-1}$, we consider to solve the following system: 
\begin{equation}
A_r x = b. 
\end{equation}
The Gradient descent method reads as follows: given $x_0$, we update  
\begin{equation}
x_{k+1} = x_k + \gamma ( b - A_r x_k ) = (I - \gamma A_r)x_k + \gamma b, \quad \forall k = 0, 1, 2, \cdots,   
\end{equation} 
where $\gamma > 0$ is the learning rate. Note that if $\gamma$ is chosen appropriately, then we have that 
\begin{equation}
x_* = A_r^{-1}b. 
\end{equation}
Particularly, we shall use $\gamma = \frac{1}{r + L_F}$, for which the convergence rate can be shown to be 
\begin{equation}
\delta = \delta(r) = \frac{\kappa(A_r) - 1}{\kappa(A_r)} = 1 - \frac{1}{\kappa(A_r)}. 
\end{equation}
Note that better choice of $\gamma$ can be made, but, it is useful for the convergence analysis, since it leads $I - \gamma A_r$ still positive. We observe that the $n-$step Gradient descent operator can be written as follows, which depends on the initial iterate and the right hand side. 
\begin{subeqnarray*}
x_n = G_n(x_0;b) := (I - \gamma A_r)^n x_0 + ( I - (I - \gamma A_r)^n) A_r^{-1} b.
\end{subeqnarray*}
We notice that $G_n$ is basically dependent on $b$, but it is also determined by the initial iterate. In any case, we have the following lemma:
\begin{lemma}
If $\gamma$ is chosen so that $\rho(I - \gamma A_r) < 1$. Then we have, 
\begin{equation}
\lim_{n \rightarrow \infty} G_n(x_0;\cdot) = A_r^{-1},  
\end{equation} 
which is independent of $x_0$. Furthermore, the operator $G_n(x_0;\cdot)$ is symmetric. Namely, $G_n^T = G_n$.
\end{lemma}
\begin{proof}
First, we note that for any given $x_0$ and independent of choice of $\gamma > 0$, we have 
\begin{subeqnarray}
(I - (I - \gamma A_r)^n) A_r^{-1} = A_r^{-1} (I - (I - \gamma A_r)^n). 
\end{subeqnarray}
This leads to $G_n^T = G_n$. The convergence of $G_n$ to $A_r^{-1}$ is a trivial consequence of spectral analysis. This completes the proof. 
\end{proof}
The Block Gauss-Sidel based on $n-$step GD for the first block shall be denoted by $R_n$ which is defined as following: 
\begin{equation}
R_n = \begin{pmatrix} G_n & 0 \\ 0 & (rK^TK)^{-1} \end{pmatrix} \approx D ^{-1} = \begin{pmatrix} A_r^{-1} & 0 \\ 0 & (rK^TK)^{-1} \end{pmatrix}.
\end{equation}
The convergence of the method can be verified under a special choice of $\gamma$. Namely, we have 
\begin{lemma}
For $\gamma$ such that $\rho(I - \gamma A_r) < 1$, the iterative method given as follows: 
\begin{equation}
U_{k+1} = U_k + (R_n^{-1} + L)^{-1}(f - \mathcal{A}_rU_k), \quad k=1,\cdots,  
\end{equation} 
converges. 
\end{lemma} 
\begin{proof} 
According to the abstract theory \cite{xu2002method}, we investigate $R_n^T + R_n - R_n^T D R_n$ and observe that 
\begin{eqnarray*}
R_n^T + R_n - R_n^T D R_n &=& 2R_n - R_nDR_n = R_n(2I - DR_n) \\
&=& \begin{pmatrix} G_n & 0 \\ 0 & \frac{1}{rN} I \end{pmatrix} \begin{pmatrix}  2I - A_r G_n  & 0 \\ 0 & I \end{pmatrix} = \begin{pmatrix} G_n(2I - A_r G_n) & 0 \\ 0 & \frac{1}{rN} I \end{pmatrix} %\\ 
%&=& \begin{pmatrix} (I - (I - \gamma A_r)^n) A_r^{-1}(I - (I - \gamma A_r)^n) & 0 \\ 0 & \frac{1}{rN} I \end{pmatrix} > 0.
\end{eqnarray*} 
Since both $G_n$ and $2I - A_rG_n$ are positive operator, this completes the proof.  
\end{proof} 
We note that our main goal is to estimate the following: 
\begin{eqnarray*}
c_0(r) &=& \sup_{v = (X,z)} \frac{\langle R_n \overline{R_n}^{-1} R_n Mv, Mv \rangle}{\|v\|_{\mathcal{A}_r}^2}, 
\end{eqnarray*}
where 
\begin{eqnarray*}
M = D + U - R_n^{-1} &=& \begin{pmatrix} A_r & -rK \\ 0 & rN I \end{pmatrix} - \begin{pmatrix} G_n^{-1} & 0 \\ 0 & rN I \end{pmatrix} \\
&=& \begin{pmatrix} A_r - G_n^{-1} & -rK \\ 0 & 0 \end{pmatrix} %= \begin{pmatrix} A_r - A_r(I - (I - \gamma A_r)^n)^{-1} & -rK \\ 0 & 0 \end{pmatrix}
\end{eqnarray*}
Thus, we have that 
\begin{equation}
Mv = M \begin{pmatrix} X \\ z \end{pmatrix} = \begin{pmatrix} (A_r - G_n^{-1}) X - rKz \\ 0 \end{pmatrix} = \begin{pmatrix} \Delta_1 X - rKz \\ 0 \end{pmatrix} 
\end{equation}
On the other hand, we have 
\begin{eqnarray*}
R_n \overline{R_n}^{-1} R_n &=& R_n (R_n (2I - DR_n))^{-1} R_n = R_n (2I - DR_n)^{-1} = (2R_n^{-1} - D)^{-1} \\
&=& \begin{pmatrix} (2 G_n^{-1} - A_r)^{-1} & 0 \\ 0 & r N I \end{pmatrix} = \begin{pmatrix} \Delta_2 & 0 \\ 0 & r N I \end{pmatrix}. 
\end{eqnarray*}
and further, we have that 
\begin{eqnarray}
M^T R_n \overline{R_n}^{-1} R_n^TM &=& \begin{pmatrix} \Delta_1 & 0 \\ -rK^T & 0\end{pmatrix} \begin{pmatrix} \Delta_2 & 0 \\ 0 & rN I \end{pmatrix}\begin{pmatrix} \Delta_1 & -rK  \\ 0 & 0\end{pmatrix} 
\\ 
&=& \begin{pmatrix} \Delta_1 \Delta_2 & 0 \\ -rK^T \Delta_2 & 0\end{pmatrix} \begin{pmatrix} \Delta_1 & -rK  \\ 0 & 0\end{pmatrix} = \begin{pmatrix} \Delta_1 \Delta_2 \Delta_1 & -r \Delta_1 \Delta_2 K  \\ -r K^T \Delta_2 \Delta_1 & r^2 K^T \Delta_2 K \end{pmatrix},
\end{eqnarray}
where 
\begin{subeqnarray*}
\Delta_1 &=& A_r - G_n^{-1} \\ %= A_r(I - A_r^{-1} G_n^{-1}) = A_r(I - (I - (I - \gamma A_r)^n)^{-1}) \\
\Delta_2 &=& (2 G_n^{-1} - A_r)^{-1} = (2 I - G_n A_r)^{-1} G_n, 
%&=& (2 A_r(I - (I - \gamma A_r)^n)^{-1} - A_r)^{-1} = (2(I - (I - \gamma A_r)^n)^{-1} - I)^{-1}A_r^{-1}  
\end{subeqnarray*}

We shall further investigate operators $\Delta_1$, $\Delta_2$ and $\Delta_1 \Delta_2 \Delta_1$. 
\begin{lemma}\label{tool}
Let $E_n = I - G_n A_r$ and
\begin{subeqnarray*}
\Delta_1 &=& A_r - G_n^{-1}, \\ %= A_r(I - A_r^{-1} G_n^{-1}) = A_r(I - (I - (I - \gamma A_r)^n)^{-1}) \\
\Delta_2 &=& (2 G_n^{-1} - A_r)^{-1} = (2 I - G_n A_r)^{-1} G_n. 
%&=& (2 A_r(I - (I - \gamma A_r)^n)^{-1} - A_r)^{-1} = (2(I - (I - \gamma A_r)^n)^{-1} - I)^{-1}A_r^{-1}  
\end{subeqnarray*}
Then, we have 
\begin{eqnarray}
\Delta_2 &=& (I + E_n)^{-1} (I - E_n) A_r^{-1}, \\
\Delta_1 \Delta_2 &=& = -I + (2I - A_rG_n)^{-1}, \\
\Delta_2 \Delta_1 &=& -I + (2 I - G_n A_r)^{-1}, \\
\Delta_1 \Delta_2 \Delta_1 &=& A_r (I - E_n^2)^{-1} E_n^2. 
\end{eqnarray}
\end{lemma} 
\begin{proof} 
First, we observe that 
\begin{subeqnarray*}
\Delta_2 = (2I - G_n A_r)^{-1} G_n = (I + E_n)^{-1} G_n A_r A_r^{-1} = (I + E_n)^{-1} (I - E_n) A_r^{-1},
\end{subeqnarray*}
which proves the first identity. We now observe that 
\begin{subeqnarray*}
\Delta_1 \Delta_2 &=& (A_r - G_n^{-1}) ( 2 G_n^{-1} - A_r)^{-1} = (A_r - G_n^{-1}) (2I - G_n A_r)^{-1} G_n \\
&=& (A_r G_n - I) (2I - A_r G_n)^{-1} = -I + (2I - A_rG_n)^{-1} \\
\Delta_2 \Delta_1 &=& (2 I - G_n A_r)^{-1} G_n (A_r - G_n^{-1}) = (2I - G_n A_r)^{-1} (G_n A_r - I) \\
&=& -I + (2 I - G_n A_r)^{-1}. 
\end{subeqnarray*}
Finally, we have that 
\begin{subeqnarray*}
\Delta_1 \Delta_2 \Delta_1 &=& -\Delta_1 + \Delta_1 (2I - G_n A_r)^{-1} \\
&=& G_n^{-1} - A_r + (A_r - G_n^{-1}) (2 - G_n A_r)^{-1} \\
&=& G_n^{-1} - A_r + (A_r - G_n^{-1}) (2G_n^{-1} - A_r)^{-1} G_n^{-1} \\
&=& G_n^{-1} - A_r + (A_r - 2G_n^{-1} + G_n^{-1}) (2G_n^{-1} - A_r)^{-1} G_n^{-1} \\
&=& G_n^{-1} - A_r - G_n^{-1} + G_n^{-1} (2G_n^{-1} - A_r)^{-1} G_n^{-1} \\
&=& -A_r + G_n^{-1} (2G_n^{-1} - A_r)^{-1} G_n^{-1} = -A_r + G_n^{-1} (2I - G_n A_r)^{-1}.  
\end{subeqnarray*}
On the other hand, we have 
\begin{subeqnarray*}
-A_r + G_n^{-1} ( 2I - G_n A_r)^{-1} &=& G_n^{-1} \left [ -G_n A_r (2 I - G_n A_r) + I \right ] (2I - G_n A_r)^{-1}  \\
&=& G_n^{-1} \left [ (G_n A_r)^2 - 2G_n A_r + I \right ] (2I - G_n A_r)^{-1} \\
&=& G_n^{-1}  (I - G_n A_r)^2 (2I - G_n A_r)^{-1} \\
&=& G_n^{-1} E_n^2 (I + E_n)^{-1} \\ 
&=& A_r ( G_n A_r)^{-1} E_n^2 ( I + E_n)^{-1} \\
&=& A_r (I - E_n)^{-1} E_n^2 (I + E_n)^{-1} = A_r (I - E_n^2)^{-1} E_n^2. 
\end{subeqnarray*}
This completes the proof. 
\end{proof} 
\begin{lemma}
The following holds: 
\begin{eqnarray*}
\langle \Delta_2 Y,Y \rangle &\leq& \langle A_r^{-1} Y,Y \rangle, \quad \forall Y, \\ 
\langle \Delta_1 \Delta_2 \Delta_1 Y,Y \rangle &\leq& \frac{\delta^{2n}}{1 - \delta^{2n}} \langle A_r Y,Y \rangle, \quad \forall Y. 
\end{eqnarray*}
\end{lemma}
\begin{proof}
The proof is a consequence of Lemma \ref{tool}. This completes the proof. 
\end{proof}
We consider the special case when GD becomes the GS method. 
\begin{lemma} 
Note that if $G_n = A_r^{-1}$, then we have $\Delta_1 = 0$ and $\Delta_2 = A_r^{-1}$, thus, we have 
\begin{eqnarray*}
M^T R \overline{R}^{-1} R^TM = \begin{pmatrix} \Delta_1 \Delta_2 \Delta_1 & -r \Delta_1 \Delta_2 K  \\ -r K^T \Delta_2 \Delta_1 & r^2 K^T \Delta_2 K \end{pmatrix} = \begin{pmatrix} 0 & 0  \\ 0 & r^2 K^T A_r^{-1} K \end{pmatrix}.
\end{eqnarray*} 
\end{lemma}
\begin{comment}
We note that with $Y = A_r^{1/2}X$ and $Z = (I - rA_r^{-1})^{1/2}Kz$, we see that
\begin{subeqnarray*}
\langle Y, Y \rangle &=& \langle A_r X, X\rangle \\ 
\langle r^{1/2} Z, r^{1/2} Z \rangle &=& \langle C z, z \rangle. 
\end{subeqnarray*}
Furthermore, we have that 
\begin{eqnarray*}
\langle \Delta_2 \Delta_1 X, \Delta_1 X \rangle &=& \langle A_r^{-1/2} \Delta_1 \Delta_2 \Delta_1 A_r^{-1/2} Y, Y\rangle \\
r^2 \langle \Delta_2 Kz, Kz)\rangle &=& r \langle (I - rA_r^{-1})^{-1/2} \Delta_2 (I - rA_r^{-1})^{-1/2} r^{1/2} Z, r^{1/2} Z)\rangle 
\end{eqnarray*} 
Therefore, we only need to estimate the spectral radius of the following two operators: 
\begin{subeqnarray*}
A_r^{-1/2} \Delta_1 \Delta_2 \Delta_1 A_r^{-1/2} &=& A_r^{-1/2} (A_r - G_n^{-1}) (2 G_n^{-1} - A_r)^{-1}(A_r - G_n^{-1}) A_r^{-1/2} \\
r (I - rA_r^{-1})^{-1/2} \Delta_2 (I - rA_r^{-1})^{-1/2} &=& r (I - rA_r^{-1})^{-1/2} (2 G_n^{-1} - A_r)^{-1} (I - rA_r^{-1})^{-1/2} 
\end{subeqnarray*}
\end{comment}
%Lastly, we assume that $K^TK = NI$ and 
%\begin{equation}
%A_r = {\rm{diag}} (A_{r,1}, A_{r,2}, \cdots, A_{r,N}) \mbox{ and } G_n = %{\rm{diag}} (G_{n,1}, \cdots, G_{n,N}).  %
%\end{equation} 
%Then, we see that
%\begin{equation}
%K^T \Delta_2 K = \sum_{i=1}^N (2 G_{n,i}^{-1} - A_{r,i})^{-1}. 
%\end{equation} 
%Note that if $N = 1$, then we see that
%\begin{equation}
%K^T \Delta_2 K = (2 G_{n}^{-1} - A_{r})^{-1}. 
%\end{equation} 
We now note that by applying the explicit expression of operators, involved in $c_0(r)$ and obtain the following theorem:  
Thus, the convergence rate is given as follows: 
\begin{theorem}
For $r > 0$, the convergence of the $n-$step GD based block solver converges with the rate given as follows for sufficiently large $n$: 
\begin{equation}
\|I - (R_n + L)^{-1} \mathcal{A}_r\|_{\mathcal{A}_r}^2 \leq \frac{r +  \frac{ 2 (r + L_F) \delta^{2n}}{1 - \delta^{2n}} }{r + \frac{ 2 (r + L_F)\delta^{2n}}{1 - \delta^{2n}} + \lambda_F}. 
\end{equation} 
\end{theorem} 
\begin{proof}
Using the abstract theory \cite{xu2002method}, the convergence rate can be established as 
\begin{equation}
\rho(r) = 1 - \frac{1}{1 + c_0(r)},
\end{equation}
where 
\begin{eqnarray}
c_0(r) = \sup_{v = (X,z)} \frac{\langle R_n \overline{R_n}^{-1} R_n Mv, Mv \rangle}{\|v\|_{\mathcal{A}_r}^2}.
\end{eqnarray}
To estimate $c_0(r)$, we investigate it as follows:
\begin{eqnarray*}
c_0(r) &=& \sup_{v = (X,z)} \frac{\langle (\Delta_1 \Delta_2 \Delta_1 X - r\Delta_1\Delta_2 Kz), X \rangle + \langle -rK^T \Delta_2 \Delta_1 X + r^2 K^T \Delta_2 Kz, z\rangle} {\langle A_r X - rKz, X \rangle + \langle -rK^T X + rK^T Kz, z\rangle } \\
&=& \sup_{v = (X,z)} \frac{\langle \Delta_2 \Delta_1 X, \Delta_1 X \rangle  - 2r \langle \Delta_2\Delta_1X,Kz\rangle + r^2 \langle \Delta_2 Kz, K z\rangle}{\langle A_r X,X\rangle - 2r \langle Kz, X \rangle + r\langle Kz, Kz\rangle } 
\end{eqnarray*}
The numerator can be estimated as follows:
\begin{eqnarray*}
&& \langle \Delta_1 \Delta_2 \Delta_1 X, X \rangle  - 2r \langle \Delta_2\Delta_1X,Kz\rangle + r^2 \langle \Delta_2 Kz, K z\rangle \\
&& \qquad \leq 2 \frac{\delta^{2n}}{1 - \delta^{2n}} \langle A_r X, X\rangle + 2 r^2 \langle A_r^{-1}Kz,Kz\rangle. 
\end{eqnarray*}
Thus, we have that for any $v = (X,z)$, we have 
\begin{eqnarray*}
&& \langle \Delta_2 \Delta_1 X, \Delta_1 X \rangle  - 2r \langle \Delta_2\Delta_1X,Kz\rangle + r^2 \langle \Delta_2 Kz, K z\rangle \\
&& \qquad = \frac{2 \delta^{2n}}{1 - \delta^{2n}} \langle A_r X,X\rangle + \frac{r}{r+\lambda_F} r \langle Kz, Kz\rangle = \alpha_1 \langle X, X\rangle + \beta_1 r \langle Kz,Kz\rangle, 
\end{eqnarray*}
where $\alpha_1$ and $\beta_1$ denote the following: 
\begin{equation} 
\alpha_1 = \frac{2 \delta^{2n}}{1 - \delta^{2n}} \quad \mbox{ and } \quad \beta_1 =  \frac{r}{r+\lambda_F}. 
\end{equation} 
Therefore, we have that $v = (X,Kz)$ leads the smaller denominator for the case when $X = \alpha Kz$ with $\alpha > 0$. Thus, with such a choice, we see that the following bounds hold: 
\begin{eqnarray}
c_0(r) &\leq& \sup_{v = (X,z)} \frac{\alpha_1 \langle X,X \rangle + \beta_1 r \langle Kz, Kz\rangle}{\langle A_rX,X\rangle - 2r \langle Kz, X\rangle + r \langle Kz, Kz\rangle} \\ 
&\leq& \sup_{v = (X,Y)} \frac{\alpha_1 \langle A_r X,X \rangle + \beta_1 r \langle Y, Y\rangle}{\langle A_r X,X\rangle - 2r \langle Y, X\rangle + r \langle Y, Y\rangle} \\ 
&\leq& \sup_{v = (X,Y)} \frac{\begin{pmatrix} X \\ Y \end{pmatrix}^T \begin{pmatrix} \alpha_1 (r + L_F) & 0 \\ 0 & \beta_1 r \end{pmatrix} \begin{pmatrix} X \\ Y \end{pmatrix}}{\begin{pmatrix} X \\ Y \end{pmatrix}^T \begin{pmatrix} r + \lambda_F & -r \\ -r & r \end{pmatrix} \begin{pmatrix} X \\ Y \end{pmatrix}} \\ 
&=& \sup_{v = (X,Y)} \frac{\begin{pmatrix} X \\ Y \end{pmatrix}^T \begin{pmatrix} \alpha & 0 \\ 0 & \epsilon \end{pmatrix} \begin{pmatrix} X \\ Y \end{pmatrix}}{\begin{pmatrix} X \\ Y \end{pmatrix}^T \begin{pmatrix} \beta & -r \\ -r & r \end{pmatrix} \begin{pmatrix} X \\ Y \end{pmatrix}}, 
\end{eqnarray}
where $\alpha = \alpha_1 (r + L_F)$, $\beta = r + \lambda_F$ and $\epsilon = r^2/(r + \lambda_F)$. By applying the change of variable, i.e., $\frac{1}{\sqrt{\alpha}} X_1 = X$ and $\frac{1}{\sqrt{\epsilon}} Y_1 = Y$ and with the abuse of notation, we have 
\begin{eqnarray} 
c_0(r) &\leq& \sup_{v = (X,Y)} \frac{(X,X) +  (Y,Y)}{\begin{pmatrix} X/\sqrt{\alpha} & Y/\sqrt{\epsilon} \end{pmatrix} \begin{pmatrix} \beta & -r \\ -r & r \end{pmatrix} \begin{pmatrix} X/\sqrt{\alpha} \\ Y/\sqrt{\epsilon} \end{pmatrix}} \\  
&=& \sup_{v = (X,Y)} \frac{ (X,X) + (Y,Y) }{\begin{pmatrix} X & Y \end{pmatrix} \begin{pmatrix} \frac{\beta}{\alpha} & -\frac{r}{\sqrt{\alpha \epsilon}} \\ 
-\frac{r}{\sqrt{\alpha \epsilon}} & \frac{r}{\epsilon} \end{pmatrix} \begin{pmatrix} X & Y \end{pmatrix}} \\
&=& \frac{1}{\min \left\{ \sigma \left ( 
\begin{pmatrix} \frac{\beta}{\alpha} & -\frac{r}{\sqrt{\alpha \epsilon}} \\ 
-\frac{r}{\sqrt{\alpha \epsilon}} & \frac{r}{\epsilon} \end{pmatrix} \right ) \right \}}. 
\end{eqnarray}
To compute the denominator, we consider the characteristic polynomial: 
\begin{equation}
\left ( \frac{\beta}{\alpha} - \lambda \right ) \left ( \frac{r}{\epsilon} - \lambda \right ) - \frac{r^2}{\alpha \epsilon} = 0.  
\end{equation}
The minimum eigenvalue is given as follows: 
\begin{eqnarray*}
\lambda &=& \frac{\frac{r}{\epsilon} + \frac{\beta}{\alpha} - \sqrt{ \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 + \frac{4r^2}{\alpha \epsilon} } }{2} 
\end{eqnarray*}
Therefore, we have that
\begin{eqnarray}
c_0(r) &\leq& \frac{2}{
\frac{r}{\epsilon} + \frac{\beta}{\alpha} - \sqrt{ \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 + \frac{4r^2}{\alpha \epsilon}}} = \frac{2 \left ( \frac{r}{\epsilon} + \frac{\beta}{\alpha} + \sqrt{ \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 + \frac{4r^2}{\alpha \epsilon}} \right ) }{
\left ( \frac{r}{\epsilon} + \frac{\beta}{\alpha} \right )^2 - \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 - \frac{4r^2}{\alpha \epsilon}} \\
&=& \frac{2 \left ( \frac{r}{\epsilon} + \frac{\beta}{\alpha} + \sqrt{ \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 + \frac{4r^2}{\alpha \epsilon}} \right ) }{
\frac{4r\beta}{\epsilon \alpha} - \frac{4r^2}{\alpha \epsilon}} = \frac{2 \left ( \frac{r}{\epsilon} + \frac{\beta}{\alpha} + \sqrt{ \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 + \frac{4r^2}{\alpha \epsilon}} \right ) }{
\frac{4r\lambda_F}{\epsilon \alpha}} \\
&=& \frac{\left ( \frac{r}{\epsilon} + \frac{\beta}{\alpha} + \sqrt{ \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 + \frac{4r^2}{\alpha \epsilon}} \right ) }{
\frac{2r\lambda_F}{\epsilon \alpha}} = \frac{\left ( \alpha r + \epsilon \beta  + \epsilon \alpha \sqrt{ \left ( \frac{r}{\epsilon} - \frac{\beta}{\alpha} \right )^2 + \frac{4r^2}{\alpha \epsilon}} \right ) }{2r\lambda_F}   \\
&=& \frac{\left ( \alpha r + \epsilon \beta  + \epsilon \alpha \sqrt{ \left ( \frac{r}{\epsilon} \right )^2 + \left( \frac{\beta}{\alpha} \right )^2 - \frac{2r\beta}{\epsilon \alpha} + \frac{4r^2}{\alpha \epsilon}} \right ) }{2r\lambda_F} \\ 
&\leq& \frac{\left ( \alpha r + \epsilon \beta  + \epsilon \alpha \sqrt{ \left ( \frac{r}{\epsilon} \right )^2 + \left( \frac{\beta}{\alpha} \right )^2 - \frac{2r\beta}{\epsilon \alpha} + \frac{4r\beta}{\alpha \epsilon}} \right ) }{2r\lambda_F} \\
&\leq& \frac{\left ( \alpha r + \epsilon \beta  + \epsilon \alpha \sqrt{ \left ( \frac{r}{\epsilon} \right )^2 + \left( \frac{\beta}{\alpha} \right )^2 + \frac{2r\beta}{\epsilon \alpha}} \right ) }{2r\lambda_F} \leq \frac{\left ( \alpha r + \epsilon \beta  + \epsilon \alpha \sqrt{ \left ( \frac{r}{\epsilon} + \frac{\beta}{\alpha} \right )^2} \right ) }{2r\lambda_F} \\
&=& \frac{\left ( \alpha r + \epsilon \beta  + \epsilon \alpha \left ( \frac{r}{\epsilon} + \frac{\beta}{\alpha} \right ) \right ) }{2r\lambda_F} = \frac{2\alpha r + 2\epsilon \beta}{2r\lambda_F} = \frac{\alpha r + \epsilon \beta}{r \lambda_F} = \frac{\alpha}{\lambda_F} + \frac{r}{\lambda_F}. 
\end{eqnarray}
Therefore, as $n \rightarrow \infty$, $c_0(r) \leq \frac{r}{\lambda_F}$. We then have the convergence rate given as follows:
\begin{equation}
\rho_1(r) = 1 - \frac{1}{1 + \frac{\alpha}{\lambda_F} + \frac{r}{\lambda_F}} = 1 - \frac{\lambda_F}{\lambda_F + \alpha + r} = \frac{r + \alpha}{r + \alpha + \lambda_F}. \end{equation}
This completes the proof. 
\end{proof} 
%For $r = 0$, it is clear that without going through the Cauchy Schwartz inequality, 
%\begin{equation}
%c_0(0) = \frac{\delta^{2n}}{1 - \delta^{2n}}. 
%\end{equation} 
%Thus, the convergence rate is given as follows:
%\begin{theorem}
%For $r = 0$, the convergence of the $n-$step GD based block solver converges with the rate given as follows: 
%\begin{equation}
%\|I - (R_n + L)^{-1} \mathcal{A}_r\|_{\mathcal{A}_r}^2 %\leq \delta^{2n}. 
%\end{equation} 
%\end{theorem}


\begin{remark}
%For $r = 0$, it is clear that without going through the Cauchy Schwartz inequality, 
%\begin{equation}
%c_0(0) = \frac{\delta^{2n}}{1 - \delta^{2n}}. 
%\end{equation} 
%Thus, the convergence rate is given as follows:
%\begin{theorem}
%For $r = 0$, the convergence of the $n-$step GD based block solver converges with the rate given as follows: 
%\begin{equation}
%\|I - (R_n + L)^{-1} \mathcal{A}_r\|_{\mathcal{A}_r}^2 %\leq \delta^{2n}. 
%\end{equation} 
%\end{theorem}

If $\lambda_F \ll 1$, then the convergence rate can deteriorate significantly unless $r$ is chosen very small and $n$ is sufficiently large.  
\end{remark}
\begin{comment} 
Note that if $\gamma = \frac{1}{r+L_F} < \frac{2}{\rho(A_r)}$, then $I - \gamma A_r$ is positive semi-definite (I believe it is the choice poeple make in the analysis). Thus, the analysis can be further achieved. In that case, we see that 
\begin{equation}
\Delta_1 \leq 0 I.  
\end{equation}

\begin{remark} 
Therefore, it is desired that $X = \alpha Kz$ with $\alpha > 0$. 
\textcolor{red}{I have made a mistake here since $\Delta_1$ may not be invertible.} With $^{\dag} = ^{-1}$, we make a note that the following spectral analysis holds: 
\begin{eqnarray*}
&& \begin{pmatrix} \Delta_1 \Delta_2 \Delta_1 & -r \Delta_1 \Delta_2 K  \\ -r K^T \Delta_2 \Delta_1 & r^2 K^T \Delta_2 K \end{pmatrix} \\
&& \quad = \begin{pmatrix} I & 0 \\ -rK^T (\Delta_1)^{-1} & I \end{pmatrix} \begin{pmatrix} \Delta_1 \Delta_2 \Delta_1 & 0 \\ 0 & 0 \end{pmatrix}   \begin{pmatrix} I & (\Delta_1)^{-1} (-rK) \\ 0 & I \end{pmatrix} = C_1 D_1 C_1^T
\end{eqnarray*}

On the other hand, 
\begin{eqnarray*}
&& \begin{pmatrix} A_r & -r K  \\ -r K^T & r K^T K \end{pmatrix} \\
&& \quad = \begin{pmatrix} I & 0 \\ -r K^T A_r^{-1} & I \end{pmatrix} \begin{pmatrix} A_r & 0 \\ 0 & rK^T(I - rA_r^{-1}) K \end{pmatrix}   \begin{pmatrix} I & -r A_r^{-1} K \\ 0 & I \end{pmatrix} = C_2 D_2 C_2^T. 
\end{eqnarray*}

\begin{lemma}
Assuming $\Delta_1 \neq 0$, then by applying the transformation, we have to estimate the following: 
\begin{eqnarray*}
c_0(r) &=& \sup_{v = (X,z)} \frac{\langle R \overline{R}^{-1} R Mv, Mv \rangle}{\|v\|_{\mathcal{A}_r}^2}, \\
&=& \sup_{v = (X,z)} \frac{\langle C_1 D_1 C_1^T v,v \rangle }{\langle C_2 D_2 C_2^T v,v \rangle} = \sup_{w = (X,z)} \frac{\langle D_1 w,w \rangle }{\langle D_2 Q w, Q w \rangle} \\
&=& \sup_{w = (X,z)} \frac{\langle \Delta_1 \Delta_2 \Delta_1 X,X \rangle}{\langle A_r ( X - r[A_r^{-1} - \Delta_1^{-1}] Kz), (X - r[A_r^{-1} - \Delta_1^{-1}] Kz \rangle + r \langle (I - rA_r^{-1})Kz,Kz \rangle} \\
&=& \sup_{w = (X,z)} \frac{\langle \Delta_2 X,X \rangle}{\langle A_r (\Delta_1^{-1} X - r[A_r^{-1} - \Delta_1^{-1}] Kz), (\Delta_1^{-1} X - r[A_r^{-1} - \Delta_1^{-1}] Kz \rangle + r \langle (I - rA_r^{-1})Kz,Kz \rangle}
\end{eqnarray*}
\end{lemma} 
where 
\begin{eqnarray*}
\mathcal{A}_r &=& \begin{pmatrix}
A_r & -rK \\ 
-rK^T & rK^TK 
\end{pmatrix} \\
Q &=& C_2^T C_1^{-T} =  \begin{pmatrix} I & -r A_r^{-1} K \\ 0 & I \end{pmatrix}
\begin{pmatrix} I & r (\Delta_1)^{-1}K \\ 0 & I \end{pmatrix} = \begin{pmatrix} I & r [(\Delta_1)^{-1} - A_r^{-1} ] K \\ 0 & I \end{pmatrix}
\end{eqnarray*}
and 
\begin{eqnarray}
M^T R \overline{R}^{-1} R^TM &=& \begin{pmatrix} \Delta_1 & 0 \\ -rK^T & 0\end{pmatrix} \begin{pmatrix} \Delta_2 & 0 \\ 0 & rN I \end{pmatrix}\begin{pmatrix} \Delta_1 & -rK  \\ 0 & 0\end{pmatrix} 
\\ 
&=& \begin{pmatrix} \Delta_1 \Delta_2 & 0 \\ -rK^T \Delta_2 & 0\end{pmatrix} \begin{pmatrix} \Delta_1 & -rK  \\ 0 & 0\end{pmatrix} = \begin{pmatrix} \Delta_1 \Delta_2 \Delta_1 & -r \Delta_1 \Delta_2 K  \\ -r K^T \Delta_2 \Delta_1 & r^2 K^T \Delta_2 K \end{pmatrix} 
\end{eqnarray}

\begin{lemma}
For $n = 1$, we have 
\begin{subeqnarray*}
G_1 &=& \gamma I \\ 
\Delta_1 &=& A_r - \frac{1}{\gamma} I = - \frac{1}{\gamma} (I - \gamma A_r). \\ 
\Delta_2 &=& \left ( \frac{2}{\gamma}I - A_r \right )^{-1} = \gamma (2I - \gamma A_r)^{-1}. 
\end{subeqnarray*}
\end{lemma}
\textcolor{red}{
If I were to prove that $X = r \Delta_1 (A_r^{-1} - \Delta_1^{-1}) Kz$ is the relation between $X$ and $Kz$, that gives the supremum, then, we have that
\begin{eqnarray*}
c_0(r) &=& \sup_{w = (X,z)} r^2 \frac{\langle \Delta_2 \Delta_1 (A_r^{-1} - \Delta_1^{-1}) Kz, \Delta_1 (A_r^{-1} - \Delta_1^{-1})Kz \rangle}{
r \langle (I - rA_r^{-1})Kz,Kz \rangle} \\ 
&=& \sup_{w = (X,z)} r \frac{\langle \Delta_2 \Delta_1 (A_r^{-1} - \Delta_1^{-1}) Kz, \Delta_1 (A_r^{-1} - \Delta_1^{-1})Kz \rangle}{
\langle (I - rA_r^{-1})Kz,Kz \rangle} \\ 
&=& \sup_{w = (X,z)} r \frac{\langle \Delta_2 (I - \Delta_1 A_r^{-1}) Kz, (I - \Delta_1 A_r^{-1})Kz \rangle}{
\langle (I - rA_r^{-1})Kz,Kz \rangle} \\ 
&\leq& \sup_{w = (X,z)} \frac{r \gamma (2 - \gamma (r + L_F))^{-1} \rho(I - \Delta_1 A_r^{-1})^2 \|Kz\|_0^2}{\left (1 - \frac{r}{r+\lambda_F} \right )\|Kz\|_0^2} \\
&\leq& \sup_{w = (X,z)} \frac{r \gamma (2 - \gamma (r + L_F))^{-1} \left ( 1 - \frac{1}{\gamma (r+ L_F)} \right )^2 \|Kz\|_0^2}{\left ( 1 - \frac{r}{r+\lambda_F} \right )\|Kz\|_0^2} \\
&\leq& \sup_{w = (X,z)} \frac{r \gamma (2 - \gamma (r + L_F))^{-1} \left ( \frac{\gamma (r + L) - 1}{\gamma (r+ L_F)} \right )^2 \|Kz\|_0^2}{\left ( 1 - \frac{r}{r+\lambda_F} \right )\|Kz\|_0^2} \\ 
%&=& \sup_{w = (X,z)} r \frac{r \gamma  \left ( \frac{1 - \gamma (r + L_F)}{(\gamma (r+ L_F))^2} \right ) \|Kz\|_0^2}{\left ( 1 - \frac{r}{r+\lambda_F} \right )\|Kz\|_0^2} \\ 
\end{eqnarray*}
}

\end{remark} 
\end{comment} 

\subsection{Convergence analysis of the full deterministic Federated Learning Algorithm}
In this section, we discuss the convergence of the iterative method based on inexact Block Gauss-Seidel for $U$ block and Richardson for $H$ block. The problem we are solving is given as follows: 
\begin{subeqnarray}\label{saddle} 
\mathcal{A}_r U + \mathcal{B}^T H &=& f \\
\mathcal{B} U &=& 0. 
\end{subeqnarray}
In this section, we shall consider the inexact Uzawa iterative method for solving the equation \eqref{saddle} using the Gauss-Seidel solver or the $n-$step Gradient descent method for the matrix $\mathcal{A}_r$. Therefore, we shall write the general form of the inexact Uzawa algorithm for the equation \eqref{saddle} as the Algorithm \ref{alginexactUzawa}. 
\begin{algorithm} 
\caption{Inexact Uzawa Method formulation of FL}\label{alginexactUzawa} 
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,T$}
    \State{$X_{k+1}$ update,  
    \begin{equation}
    U_{k+1} = U_k + \mathcal{R} (f -  \mathcal{A}_r U_k -  \mathcal{B}^T H_k), 
    \end{equation}}
    \State{$H_{k+1}$ update,   
    \begin{equation} \label{Hupdate1}
        H_{k+1} = H_k + \omega \mathcal{B}U_{k+1}. 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}
Note that $\omega$ is an approximate solver for the Schur complement operator of the system, namely, 
\begin{equation}
\omega \approx A_r^{-1}, \quad \mbox{ with } \quad \frac{1}{r+L_F} I \leq A_r^{-1} \leq \frac{1}{r+\lambda_F}I.
\end{equation} 

The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + \mathcal{R} (f - \mathcal{B}^T H_k - \mathcal{A}_r U_k) \\
H_{k+1} &=& H_k + \omega \mathcal{B} U_{k+1}. 
\end{eqnarray}
We note that the exact solutions satiafy \begin{eqnarray}
U_{*} &=& U_{*} + \mathcal{R} (f - \mathcal{B}^T H_{*} - \mathcal{A}_r U_{*}) \\
H_{*} &=& H_{*} + \omega \mathcal{B} U_{*}. 
\end{eqnarray}
Thus, with the convention that $E_{k}^U = U_* - U_k$ and $E_{k}^H = H_* - H_k$, we obtain the error equations: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{R}(-\mathcal{A}_r E^U_k - \mathcal{B}^T E^H_k) \\
E^H_{k+1} &=& E^H_k + \omega \mathcal{B} E^U_{k+1}. 
\end{eqnarray}
Therefore, we have that 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{R}(- \mathcal{A}_r  E^U_k - \mathcal{B}^T E^H_k) \\
          &=& E^U_k - \mathcal{R} \mathcal{A}_r E^U_k - \mathcal{R} \mathcal{B}^T E^H_k \\
          &=& (I - \mathcal{R} \mathcal{A}_r) E^U_k - \mathcal{R} \mathcal{B}^T E_k^H \\ 
E^H_{k+1} &=& E^H_k + \omega \mathcal{B} ((I - \mathcal{R} \mathcal{A}_r) E^U_k - \mathcal{R} \mathcal{B}^T E_k^H ) \\
&=& (I - \omega \mathcal{B} \mathcal{R} \mathcal{B}^T) E_k^H + \omega \mathcal{B} (I - \mathcal{R} \mathcal{A}_r)(E_k^U). 
\end{eqnarray} 
In the matrix form, we have 
%\begin{equation}
%\begin{pmatrix} 
%I  & 0 \\
%-\omega B & I
%\end{pmatrix} 
%\begin{pmatrix} 
%E^U_{k+1} \\
%E^H_{k+1}
%\end{pmatrix}  = 
%\begin{pmatrix} 
%I - R \mathcal{A}_r & - R B^T \\
%0 & I
%\end{pmatrix} 
%\begin{pmatrix} 
%E^U_{k} \\
%E^H_{k}
%\end{pmatrix} 
%\end{equation}
%Or, we have 
\begin{equation}
\begin{pmatrix} 
E^U_{k+1} \\
E^H_{k+1}
\end{pmatrix}  = 
\begin{pmatrix} 
I - \mathcal{R} \mathcal{A}_r & - \mathcal{R} \mathcal{B}^T \\
\omega \mathcal{B} (I - \mathcal{R} \mathcal{A}_r) & I - \omega \mathcal{B} \mathcal{R} \mathcal{B}^T
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k} \\
E^H_{k}
\end{pmatrix}.  
\end{equation}
At this moment, following the work \cite{bramble1997analysis}, we shall assume there are two contraction parameters, $\rho_1(r)$ and $\rho_2(r)$ which are defined by the following inequalities:
\begin{subeqnarray}\label{assum:main2} 
\|(I - \mathcal{R}\mathcal{A}_r) \phi \|_{\mathcal{A}_r} \leq \rho_1(r) \|\phi\|_{\mathcal{A}_r} \\ 
\|(I - \omega \mathcal{S}_r)v\|_{1/\omega} \leq \rho_2(r) \|v\|_{1/\omega}. 
\end{subeqnarray}
We now present the convergence analysis result using the result contained in \cite{bramble1997analysis}. We remark that the convergence rate $\rho_1(r)$ can be made to small if $r$ is chosen small, while $\rho_2(r)$ is generally small for larger $r$. This is because of the following simple fact: 
\begin{lemma}
The following holds:
\begin{equation}
\kappa(A_r) \leq \kappa(A), \quad \forall r \geq 0. 
\end{equation} 
Furthermore, we have 
\begin{equation}
\delta = 1 - \frac{1}{\kappa(A_r)} \leq 1 - \frac{1}{\kappa(A)}.
\end{equation}
is a decreasing function in $r \geq 0$. Lastly, we have that 
\begin{equation}
\frac{\delta^{2n}}{1 - \delta^{2n}} 
\end{equation}
is a decreasing function both in $r$ and $n$. 
\end{lemma}
\begin{proof}
We observe that $\kappa(A_r) = \frac{r + L_F}{r + \lambda_F}$ is a decreasing function in $r$ for $L_F > \lambda_F$. So is $\delta$. We note that the function $\frac{x}{(1 - x)}$ is an increasing function of $x \in (0,1)$ and thus, $\delta^{2n}/(1 - \delta^{2n})$ is a decreasing function in $n$. This is also a decreasing function in $r \geq 0$. This completes the proof. 
\end{proof}
For the convergence of Federated learning algorithm, we shall require the following assumption: 
\begin{assump}\label{param}
We set $r = 0$ and let FL use the $n-$step GD and $\gamma = \frac{1}{L_F}$ and 
$\omega =  \lambda_F$. Then, we assume that $n$ satisfies the following inequality: 
\begin{equation}
n > \frac{1}{2} \frac{\log (18 \kappa^3(A))}{\log(\kappa(A)/(\kappa(A)-1))}. 
\end{equation}
\end{assump}
Note that the assumption on $n$ gives
\begin{equation}
\alpha = 2L_F \frac{\delta^{2n}}{1 - \delta^{2n}} < \frac{\lambda_F}{9\kappa^2(A)} \quad \Leftrightarrow \quad \frac{\delta^{2n}}{1 -\delta^{2n}} < \frac{1}{18 \kappa^3(A)}.  
\end{equation} 
The following lemma shows that the assumption \ref{param} establishes the compatibility condition as stated below. 
\begin{lemma}\label{compatibility} 
Under the Assumption \ref{param}, we have the following compatibility condition: 
\begin{equation}
\rho_1(r) < \frac{1 - \rho_2(r)}{3 - \rho_2(r)}.
\end{equation}
\end{lemma}
\begin{proof}
We shall first consider the assumption $\omega = \lambda_F$. Under this condition, it is easy to see that
\begin{equation}
\rho_2(r) = \rho(I - \omega A_r^{-1}) \leq 1 - \frac{1}{\kappa(A)}
\end{equation}
Thus, we have that 
\begin{equation}
(1 - \rho_2(r))^2 \geq \frac{1}{\kappa^{2}(A)}. 
\end{equation}
We shall consider the $n-$step GD case since the GS case is easier. We shall observe the following inequalities: 
\begin{eqnarray*}
\alpha < \frac{\lambda_F}
{9\kappa^2(A)} \leq \frac{ \lambda_F (1 - \rho_2(r))^2}{9} \\
\end{eqnarray*}
This gives that 
\begin{equation}
(9 - (1 - \rho_2(r))^2) \alpha  < \lambda_F ( 1 - \rho_2(r))^2. 
\end{equation}
Thus, we have 
\begin{equation}
9 \alpha < (\lambda_F + \alpha )( 1 - \rho_2(r))^2 
\end{equation}
and so, 
\begin{equation}
\sqrt{\frac{\alpha}{\alpha + \lambda_F}} 
 <  \frac{1 - \rho_2(r)}{3} \leq \frac{1 -\rho_2(r)}{3 - \rho_2(r)}. 
\end{equation}
This completes the proof. 
\end{proof}
The following theorem is presented in \cite{bramble1997analysis}, but, we provide the proof for completeness. We first introduce an inner product $[\cdot,\cdot] : \Reals{2} \mapsto \Reals{}$ defined by the following:
\begin{equation}
\left [ \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, \begin{pmatrix} x_2 \\ y_2 \end{pmatrix} \right ]  = \frac{\rho_1(r)}{1 + \rho_1(r)} x_1 x_2 + y_1 y_2, \quad \forall x_1, x_2,  y_1, y_2 \in \Reals{}.  
\end{equation}
The induced norm will be denoted by $\vertiii{\cdot}$, i.e., 
\begin{equation}
\vertiii{\begin{pmatrix} x_1 \\ y_1 \end{pmatrix}}^2 = \left [ \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, \begin{pmatrix} x_1 \\ y_1 \end{pmatrix} \right ]  = \frac{\rho_1(r)}{1 + \rho_1(r)} x_1^2 + y_1^2, \quad \forall x_1, y_1 \in \Reals{}.  
\end{equation}
More precisely, we can then establish the following theorem: 
\begin{theorem}[Bramble-Pasciak-Apostol (1997)] 
Let $\lambda_{min}(A) = \lambda_F$ and $\lambda_{max}(A) = L_F$. The parameters are chosen so that the Assumption \ref{param} holds. Then, 
Let $\{U,H\}$ be the solution pair for the equation \eqref{saddle} and $\{U_k,H_k\}$ be defined by the inexact Uzawa Algorithm \ref{alginexactUzawa}. The iterate $(U_k,H_k)$ converges to $(U_*,H_*)$, respectively. Furthermore, 
we can establish the following convergence rate: 
\begin{eqnarray}
\vertiii{ \begin{pmatrix} \|E^U_{k+1}\|_{\mathcal{A}_r} \\
\|E^H_{k+1}\|_{1/\omega} \end{pmatrix} }^2 \leq \rho^{2} \vertiii{\begin{pmatrix} \|E^U_{k}\|_{\mathcal{A}_r} \\\|E^H_{k}\|_{1/\omega}\end{pmatrix} }^2.
\end{eqnarray}
where $\rho_1$ and $\rho_2$ are convergence factors defined in \eqref{assum:main2}
and $\rho$ is given as follows: 
\begin{equation}
\rho = \frac{2\rho_1(r) + \rho_2(r) + \sqrt{ (2\rho_1(r) + \rho_2(r))^2 + 4\rho_1(r)(1-\rho_2(r))}}{2}. 
\end{equation} 
Further, we have 
\begin{eqnarray}
\|E_k^U\|^2_{\mathcal{A}_r} \leq (1 + \rho_1(r))(1 + 2\rho_1(r))\vertiii{\begin{pmatrix} \|E^U_{k}\|_{\mathcal{A}_r} \\\|E^H_{k}\|_{1/\omega}\end{pmatrix} }^2.%\rho^{2k-2} \left ( \frac{\rho_1(r)}{1 + \rho_1(r)} (\mathcal{A}_r E_0^U, E_0^U) + (E_0^H, E_0^H)_{1/\omega} \right ),  
\end{eqnarray}
where 
\end{theorem}
\begin{proof} 
We begin by noting that  
\begin{eqnarray*}
E^U_{k+1} &=& E^U_k - \mathcal{R} (\mathcal{A}_r  E^U_k + \mathcal{B}^T E^H_k) \\
          &=& (\mathcal{A}_r^{-1} - \mathcal{R})(\mathcal{A}_r E^U_k + \mathcal{B}^T E_k^H) - \mathcal{A}_r^{-1} \mathcal{B}^T E^H_k \\
E^H_{k+1} &=& E^H_k + \omega \mathcal{B} E^U_{k+1}. 
\end{eqnarray*} 
For $E_{k+1}^H$, we have that 
\begin{eqnarray*}
E^H_{k+1} &=& (I - \omega \mathcal{B} \mathcal{R} \mathcal{B}^T) E_k^H + \omega \mathcal{B} (I - \mathcal{R} \mathcal{A}_r)(E_k^U) \\ 
&=& (I - \omega \mathcal{B} \mathcal{A}_r^{-1} \mathcal{B}^T) E_k^H + \omega \mathcal{B} (\mathcal{A}_r^{-1} - \mathcal{R})(\mathcal{A}_r E_k^U + \mathcal{B}^T E_k^H). 
\end{eqnarray*} 
Thus, we can write the following equation: 
\begin{eqnarray}
\begin{pmatrix} E^U_{k+1} \\ E^H_{k+1} \end{pmatrix} 
= \mathcal{M}_r 
\begin{pmatrix} E^U_{k} \\ E^H_{k} \end{pmatrix}, 
\end{eqnarray} 
where
\begin{eqnarray}\label{mbar} 
\mathcal{M}_r = \begin{pmatrix} 
(\mathcal{A}_r^{-1} - \mathcal{R})\mathcal{A}_r & (\mathcal{A}_r^{-1} - \mathcal{R})\mathcal{B}^T - \mathcal{A}_r^{-1} \mathcal{B}^T \\
\omega \mathcal{B} (\mathcal{A}_r^{-1} - \mathcal{R}) \mathcal{A}_r & (I - \omega \mathcal{B} \mathcal{A}_r^{-1} \mathcal{B}^T) + \omega \mathcal{B}(\mathcal{A}_r^{-1} - \mathcal{R}) \mathcal{B}^T 
\end{pmatrix}. 
\end{eqnarray}
We now let 
\begin{equation}
\mathcal{D}_r = \begin{pmatrix} 
\mathcal{A}_r & 0 \\ 
0 & \frac{1}{\omega} 
\end{pmatrix}. 
\end{equation} 
Then, we shall need to compute $
\|\mathcal{M}_r\|_{\mathcal{D}_r}$ to estimate the convergence rate. This can be done from the following two estimates: 
\begin{subeqnarray*}
\|E^U_{k+1}\|_{\mathcal{A}_r} &=& \|(\mathcal{A}_r^{-1} - \mathcal{R})(\mathcal{A}_r E^U_k + \mathcal{B}^T E_k^H) - \mathcal{A}_r^{-1} \mathcal{B}^T E^H_k \|_{\mathcal{A}_r} \slabel{eu} \\ 
\|E^H_{k+1}\|_{1/\omega} &=& \|(I - \omega \mathcal{B} \mathcal{A}_r^{-1} \mathcal{B}^T) E_k^H + \omega \mathcal{B} (\mathcal{A}_r^{-1} - \mathcal{R})(\mathcal{A}_r E_k^U + \mathcal{B}^T E_k^H)\|_{1/\omega}. \slabel{es}
\end{subeqnarray*}
First, by the triangle inequality together with the Assumption \eqref{param}, we have that 
\begin{eqnarray}
\|E^U_{k+1}\|_{\mathcal{A}_r} \leq \rho_1(r) \|E_k^U\|_{\mathcal{A}_r} + (1 + \rho_1(r)) \|E_k^H\|_{1/\omega}. 
\end{eqnarray}
On the other hand, the Assumption \eqref{param}, we have that 
\begin{eqnarray*}
\|E_{k+1}^H\|_{1/\omega} &\leq& \rho_2(r) \|E_k^H\|_{1/\omega} + \|\omega \mathcal{B} (\mathcal{A}_r^{-1} - \mathcal{R})(\mathcal{A}_r E_k^U + \mathcal{B}^T E_k^H)\|_{1/\omega} \\ 
&\leq& \rho_2(r) \|E_k^H\|_{1/\omega} + \|(\mathcal{A}_r^{-1} - \mathcal{R})(\mathcal{A}_r E_k^U + \mathcal{B}^T E_k^H)\|_{\mathcal{A}_r}, \quad \mbox{ since } \omega \leq r + \lambda_F \\ 
&\leq& \rho_2(r) \|E_k^H\|_{1/\omega} + \rho_1(r) \left ( \|E_k^U\|_{\mathcal{A}_r} + (\mathcal{B}\mathcal{A}_r^{-1} \mathcal{B}^T E_k^H, E_k^H)^{1/2} \right ) \\ 
&\leq& (\rho_1(r) + \rho_2(r)) \|E_k^H\|_{1/\omega} + \rho_1(r) \|E_k^U\|_{\mathcal{A}_r}. 
\end{eqnarray*}
We now let 
\begin{equation}
M = \begin{pmatrix} 
\rho_1(r) & 1 + \rho_1(r) \\ \rho_1(r) & \rho_2(r) + \rho_1(r) 
\end{pmatrix}. 
\end{equation} 
Then, $M$ is symmetric with respect to the inner product $\left [ \cdot, \cdot \right ]$. Thus, we have 
\begin{equation}
\vertiii{\begin{pmatrix} \|E^U_{k+1}\|_{\mathcal{A}_r} \\
\|E^H_{k+1}\|_{1/\omega} \end{pmatrix}}^2 \leq \vertiii{ 
M \begin{pmatrix} \|E^U_{k}\|_{\mathcal{A}_r} \\
\|E^H_{k}\|_{1/\omega} \end{pmatrix}}^2 \leq \rho^2(M) \vertiii{ 
\begin{pmatrix} \|E^U_{k}\|_{\mathcal{A}_r} \\
\|E^H_{k}\|_{1/\omega} \end{pmatrix}}^2. 
\end{equation}
The spectral radius of $M$ can be obtained from the following characteristic polynomial roots: 
\begin{equation}
\lambda^2 - (2\rho_1(r) + \rho_2(r))\lambda - \rho_1(r) (1 - \rho_2(r)) = 0. 
\end{equation} 
It is clear to see that the spectral radius $\rho$ is an increasing function of $\rho_1(r)$ for any fixed $\rho_2(r) \in [0,1]$. Moreover, we have that
\begin{equation}
\rho = 1, \quad \mbox{ for } \quad \rho_1(r) = \frac{1 - \rho_2(r)}{3 -\rho_2(r)} \quad \mbox{ and } \quad \rho_2(r) \in [0,1]. 
\end{equation} 
Note that the Assumption \ref{param} guarantees the following holds that 
the contraction factors $\rho_1(r)$ and $\rho_2(r)$ satisfy the compatibility condition (see Lemma \ref{compatibility}): 
\begin{equation}\label{cond:main}
\rho_1(r) < \frac{1 - \rho_2(r)}{3 - \rho_2(r)}. 
\end{equation}
Thus, we obtain the convergence and the convergence factor is then given by 
\begin{equation}
\rho = \frac{2\rho_1(r) + \rho_2(r) + \sqrt{ (2\rho_1(r) + \rho_2(r))^2 + 4\rho_1(r)(1-\rho_2(r))}}{2}. 
\end{equation} 
The convergence estimate is given as follows: 
\begin{eqnarray*}
\vertiii{\begin{pmatrix} \|E_{k+1}^U\|_{\mathcal{A}_r} \\ \|E_{k+1}^H\|_{\omega^{-1}} \end{pmatrix}}^2 \leq \rho^{2} 
\vertiii{\begin{pmatrix} \|E_k^U\|_{\mathcal{A}_r} \\
\|E_k^H\|_{\omega^{-1}} \end{pmatrix}}^2.  
\end{eqnarray*}
We now consider the inequality that 
\begin{eqnarray*}
&& \|E^U_{k+1}\|_{\mathcal{A}_r}^2 \leq \left ( \rho_1(r) \|E_k^U\|_{\mathcal{A}_r} + (1 + \rho_1(r)) \|E_k^H\|_{1/\omega} \right )^2 \\
&& \qquad \leq \rho_1(r)^2 \|E_k^U\|_{\mathcal{A}_r}^2 + (1 + \rho_1(r))^2 \|E_k^H\|_{1/\omega}^2 \\
&& \qquad \qquad + 2 \rho_1(r) (1 + \rho_1(r)) \|E_k^U\|_{\mathcal{A}_r} \|E_k^H\|_{1/\omega}^2 \\
&& \qquad \leq (1 + \epsilon) \rho_1(r)^2 \|E_k^U\|_{\mathcal{A}_r}^2 + (1 + \epsilon^{-1}) (1 + \rho_1(r))^2 \|E_k^H\|_{1/\omega}^2,   
\end{eqnarray*}
where $\epsilon > 0$. We choose $\epsilon = 1 + 1/\rho_1(r)$. Then, we have 
\begin{eqnarray*}
\|E^U_{k+1}\|_{\mathcal{A}_r}^2 &\leq& (1 + 2 \rho_1(r)) \rho_1(r) \|E_k^U\|_{\mathcal{A}_r}^2 + (1 + 2\rho_1(r))(1 +  \rho_1(r)) \|E_k^H\|_{1/\omega}^2 \\ 
&\leq&(1 + 2 \rho_1(r)) (1 + \rho_1(r)) \vertiii{\begin{pmatrix} \|E_k^U\|_{\mathcal{A}_r} \\
\|E_k^H\|_{\omega^{-1}} \end{pmatrix}}^2.
\end{eqnarray*}
This establishes the convergence of $E_k^U$ in $\mathcal{A}_r$ norm. This completes the proof. 
\end{proof} 
\begin{remark}
We remark that the choice of $r$ has to be made in a way that $\rho_1(r)$ and $\rho_2(r)$ well-balanced. For example, for $\omega$ given as follows:
\begin{equation}
\omega = \frac{2}{\lambda_{min}(\mathcal{S}_r) + \lambda_{max}(\mathcal{S}_r)},
\end{equation}
the convergence rate $\rho_2(r)$ is given as follows: 
\begin{equation}
\rho_2(r) = \frac{\kappa(\mathcal{S}_r) - 1}{\kappa(\mathcal{S}_r) + 1} = \frac{\kappa(A_r) - 1}{\kappa(A_r) + 1} = \frac{L_F - \lambda_F}{2r + L_F + \lambda_F}, 
\end{equation}
while we can show that the estimate for $\rho_1(r)$ can be given as follows: 
\begin{equation}
\|(I - \mathcal{R} \mathcal{A}_r) \phi \|_{\mathcal{A}_r}^2 \leq \left (1 - \frac{1}{1 + c_0} \right )  \|\phi\|_{\mathcal{A}_r}^2 
\end{equation} 
with $\rho_1(r)$ being given as follows:
\begin{equation}
\rho_1(r) = 1 - \frac{1}{1 + c_0} = \sqrt{\frac{ r + \alpha }{ r + \alpha + \lambda_F}}. 
\end{equation} 
We then observe that $\rho_2(r)$ becomes smaller for the larger $r$ while $\rho_1(r)$ becomes larger for the larger $r$. 
\end{remark} 
\begin{remark} 
The improvement can not possibly be made since it is the optimal choice for the Richardson method. One many consider to use $r$ to improve the convergence. However, by enlarging $r$, we should be concerned with the convergence factor $\rho_1(r)$, which is given as 
\begin{equation}
\rho_1(r) = \frac{r}{r + \lambda_F}. 
\end{equation}
Therefore, unless we introduce $m-$ solve of the first block, we can not expect to make $\rho_1(r)$ small. Furthermore, by $m-$solve, we mean already $m-$ communications to update $Kz$ variables. 
%\begin{equation} 
%\rho(I - \omega \mathcal{S}) = 1 - \frac{1}{n^\nu} = 1 - \frac{1}{\sqrt{\kappa(F)}}.
%\end{equation}
%We note that 
%\begin{equation}
%\sigma(I - \omega \mathcal{S}) = \left \{ 1 - \frac{L_F}{n^\nu \lambda} : \lambda \in %A \right \}. 
%\end{equation}
%This gives an optimal convergence rate under the assumption that $\rho_1(r)$ is %negligible. This completes the proof. 
\end{remark} 


\import{./}{nonlin.tex} 


%Our discussion above indicates that the convergence is crucially dependent on the smallness of $\rho_1(r)$, whose outcome is mainly due to \cite{xu2002method}.

%\begin{theorem}
%We assume that $n$ is sufficiently large and $r$ is sufficiently small so that the convergence rate of FL reduces to that of $\rho_2(r)$. Thus if we choose parameters $\gamma, r$ and $\omega$ satisfy the Assumption \ref{param}, then the FL algorithm converges with the following rate: 
%\begin{equation}
%\rho = \epsilon(r) + 1 - \frac{1}{\kappa(A)}. 
%\end{equation}
%\end{theorem} 
%\begin{remark}
%For stochastic algorithm, it was assumed that 
%\begin{equation}\label{assumQb2} 
%\omega = \frac{\lambda_F}{n} \quad \mbox{ and } \quad n = \lceil \sqrt{\kappa(A_r)} \rceil,
%\end{equation} 
%and the convergence rate $\rho(r)$ has been estimated as follows: 
%\begin{eqnarray}
%\rho = \epsilon(r) + \rho_2(r) = \epsilon(r) + \frac{\sqrt{\kappa(A)} - 1}{\sqrt{\kappa(A)}}, 
%\end{eqnarray} 
%where $\epsilon(r)$ is due to $\alpha$ and $r$. On the other hand, such a proof can not be made for the deterministic case, since we must assume $n$ is sufficiently large so that we can ignore the effect of $\rho_1(r)$. In fact, if we choose $\omega = \lambda_F/n$. then the convergence will deteriorate for large $n$ in general. More precisely, if we choose $\omega$ as 
%\begin{equation} 
%\omega = \frac{\lambda_F}{n}.
%\end{equation}
%Then, the convergence is guaranteed for $\omega < 2 \lambda_F$ to handle the Schur complement operator $\mathcal{S} = A^{-1}$. A simple calculation leads to the following: 
%\begin{equation}
%\rho(I - \omega \mathcal{S}) = 1 - \frac{1}{n} = \frac{n - 1}{n}.
%\end{equation} 
%Therefore, if we choose $n = \sqrt{\kappa(A)}$, then we obtain an optimal convergence rate under the assumption that $\rho_1(r)$ is negligible. If not, with $n$, the convergence of $\rho_2(r)$ will go to one with $n$ large. Therefore, it is desired to choose $\omega$ independent on $n$ for the optimal performance. It is unlike the result established for the stochastic ProxSkip Algorithm in \cite{mishchenko2022proxskip}. 
%\end{remark}

%\begin{remark} 
%Since we have that with $\omega = \frac{2}{r + L_F + r + \lambda_F}$, 
%\begin{eqnarray*}
%\lambda_{max}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+L_F} = \frac{r + L_F - \omega}{r + L_F} \\  
%\lambda_{min}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+\lambda_F} = \frac{r %+ \lambda_F - \omega}{r + \lambda_F}.
%\end{eqnarray*}
%Thus, we have that
%\begin{equation}
%\frac{\kappa(A_1) - 1}{\kappa(A_1) + 1} = \frac{ \frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} - 1}{\frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} + 1} =\frac{ \frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} - 1}{\frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} + 1} = \frac{\frac{r + \lambda_F}{r + L_F}  - \frac{\lambda_F}{L_F}}{\frac{r + \lambda_F}{r + L_F} + \frac{\lambda_F}{L_F}}  
%\end{equation} 
%and 
%\begin{equation}
%\frac{\kappa(A_2) - 1}{\kappa(A_2) + 1} = \frac{ \frac{r + \lambda_F}{r + L_F} - 1}{\frac{r + \lambda_F}{r + L_F}  + 1}  
%\end{equation}
%On the other hand, we have that 
%\begin{equation}
%\sin (\theta_1 + \theta_2) = 
%\end{equation}
%and $\lambda_{min} = 1 - \frac{\omega}{r + \lambda_F} = \frac{\lambda_F}{r + \lambda_F}$ while $H_{A_r} = 1/(r + L_F)$ and $1/(r + \lambda_F)$. Therefore, if $r$ is sufficiently large, then $\sin\theta$ is quite small. 
%\begin{theorem}
%For $A_1$ and $A_2$, symmetric positive definite matrices, we have 
%\begin{equation}
%(x,A_1A_2y) \leq \sin (\theta_1 + %\theta_2) \|A_1x\|\|A_2y\|,  
%\end{equation}
%where $\sin \theta_1 = \frac{\kappa(A_1) - 1}{\kappa(A_2) + 1}$ and $\sin \theta_2 = \frac{\kappa(A_2) - 1}{\kappa(A_2) + 1}$. 
%\end{theorem}
%\end{remark}

\begin{comment} 
\subsubsection{Convergence analysis of Algorithm \ref{algADMM3} with $G_{n,r} = A_r^{-m}$}
In this section, we shall discuss the case when the exact solver for $A_r$ is applied a number of times, say $m$. The main motivation behind this investigation is to use the action of the parameter $r$. 

\begin{theorem}\label{main:theorem04} 
The Algorithm \ref{algADMM3} with $G_{n,r} = A_r^{-m}$ produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 + \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM3} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^{-m}(H_k + rKz_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ). 
\end{eqnarray*}
We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z = K(K^TK)^{-1}K$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& A_r^{-m}(H_k + rKz_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T A_r^{-m}(H_k + rKz_k)) = P_Z [A_r^{-m} (H_k + rKz_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{-m} (H_*+rKz_*) \\
Kz_{*} &=& P_Z [A_r^{-m}(H_*+rKz_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{-m} (H_* + rKz_*) - A_r^{-m}(H_k + rKz_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{-m} (H_*+rKz_*) - A_r^{-m}(H_k + rKz_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH2}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{-m} (H_*+rKz_*) - A_r^{-m}(H_k +rKz_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH2}, and using the orthogonality between $E_i^H$ and $E_j^Z$ for all $i,j$, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{-m}(H_*+rKz_*) - A_r^{-m}(H_k+rKz_k))\|^2. 
\end{eqnarray*}
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^{-m}(H_k + rKz_*) - A_r^{-m}(H_k + rKz_k). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{-m} (H_* + r K z_*) - A_r^{-m} (H_k + r K z_k))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^{-m}} = 1/(r + L_F)^m$ and $L_{G^{-m}} = 1/(r + \lambda_F)^m$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{(r + L_F)^m} + \frac{1}{(r + \lambda_F)^m}} = \frac{2 (r + \lambda_F)^m(r + L_F)^m}{(r + L_F)^m + (r + \lambda_F)^m}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right ) \frac{r}{(r+\lambda_F)^m} \|E_k^H\|\|E_k^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right ) \frac{r}{(r+\lambda_F)^m}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^{-m}} + L_{G^{-m}})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^{-m}(H_k + r Kz_*) - A_r^{-m} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{(r+\lambda_F)^m} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 + \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\end{comment} 

\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}

We remark that 
\begin{equation}
\frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} = \frac{\frac{(r+L_F)^m}{(r+\lambda_F)^m} - 1}{\frac{(r+L_F)^m}{(r+\lambda_F)^m}+1} = \frac{(r+L_F)^m - (r+\lambda_F)^m}{(r+L_F)^m + (r+\lambda_F)^m}. 
\end{equation} 
This provides the convergence rate that speeds up with the larger $r$ now. 
\end{proof} 


\end{comment} 

\bibliographystyle{plain}
\bibliography{mybib}
\end{document} 

\section{appendix} 

With $\mathcal{M}_r$, 
\begin{eqnarray*}
\mathcal{M}_r = \begin{pmatrix} 
(\mathcal{A}_r^{-1} - \mathcal{R})\mathcal{A}_r & (\mathcal{A}_r^{-1} - \mathcal{R})\mathcal{B}^T - \mathcal{A}_r^{-1} \mathcal{B}^T \\
\omega \mathcal{B} (\mathcal{A}_r^{-1} - \mathcal{R}) \mathcal{A}_r & (I - \omega \mathcal{B} \mathcal{A}_r^{-1} \mathcal{B}^T) + \omega \mathcal{B}(\mathcal{A}_r^{-1} - \mathcal{R}) \mathcal{B}^T 
\end{pmatrix}, 
\end{eqnarray*}
we have that 
{\small{\begin{eqnarray*}
\mathcal{M}_r^T \mathcal{M}_r &=& \begin{pmatrix} 
I - A R^T & \omega (I - AR^T) B^T \\
- B R^T & I - \omega BR^TB^T  
\end{pmatrix} \begin{pmatrix} 
I - R A & - RB^T \\
\omega B(I - R A) & I - \omega BRB^T  
\end{pmatrix} \\ 
&=& \begin{pmatrix} 
(I - A R^T)(I - RA) + \omega^2 (I - AR^T) B^TB(I - RA) & - (I - AR^T)RB^T + \omega (I - AR^T) B^T ( I - \omega BRB^T) \\
- BR^T (I - RA) + \omega (I - \omega BR^TB^T)B(I - RA) & BR^T RB^T + (I - \omega BR^TB^T)(I - \omega BR B^T)
\end{pmatrix} \\
&=& 
\begin{pmatrix}
M_{11} & M_{12} \\
M_{21} & M_{22}
\end{pmatrix} \\
&\approx& \begin{pmatrix} 
(I - A R^T)(I - RA) + \omega^2 (I - AR^T) B^TB(I - RA) & 0 \\
0 & M_{22} - M_{21} M_{11}^{-1} M_{12} 
\end{pmatrix}. 
\end{eqnarray*}}}
Thus, we shall investigate the following:
\begin{equation}
\lambda_{min}(M_{11}) \leq M_{11} \leq \lambda_{max}(M_{11}). 
\end{equation} 

Spectrum can be obtained by the Schur complement.



\subsection{Note to reminder} 

\textcolor{red}{Jinchao's Suggestion : 
We define 
\begin{equation}
S_r^n = B R_n B^T 
\end{equation}
Then, we have with $g = BA_r^{-1} f$,  
\begin{subeqnarray*}
H_{k+1} &=& H_k + \omega ( g - S_r^n H_k) \\ 
H_* &=& H_* + \omega (g - S_r H_*). 
\end{subeqnarray*}
If $R = A_r^{-1}$, then, it is true that 
$$xxx 
U_{k+1} = U_k + R (f - B^T H_k - A_r U_k) = A_r^{-1}f - A_r^{-1} B^T H_k. 
$$
Thus, we have that 
$$
H_{k+1} = H_k + \omega (BA_r^{-1} f - S_r H_k). 
$$
On the other hand, if $R \neq A_r^{-1}$, then we have that
$$
H_{k+1} = H_k + \omega B U_{k+1} = H_k + \omega (BRf + B(I - RA_r)U_k - BR B^T H_k).  
$$
Therefore, we will have a different $g$ here. 
}

\section{Exact Uzawa Derivation}
The Exact Uzawa is given as follows:  
\begin{subeqnarray*} 
\mathcal{A}_r U_{k+1} &=& f - \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
This is equivalent to say that 
\begin{subeqnarray*} 
U_{k+1} &=& U_k + \mathcal{A}_r^{-1} ( f - \mathcal{B}^T H_k - \mathcal{A}_r U_k) = (I - \mathcal{A}_r^{-1} \mathcal{A}_r)U_k + \mathcal{A}_r^{-1} (f - \mathcal{B}^TH_k) \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
The last equation is due to the fact that
\begin{equation}
\mathcal{B} \mathcal{A}_r^{-1} f- \mathcal{S}_r H_k = \mathcal{B} \mathcal{A}_r^{-1} (f - \mathcal{B}^T H_k - (I - \mathcal{A}_r^{-1} \mathcal{A}_r) U_k) = \mathcal{B}U_{k+1}.  
\end{equation} 
\begin{remark}
The operator $\mathcal{A}_r$ will be directly inverted and thus, independent of $k$, the iterate count, the action of $\mathcal{A}_r$ does not require any initial condition such as $U_k$.  
\end{remark}

\section{Inexact Uzawa Derivation} 
The inexact Uzawa is to introduce $\mathcal{R} \approx \mathcal{A}_r^{-1}$, but, it is dependent on $U_k$,   
\begin{subeqnarray*} 
\mathcal{R}_k^{-1} U_{k+1} &=& f - \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
This is equivalent to say that 
\begin{subeqnarray*} 
U_{k+1} &=& U_k + \mathcal{R}( f - \mathcal{B}^T H_k - \mathcal{A}_r U_k) = (I - \mathcal{R} \mathcal{A}_r)U_k + \mathcal{R} (f - \mathcal{B}^TH_k) \\ 
H_{k+1} &=&  \cancel{H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k)} \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
The last equation leads to 
\begin{equation}
H_{k+1} = H_k + Q_B \mathcal{B} U_{k+1} = H_k + Q_B \left [  \mathcal{B} ( (I - \mathcal{R}\mathcal{A}_r)U_k + \mathcal{R}(f - \mathcal{B}^T H_k) \right ]. 
\end{equation} 

Another derivation of the inexact uzawa is to introduce a approximate solver $\mathcal{R} \approx \mathcal{A}_r^{-1}$, which is assumed to be linear:   
\begin{subeqnarray*} 
\mathcal{R}^{-1} \overline{U} + \mathcal{B}^T \overline{H} &=& f \\ 
\mathcal{B} \overline{U} &=& 0. %H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
%&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
Namely, we consider an approximate equation: 
\begin{equation} 
\overline{U} = \mathcal{R} ( f - B^T \overline{H}) \quad \mbox{ and } \quad B \mathcal{R} B^T \overline{H} = \mathcal{R} f.  
\end{equation} 
We then consider the Uzawa for the new system:
\begin{subeqnarray*} 
\mathcal{R}^{-1} \overline{U}_{k+1} &=& f - \mathcal{B}^T \overline{H}_k \\ 
\overline{H}_{k+1} &=& \overline{H}_k + Q_B ( \mathcal{B} \mathcal{R}f - \mathcal{S}^n_r \overline{H}_k) \\
&=& \overline{H}_k + Q_B \mathcal{B} \overline{U}_{k+1}.  
\end{subeqnarray*}
Thus, the error equation is given as follows: 
\begin{eqnarray*}
U_{k+1} - \overline{U}_{k+1} = \mathcal{A}_r^{-1} (f - \mathcal{B}^T H_k) - \mathcal{R} (f - \mathcal{B}^T \overline{H}_k) = (\mathcal{A}_r^{-1} - \mathcal{R})f 
\end{eqnarray*}
Therefore, we have
\begin{equation}
B \mathcal{R} B^T (H- \overline{H}) = (\mathcal{A}_r^{-1} -  R)f - B (\mathcal{A}_r^{-1} - R) B^TH. 
\end{equation} 
On the other hand, we have that
\begin{eqnarray*}
U - \overline{U} &=& \mathcal{A}_r^{-1} ( f - B^T H) - R (f - B^T \overline{H} ) = (\mathcal{A}_r^{-1} - R)f - \mathcal{A}_r^{-1} B^T H + R B^T \overline{H} \\
&=& (\mathcal{A}_r^{-1} - R)f - \mathcal{A}_r^{-1} B^T (H - \overline{H}) + (R - \mathcal{A}_r^{-1} )B^T \overline{H}. 
\end{eqnarray*}








\begin{subeqnarray*} 
U_{k+1} &=& \overline{\mathcal{R}}( f - \mathcal{B}^T H_k ) = \overline{\mathcal{R}} f - \overline{\mathcal{R}} \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}

\end{document} 


This is equivalent to say that 
\begin{subeqnarray*} 
U_{k+1} &=& U_k + \mathcal{R}( f - \mathcal{B}^T H_k - \mathcal{A}_r U_k) = (I - \mathcal{R} \mathcal{A}_r)U_k + \mathcal{R} (f - \mathcal{B}^TH_k) \\ 
H_{k+1} &=& \textcolor{red}{Skip this !} H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
The last equation leads to

\end{document} 

\section{Convergence analysis of  Algorithm \ref{algADMM1}}

In this section, we shall present the convergence of the Algorithm \ref{algADMM1}. This section consists of two parts. One part is the convergence when $D_r^* = A_r^*$ and the other part is the convergence when $D_r^*$ is the $N-$step GD method. 

\subsection{Linear Convergence of Algorithm \ref{algADMM1} for $D_r = A_r$}

In this section, we shall establish the linear convergence of the exact ADMM method with $D_r = A_r$.
\begin{comment} 
\begin{lemma}
The following holds true: for all $Y, Y_k$ in $\Reals{N_x}$, 
\begin{eqnarray}
\|A_r^*(Y) - A_r^*(Y_k) \| &\leq& \frac{1}{r + \lambda_F} \|Y- Y_k\|, \\ 
\|(I - \omega A_r^*) (Y) - (I - \omega A_r^*) (Y_k)   \| &\leq& \left(1 - \frac{\omega}{r + L_F} \right) \|Y -Y_k \|
\end{eqnarray}
\end{lemma}
\begin{proof}
    Note that $A_r^*(Y)$ corresponds to the gradient of the function $F_r^*(Y)$ that is the dual of $F_r(X) = F(X) + \frac{r}{2} \| X\|^2$. 
    $F_r^*$ is $\frac{1}{r +L_F}$-strongly convex and $\frac{1}{r + \lambda_F}$-smooth since $F_r$ is $(r + \lambda)$-strongly convex and $(r + L_F)$-smooth.
    Therefore, we have 
    \begin{equation}
        \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|. 
    \end{equation}

For $ (I -  \omega A_r^*)(Y)$, we have
 that the 
 \begin{equation}
 \begin{aligned}
    \|  I - \omega \nabla^2F^*_r(Y) \| 
 = \rho ( I - \omega \nabla^2F^*_r(Y)) & \leq \max \left\{|1 - \omega \lambda_{min}(\nabla^2 F_r^*(Y))|, | 1-\omega \lambda_{max} \nabla^2 F_r^*(Y)| \right\} \\
 & \leq \max \left\{  |1 - \frac{\omega}{r + L_F} |, | 1-\frac{\omega}{r + \lambda_F}| \right\}
 \end{aligned}
 \end{equation}
 where $\omega < \frac{2}{ \frac{1}{r + \lambda_F}}$ for any $Y$. 
 For optimal $\omega$ is $\frac{2}{ \frac{\omega}{r + \lambda_F}+ \frac{\omega}{r + L_F}}$, which leads to 
 \begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa + 1} \|Y - Y_k \|,
\end{equation}
where $\kappa = \frac{r+L_F}{r+\lambda_F}$. 

In other analysis where we restrict $\omega \leq \frac{1}{ \frac{1}{r + \lambda_F} }$, we have the following optimal bound
\begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa} \|Y - Y_k \|.
\end{equation}
\end{proof}
\begin{remark}
    This Hessian argument always requires $L$-smoothness of the objective thanks to the mollifier argument. See Lemma \ref{lemmaGD}.
\end{remark}
\end{comment} 

% \begin{proof}
% Let $X = A_r^*(Y)$, $X_k = A_r^*(Y_k)$, that is $Y  = A_r(X)$, $Y_k = A_r(X)$
% \begin{eqnarray*}
% (r+\lambda) \|X - X_{k}\|^2 &\leq& \langle X - X_{k+1}, A_r(X) - A_r(X_{k+1}) \rangle \\
% &=& \langle X - X_k, Y - Y_k \rangle \\ 
% &\leq& \|X - X_k\|  \|Y - Y_k\|. 
% \end{eqnarray*}
% Hence, 
% \begin{eqnarray*}
% \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|.   
% \end{eqnarray*}
% This completes the proof.
% \end{proof}

\begin{theorem}\label{thm:GS}
The Algorithm \ref{algADMM1} with $D_r = A_r$ and $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ has the convergence rate given as follows: 
\begin{eqnarray}\label{gsrate}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \rho^2_{GS}(r,L_F,\lambda_F) \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ), 
\end{eqnarray}
where with $\kappa(G) = \frac{r+L_F}{r + \lambda_F}$, 
\begin{equation} 
\rho^2_{GS}(r,L_F,\lambda_F) =  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2. 
\end{equation} 
\textcolor{red}{The first one is from error in Schur Complement and the second one is from error between exact $X$ and the $X_k$. The expression is from the case when GS is used. I am thinking how it will be like for linear analysis.}


We also have that 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ). 
\end{eqnarray*}
Furthermore, there exists a single optimal $r > 0$ which gives the optimal convergence rate and the convergence factor is always smaller than one for all $r \geq 0$. 
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM1} produces iterates given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^* (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $A_r^*$ is the Fenchel-dual conjugate of $A_r$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. Now due to Lemma \ref{main:lem1}, we have that with $D_r = A_r$,  
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 %\\   
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\begin{comment} 
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{*} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{*}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ]. 
\end{eqnarray*}
The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] \right ). 
\end{eqnarray*}
Lastly, for $H$, we have 
\begin{eqnarray*}
H_{*} - H_{k+1} &=& H_* - H_k + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
&=& H_* - H_k - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
&=& H_* - H_k - \omega [ A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
&& - P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] ] \\
&=& H_* - H_k - \omega Q_Z (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)) \\ 
&=& Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))] 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
H_{*} - H_{k+1} - \omega (Kz_* - K z_{k+1}) = H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)). 
\end{eqnarray*}
\end{comment} 
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^*(H_k + rKz_*) - A_r^*(H_k + rKz_k). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^*} = 1/(r + L_F)$ and $L_{G^*} = 1/(r + \lambda_F)$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{r + L_F} + \frac{1}{r + \lambda_F}} = \frac{2 (r + \lambda_F)(r + L_F)}{2r + L_F + \lambda_F}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F} \|E_k^H\|\|E_k^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{r + \lambda_F} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^*} + L_{G^*})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^*(H_k + r Kz_*) - A_r^{*} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r + \lambda_F} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}
\end{comment} 
This provides the convergence rate. Finally, we notice that for all $r \geq 0$, 
\begin{eqnarray*}
\frac{r^2}{\omega^2} = \frac{r^2 (2r + L_F + \lambda_F)^2}{4(r + \lambda_F)^2(r + L_F)^2} < 1. 
\end{eqnarray*}
Thus, we obtain that due to the orthogonality, 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 &=& \|A_r^*(H_* + rKz_*) - A_r^*(H_k + r K z_k) \|^2 \\
&\leq& \frac{1}{(r + \lambda_F)^2}\|E_k^H - r E_k^Z\|^2 = \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + r^2 \|E_k^Z\|^2 \right ) \\
&=& \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \frac{r^2}{\omega^2} \|E_k^Z\|_\omega ^2 \right ) \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) 
\end{eqnarray*}
We now discuss the convergence factor denoted by $\rho_{GS}^2$ and given by 
\begin{equation}
f(r) = \rho^2_{GS}(r, L_F, \lambda_F) = \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 = \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 
\end{equation}
We note that the simple calculation shows that the derivative of $f(r)$ is given as follows: 
\begin{equation}
f'(r) = \frac{-4(L-\lambda)^2}{(2r + L_F + \lambda_F)^3} + \frac{2r \lambda_F}{(r + \lambda_F)^3}. 
\end{equation}
Therefore, for small $r$, it takes the negative sign, but it changes its sign for larger $r$ and continues to be positive. Thus, there exists a single critical point, which gives the optimal $r_{\rm opt}$. On the other hand, it is continuously increasing (see Figure \ref{exam}). Since $\lim_{r \rightarrow \infty} f(r) = 1$ and thus, for any $r \geq 0$, the convergence factor is smaller than one. Thus, it converges linearly for all fixed $r \geq 0$. This completes the proof.
\end{proof} 
\begin{figure}[h]
\centering
\includegraphics[width=12cm,height=6cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
\begin{comment} 

\begin{remark}[Convergence rate when $\omega = r$]
In our current convergence analysis, we require $\beta \in \left( 0,\frac{1}{2} \right )$. We shall make a more precise convergence analysis as follows.
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{ L_F r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{L_F }{r + L_F} \right )^2 \right ) &=& \left ( \frac{ L_F  r^{2\alpha} (r + L_F)}{(r + L_F)^2(r + \lambda_F)} + \frac{L_F^2 (r+ \lambda_F) }{(r + L_F)^2 (r + \lambda_F)} \right ). 
%&& \Longleftrightarrow ((r + L)(r + \lambda) + L r^2 ) L^2 < (r + L)^3 (r + \lambda). 
\end{eqnarray*} 
The convergence rate in $z$-error is given by 
\begin{eqnarray*}
    \left ( \frac{L_F r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) &=&    \left ( \frac{L_F }{(r^{1 - 2 \beta} + L_F/r^{2\beta})(r + \lambda_F)} + \left ( \frac{1}{1 + \lambda_F/r} \right )^2 \right )   
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, we have that 
\begin{equation}
\frac{L_F }{(r^{1 - 2 \beta} + L_F r^{-2\beta})(r + \lambda_F)} = \frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta} + \lambda_F L_F r^{-2\beta}}    
\end{equation}
and we have 
\begin{equation}
    \left ( 1 + \lambda_F/r \right )^{-2}  \leq 1 - \frac{2 \lambda_F}{r} + \frac{3 \lambda_F^2}{(1 -\varepsilon)^4 r^2},  
\end{equation}
since it holds that 
\begin{eqnarray*}
(1 + x)^{-2} &=& 1 - 2x + \frac{3}{ (1 + \xi )^4} x^2, \quad x \in (-\varepsilon,1), \quad \xi \in[-\varepsilon, x] \\ 
&\leq&   1 - 2x + \frac{3}{ (1 -\varepsilon)^4} x^2 \end{eqnarray*}
Thus, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}

\textbf{Case 2: $r \ll 1$}.
For error in $H$, the convergence rate is bounded below by $\left( \frac{L_F}{r + L_F} \right)^2$. The convergence in error $H$ deteriorates as $r \to 0$. 

For error in $z$, the convergence rate can be made arbitrarily small if $r$ is sufficiently small. 

\textbf{Therefore, we conclude that when $\omega = r$. For sufficiently large $r$, the convergence in $H$ error becomes better while the convergence in $z$ error deteriorates when $r$ becomes larger. The asymptotic rate in $z$ error is $1 - \frac{2 \lambda_F}{r}$. 
On the other hand, for small $r \ll 1$, the convergence in $H$ deteriorates while the convergence in $z$ error becomes better. }
\end{remark}

\begin{remark}[Convergence rate when $\omega = r + \lambda_F$]
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{(L_F - \textcolor{red}{\lambda_F})}{r + L_F} \right )^2 \right ) 
\end{eqnarray*} 

The convergence rate in $z$-error is given by 
\begin{eqnarray*} 
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) 
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
The analysis is similar to that when $\omega = r$.

For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F - \lambda_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}
The asymptotic convergence rate is $1 - \frac{2\lambda_F}{r}$. 

\textbf{Case 2: $r \ll 1$}.
For $H$ error, the convergence rate is bounded below by $\left( \frac{L_F - \lambda_F}{r + L_F} \right)^2 $. Asymptotically, it is $\left( \frac{L_F - \lambda_F}{L_F} \right)^2$.

For $z$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently small.
\end{remark}

\begin{remark}
    In fact, from standard asymptotic analysis, one should always choose $\omega > r$, say, $\omega = r + \lambda_F$, with sufficiently small $r$.
\end{remark}
\end{comment} 

% \begin{remark}
% We note that the larger the $r$, the convergence of $H_k$ to $H_*$ seems to be faster while the convergence of $Kz_k$ to $Kz_*$ deteriorates.% On the other hand, the larger $r$ guarantees the sufficiently near orthogonality for the cross terms.  
% \end{remark}
%\begin{remark}
%It is interesting to note that if $\omega = r + L_F$, then the cross term disappears. Thus, the error analysis reduces to the following form: 
%\begin{eqnarray*}
%\|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} &\leq& \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - K z_k\|^2_{\omega} \\ 
%&=&  \left ( \frac{r}{r + \lambda_F} \right )^2 \left ( \|H_{*} - H_{k}\|^2 + \|Kz_* - K z_k\|^2_{\omega} \right %).
%\end{eqnarray*}
%\end{remark} 

\subsection{Analysis based on $E_k^X$ and the convexity of $F$ due to \cite{shi2014linear}} 
In this section, we shall adapt the proof originated from \cite{shi2014linear} to establish the result of linear convergence.
\begin{lemma} 
The algorithm \ref{algADMM1} produces iterates $(X_k,z_k,H_k)$ such that the following identity holds: for all $k=0,1,2\cdots$, 
\begin{eqnarray*}\label{equal form 2}
\nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &=& 0, \\
H_{k+1} - H_{k} + r \left( I - P_Z \right) X_{k+1} &=& 0,\\
K z_{k+1} - P_Z X_{k+1} &=& 0.
\end{eqnarray*}
\end{lemma} 
\begin{proof} 
First of all, we note that $K^TH = 0$ with $H = H_k$ or $H = H_*$ and also 
\begin{equation}
Kz = P_Z X. 
\end{equation}
Now, subtracting \eqref{H update} from \eqref{X update}, multiplying $K^T$ to \eqref{H update} and adding it to \eqref{z update}, we have
\begin{equation}\label{equal form 1}
    \begin{split}
        \nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &= 0, \\
        K^T H_{k+1} &= 0, \\
        H_{k+1} - H_{k}  - r (K z_{k+1} - X_{k+1}) &= 0. 
    \end{split}
\end{equation}
Multiplying the third equation in \eqref{equal form 1} by $K^T$ and using the second equation, we complete the proof. 
\end{proof}





We shall obtain a simple but important lemma: 
\begin{lemma} 
The following identity holds: 
\begin{eqnarray}
\nabla F(X_{k+1}) - \nabla F(X_*) &=& r K (z_{k} - z_{k+1}) + H_{k+1} - H_*  \label{difference1}\\
H_{k+1} - H_k &=& - r Q_Z (X_{k+1} - X_*)  \label{difference2} \\
K(z_{k+1} - z_*) &=& P_Z (X_{k+1} - X_*) \label{difference3}    
\end{eqnarray}
\end{lemma} 
\begin{proof} 
We recall the optimality condition, which can be given as follows:
\begin{subeqnarray*}
\nabla F(X_*) - H_* &=& 0 \\ 
K^T H_* &=& 0 \\
K z_* - X_* &=& 0 
\end{subeqnarray*}
However, using the property of $Q_Z$ and $P_Z$, the optimality condition implies that it holds true 
\begin{subeqnarray}\label{KKT}
0 &=& P_Z (K z_* - X_{*}) = K z_* - P_Z X_* \\ 
0 &=& Q_Z (Kz_* - X_*) = - Q_Z X_*. 
\end{subeqnarray}
Subtracting the optimality conditions \eqref{KKT} from \eqref{equal form 2}. This completes the proof. 
\end{proof} 
%\begin{proposition}
%If each $F_i$ is $\lambda_i$-strongly convex, and $L_i$-smooth, then $F(X) = \frac{1}{n}\sum_{i = 1}^n F_i(x_i)$ is $\lambda$-strongly convex and $\frac{M_f}{n}$-smooth, where $\lambda = \frac{\min_{i} m_i}{n}$, $M_f = \max_i M_i$.  
%\end{proposition}
The main theorem considers the convergence of a vector $Y$ that combines the primal variable $Kz$ and the dual variable $H$,
\begin{equation}
Y = \begin{pmatrix}
Kz \\
H
\end{pmatrix} \quad \mbox{ and } \quad C = \begin{pmatrix}
r I & 0 \\
0 & \frac{1}{r} I 
\end{pmatrix}
\end{equation}
We also define a $C-$norm on $Y = (y_1,y_2)^T$ by 
\begin{eqnarray*}
\|Y\|_C^2 &=& (CY, Y) = r \|y_1\|^2 + \frac{1}{r}\|y_2\|^2 \\ 
&=& r (Kz, Kz) + \frac{1}{r} (H, H) = (rK^TK z, z) + \frac{1}{r}(H,H).   
\end{eqnarray*}

We shall need the following lemma: 
\begin{lemma}
We have the following identity:
\begin{eqnarray*}
2 \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \|Y_{k} - Y_*\|^2 - \|Y_{k+1} - Y_* \|^2 - \|Y_{k+1} - Y_k\|^2. 
\end{eqnarray*} 
\end{lemma} 
\begin{proof} 

\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\ 
&=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\
&=&  \langle Y_{k} - Y_*, Y_{k+1} - Y_* \rangle - \| Y_{k+1} - Y_* \|^2
\end{eqnarray*} 
On the other hand, we have 
\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_k + Y_k - Y_* \rangle \\ 
&=& - \|Y_{k+1} - Y_{k}\|^2 + \langle Y_{k} - Y_{k+1}, Y_k - Y_* \rangle . 
\end{eqnarray*} 
By adding these two identities, we obtain the result. This completes the proof. 
\end{proof}
\begin{theorem}
Assume that $K^TH_0 = 0$. Then the ADMM iterations produces $Y_k = [Kz_k; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [Kz_*; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1+\delta} \| Y_{k} - Y_*\|^2_C,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C
\end{equation}
\end{theorem}
\begin{proof}
We begin with using the $\lambda-$strongly convexity condition for $F$ as follows: 
\begin{eqnarray*}
\lambda \| X_{k+1} - X_* \|^2 &\leq& \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*)\rangle \\
&\leq& \langle X_{k+1} - X_*, r K(z_k - z_{k+1}) \rangle + \langle X_{k+1} - X_*, H_{k+1} - H_* \rangle \\
&=& r \langle X_{k+1} - X_*, P_Z (K(z_k - z_{k+1}))  \rangle + \langle X_{k+1} - X_*, Q_Z(H_{k+1} - H_*) \rangle \\
&=& r \langle  P_Z (X_{k+1} - X_*) , Kz_k - Kz_{k+1}   \rangle + \langle Q_Z (X_{k+1} - X_*), H_{k+1} - H_* \rangle  \\
&=& r \langle Kz_k - Kz_{k+1}, Kz_{k+1} - Kz_*  \rangle + \frac{1}{r} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle \\
&=& (Y_{k} - Y_{k+1})^T C (Y_{k+1} - Y_*),  
\end{eqnarray*}
where 
\begin{equation}
Y_k = \begin{pmatrix} 
      Kz_k \\
      H_k 
      \end{pmatrix}, \quad Y_{k+1} = \begin{pmatrix} 
      Kz_{k+1} \\
      H_{k+1}  
      \end{pmatrix}
,\quad Y_{*} = \begin{pmatrix} 
      Kz_{*} \\
      H_{*}  
      \end{pmatrix}
\end{equation}

and the matrix 
\begin{equation}
    C = \begin{pmatrix}
r I & 0 \\
0 &  \frac{1}{r} I
\end{pmatrix}
\end{equation}
This implies
\begin{equation}
    \lambda  \| X_{k+1} - X_* \|^2 \leq  \frac{1}{2} \|Y_k -Y_* \|^2_C - \frac{1}{2}  \|Y_{k+1} - Y_* \|^2_C -  \frac{1}{2}  \| Y_k - Y_{k+1}\|^2_C. 
\end{equation}
Now, by rearranging terms, we have 
\begin{equation}\label{ineq from strong convexity}
2\lambda \| X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C + \|Y_{k+1} - Y_* \|^2_C \leq  \|Y_k -Y_* \|^2_C.
\end{equation}
This immediately, leads to 
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C.
\end{equation}
Having \eqref{ineq from strong convexity}, it suffices to show for some $\delta > 0$, we have 
\begin{equation}\label{claim}
\delta \|Y_{k+1} - Y_* \|^2_C \leq 2 \lambda \|X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C,
\end{equation}
or equivalently,
\begin{eqnarray*}\label{claimequivalence}
\delta \left( r  \|Kz_{k+1}- Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*} \|^2 \right) &\leq&  2 \lambda \|X_{k+1} - X_* \|^2 \\
&+& r \|Kz_{k}- Kz_{k+1} \|^2 + \frac{1}{r} \|H_k - H_{k+1} \|^2, 
\end{eqnarray*}
which will imply the desired inequality: 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1 +\delta} \|Y_{k} - Y_* \|^2_C.
\end{equation}
To prove the inequality \eqref{claim}, first, we observe that the following holds true:   
\begin{equation}\label{bound on z-z*}
\|Kz_{k+1} - Kz_* \|^2 \leq \|X_{k+1} -X_* \|^2.  
\end{equation}
Further, from \eqref{difference1} and using $L$-smoothness of $F$, we have  
\begin{equation}
\|H_{k+1} - H_* \| \leq r \|K z_k - K z_{k+1} \| + L \| X_{k+1} -  X_*\|
\end{equation}
This implies 
\begin{equation}\label{bound on H-H*}
\begin{split}
\|H_{k+1} - H_*\|^2 & \leq \left(r \|Kz_k - Kz_{k+1} \| + L\|X_{k+1} -  X_*\| \right)^2 \\  & \leq 2 \left( r^2 \| Kz_k - Kz_{k+1}\|^2 + L^2 \| X_{k+1} -  X_*\|^2 \right). 
\end{split}
\end{equation}
Thus, we have 
\begin{equation}
\frac{1}{r} \|H_{k+1} - H_*\|^2 \leq 2 \left( r \| Kz_k - Kz_{k+1}\|^2 + \frac{L^2}{r} \| X_{k+1} -  X_*\|^2 \right).
\end{equation}
Substituting \eqref{bound on z-z*} and \eqref{bound on H-H*} into left hand side of \eqref{claimequivalence} and rearranging, we have 
\begin{eqnarray*}
\delta \left(r\|Kz_{k+1} - Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*}\|^2 \right) &\leq& \delta \left( r + \frac{2L^2}{r} \right ) \| X_{k+1} -  X_*\|^2  \\
&& + 2\delta r \|Kz_k - Kz_{k+1} \|^2 \\
&\leq& 2\lambda \|X_{k+1} - X_* \|^2 + r \|K z_{k}- Kz_{k+1}\|^2 \\
&& + \frac{1}{r}\|H_k - H_{k+1} \|^2,
\end{eqnarray*}
by making $\delta$ sufficiently small such that 
\begin{equation}
\begin{aligned}
   \delta \leq \frac{2\lambda}{ r + \frac{2L^2}{r}} \quad and \quad \delta \leq \frac{1}{2}.
\end{aligned}
\end{equation}
%To maximize $\delta$, one can choose $r = \sqrt{2} L$. Then $\delta \leq \frac{1}{\sqrt{2}} \frac{\lambda}{L}$. 
%\textcolor{red}{I guess this is not of our main interest here !}
This completes the proof. 
\end{proof}
\end{comment} 
%\begin{remark} 
%Since we have that with $\omega = \frac{2}{r + L_F + r + \lambda_F}$, 
%\begin{eqnarray*}
%\lambda_{max}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+L_F} = \frac{r + L_F - \omega}{r + L_F} \\  
%\lambda_{min}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+\lambda_F} = \frac{r %+ \lambda_F - \omega}{r + \lambda_F}.
%\end{eqnarray*}
%Thus, we have that
%\begin{equation}
%\frac{\kappa(A_1) - 1}{\kappa(A_1) + 1} = \frac{ \frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} - 1}{\frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} + 1} =\frac{ \frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} - 1}{\frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} + 1} = \frac{\frac{r + \lambda_F}{r + L_F}  - \frac{\lambda_F}{L_F}}{\frac{r + \lambda_F}{r + L_F} + \frac{\lambda_F}{L_F}}  
%\end{equation} 
%and 
%\begin{equation}
%\frac{\kappa(A_2) - 1}{\kappa(A_2) + 1} = \frac{ \frac{r + \lambda_F}{r + L_F} - 1}{\frac{r + \lambda_F}{r + L_F}  + 1}  
%\end{equation}
%On the other hand, we have that 
%\begin{equation}
%\sin (\theta_1 + \theta_2) = 
%\end{equation}
%and $\lambda_{min} = 1 - \frac{\omega}{r + \lambda_F} = \frac{\lambda_F}{r + \lambda_F}$ while $H_{A_r} = 1/(r + L_F)$ and $1/(r + \lambda_F)$. Therefore, if $r$ is sufficiently large, then $\sin\theta$ is quite small. 
%\begin{theorem}
%For $A_1$ and $A_2$, symmetric positive definite matrices, we have 
%\begin{equation}
%(x,A_1A_2y) \leq \sin (\theta_1 + %\theta_2) \|A_1x\|\|A_2y\|,  
%\end{equation}
%where $\sin \theta_1 = \frac{\kappa(A_1) - 1}{\kappa(A_2) + 1}$ and $\sin \theta_2 = \frac{\kappa(A_2) - 1}{\kappa(A_2) + 1}$. 
%\end{theorem}
%\end{remark}
\begin{remark}
We remark that it is more natural to write the convergence rate in terms of $\kappa(G^*)$, the condition number of $G^*$ since the choice of $\omega$ is made for solving the system relevant to $G^*$. However, it is also fine to use $\kappa(G)$ since it is more relvant to the problem to be solved and we have that
\begin{equation}
\kappa(G^*) = \kappa(G) \quad \rightarrow \quad 
\frac{\kappa(G) - 1}{\kappa(G) + 1} = \frac{\kappa(G^*) - 1}{\kappa(G^*) + 1}. 
\end{equation}
\end{remark} 

\subsection{Convergence analysis of Algorithm \ref{algADMM1} for $D_r^* \neq A_r^*$}\label{gdn} 

In this section, our goal is to establish the convergence of inexact Uzawa with Gauss-Seidel method, $D_r = A_r$, is replaced by inexact local solve for $X$-variable. This includes the $N-$step of Gradient Descent iteration. We recall that the Gauss-Seidel method can be interpreted as to solve the following optimization exactly in $X$-update \eqref{Xupdate} of the Algorithm \ref{algADMM1}:  
\begin{equation}
\min_{X} L_r(X,z_k,H_k).  
\end{equation}
We now let $G(X) = L_r(X,z_k,H_k)$, i.e.,  
\begin{equation}
G(X) = F(X) + \langle H_k, Kz_k - X\rangle + \frac{r}{2}\|Kz_k - X\|^2 
\end{equation}
and let $Y_*  = {\rm arg}\min_X G(X)$. Note that we reserve $X_{k+1}$ as the result of $N-$step of GD method. Then the following statements hold true: 
\begin{enumerate} 
\item $Y_* = A_r^*(H_k + rKz_k)$ 
\item $\nabla F(Y_*) + r Y_* = H_k + rKz_k$ 
\item $\nabla G(Y_*) = 0$. 
\end{enumerate} 
Note that it is easy to show that $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex. Our discussion is for general inexact local solve and the outcome will be denoted by $X_{k+1}$, i.e., 
\begin{equation}
X_{k+1} = D_r^*(H_k + rKz_k). 
\end{equation}
For the convergence analysis, the exact Gauss-Seidel case will be used as an intermediate step. Thus, naturally, the convergence estimate could be made to be sharper. We begin our discussion by introducing two quantities:
\begin{eqnarray*}
\textsf{E}_1 &:=& E_k^H - \omega \left \{A_r^{*}(H_* + rKz_*) - A_r^*(H_k + rKz_k) \right \}\\
\textsf{E}_2 &:=& A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k).  
\end{eqnarray*}
It is evident that $\textsf{E}_1$ is for the error of the Algorithm \ref{algADMM1} with $D_r^* = A_r^*$ and $\textsf{E}_2$ represents the difference between two iterates, one from the inexact solve for $X-$variable and the other from the exact solve. We shall now see that Lemma \ref{main:lem1} can lead to the following estimate easily. 
\begin{eqnarray*}
&& \left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega \{ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \} \right \|^2 \\
&& \qquad \leq \|\textsf{E}_1\|^2 + \omega^2 \|\textsf{E}_2\|^2 + 2 \omega \|\textsf{E}_1\| \|\textsf{E}_2\|. 
\end{eqnarray*}
The first term is relevant to the Algorithm \ref{algADMM1} with $D_r = A_r$. We shall assume that there exists $\delta < 1$ such that 
\begin{equation}\label{main:cvdelta}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ).
\end{equation}
Namely, the inexact solve provides a reasonably close solution to $Y_*$. Under this assumption, we shall establish the convergence of Algorithm \ref{algADMM1}. How small $\delta$ can be, for still allowing the convergence is determined by the close investigation of the convergence of Algorithm \ref{algADMM1} with exact local solve after incorporating $\delta$.   

We are now in a position to provide a main instrumental lemma in this section. 
\begin{theorem}\label{main:ins}
Let $\delta_{GS}$ be the convergence rate for the Algorithm \ref{algADMM1}, when $D_r = A_r$, as given in equation \eqref{gsrate}, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be such that the equation \eqref{main:cvdelta} hold. Then, we have the following estimate:   
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \delta_{GS} + \delta \right )^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{theorem} 
\begin{proof}
We shall let $E^2 = \|E_k^H\|^2 + \|E_k^Z\|_\omega^2$. Due to the equation \eqref{main:cvdelta}, we have that 
\begin{equation}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 E^2. 
\end{equation}
We then obtain that 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 &\leq& \|\textsf{E}_1\|^2 + \|\textsf{E}_2\|_\omega^2 + 2 \|\textsf{E}_1\| \|\textsf{E}_2\|_\omega  \\ 
&=& \delta_{GS}^2 E^2 + \delta^2 E^2 + 2 \delta_{GS} \delta E^2 = (\delta + \delta_{GS})^2 E^2. 
\end{eqnarray*}
%Here we have invoked a simple Cauchy-Schwarz inequality that 
%\begin{equation*}
%ab \leq \frac{1}{2\varepsilon} a^2 + \frac{\varepsilon}{2} b^2, \quad \forall a, b\in \Reals{}.  
%\end{equation*}
%Finally, we observe that $\delta_{GS}$
This completes the proof. 
\end{proof}

We also note that the $N-$step of GD method is basically, given as follows: for a given step size $\gamma > 0$ and $X_k = Kz_k$, 
\begin{subeqnarray}\label{ngd1}
X_{k+\frac{1}{N}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} - \gamma \nabla G(X_{k+\frac{1}{N}}) \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} - \gamma \nabla G(X_{k + \frac{N-2}{N}}) \\
X_{k+\frac{N}{N}} &=& 
X_{k+\frac{N-1}{N}} - \gamma \nabla G(X_{k+\frac{N-1}{N}}), 
\end{subeqnarray}
where $\nabla G$ is given as follows: 
\begin{equation} 
\nabla G(X) = \nabla F(X) + rX - H_k - rKz_k. 
\end{equation}

We shall now present a simple but important lemma: 
\begin{lemma}\label{main:lm} 
Let $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ and $D_r^*(H_k + rKz_k)$ be the $N$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \leq  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \|Y_* - X_k\|^2. 
\end{eqnarray*}
\end{lemma}
\begin{proof}
Let $Y_* = {\rm arg} \min_{X} G(X)$. On the other hand, $X_{k+1}$ is obtained by the following iteration: 
\begin{eqnarray*} 
X_{k+\frac{1}{N}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} - \gamma \nabla G(X_{k+\frac{1}{N}}) \\ 
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} - \gamma \nabla G(X_{k + \frac{N-2}{N}}) \\
X_{k+\frac{N}{N}} &=& 
X_{k+\frac{N-1}{N}} - \gamma \nabla G(X_{k+\frac{N-1}{N}}).  
\end{eqnarray*}
Since $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex, we see that by Lemma \ref{lemmaGD}, we have that 
\begin{eqnarray*}
\|Y_* - X_{k+1}\|^2 &\leq& \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \|Y_* - X_k\|^2. 
\end{eqnarray*}
%
%&\leq& 2 \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \|Y_* - X_*\|^2 + \|X_* - X_k\|^2 \right ) \\
%&\leq& c \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \frac{1}{r + \lambda_F} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2 \right ) + \|X_* - X_k\|^2 \right ). 
%\end{eqnarray*} 
This completes the proof. 
\end{proof}
\begin{remark} 
The choice of $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ is optimal for GD and the convergence rate can be calculated. Since $\kappa(G) = \frac{r + L_F}{r + \lambda_F}$, we have that 
\begin{equation}
\frac{\kappa(G)-1}{\kappa(G)+1} = \frac{L_F - \lambda_F}{2r + L_F + \lambda_F}. 
\end{equation}
The convergence is even faster for large $r$ and large $N$.
\end{remark}
We shall now see that the norm of $\textsf{E}_2$ can be made to be quite small. 
\begin{lemma}
Let $\gamma = \frac{2}{\lambda_G + L_G}$, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be the $N$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 \leq 4 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{lemma}
\begin{proof}
By Lemma \ref{main:lm}, we have that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 &=& \omega^2 \|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \\
&\leq& \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \omega^2 \|Y_* - X_k\|^2. 
\end{eqnarray*}
On the other hand, we have that since $X_k = Kz_k$ and due to Theorem \ref{thm:GS},  
\begin{eqnarray*}
\omega^2 \|Y_* - X_k\|^2 &\leq& 2 \omega^2 \|Y_* - X_*\|^2 + 2 \|X_* - X_k\|_\omega^2 \\ 
&\leq& \frac{2 \omega^2}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|X_* - X_k\|_\omega^2 \\
&\leq& \frac{8(r + L_F)^2}{(2r + L_F + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|E_k^Z\|_\omega^2 \\ 
&\leq& 4 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) \end{eqnarray*} 
The last inequality is due to the fact that $X_k = Kz_k$ and 
\begin{equation}
\omega = \frac{2(r + \lambda_F)(r + L_F)}{(2r + L_F + \lambda_F)},
\end{equation} 
thus, 
\begin{equation}
\frac{2 \omega^2}{(r + \lambda_F)^2} = \frac{8(r+L_F)^2}{(2r + L_F + \lambda_F)^2} \leq 2.  
\end{equation}
This completes the proof. 
\end{proof}
This result gives that if $N$ is large enough, then we can obtain the convergence of $N-$step GD based FL. 


%We shall compute %\begin{equation}
%I - \gamma A_r =^{???} \frac{\kappa(A_r) - 1}{\kappa(A_r)+1} I. 
%\end{equation}



%Therefore, we have that 
%\begin{eqnarray*}
%c_1(r) &=& \sup_{v = (X,z)} \frac{ \left \langle \overline{R}^{-1} \begin{pmatrix} X - r \gamma Kz \\ z \end{pmatrix},\begin{pmatrix} X - r \gamma Kz \\ z \end{pmatrix} \right \rangle }{(A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)} \\
%&=& \sup_{v = (X,z)} \frac{ \langle C (X - r \gamma Kz), (X - r \gamma Kz) \rangle + \langle rK^T K z, z \rangle }{(A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)}, 
%\end{eqnarray*}
%where $C = [\gamma I + \gamma (I - \gamma A_r^{-1})]^{-1}$. We note that if $\gamma 
%\ll 1$, then 


\end{document} 

\section{Gauss-Seidel Solve for the Linear Case for two blocks $U = (X,z)$ and $H$} 

We recall that 
\begin{equation}
\mathcal{A}_r = \begin{pmatrix} 
A_0 + rI & - rK \\ -rK^T & rK^TK 
\end{pmatrix} 
\quad \mbox{ and } \quad 
\mathcal{B} = \begin{pmatrix} 
-I & K \end{pmatrix}. 
\end{equation} 
In this section, we shall consider the Gauss-Seidel solve for the matrix $ \nabla G = \mathcal{A}_r$. We write an equivalent form of the above equation using the Schur complement system. We shall consider applying the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
\mathcal{L} = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad \psi = \mathcal{L}^{-1} = \begin{pmatrix}
A_r^{-1} & 0\\
(rK^T K)^{-1} rK^T A_r^{-1} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + \mathcal{L}^{-1} (f - B^T H_k - \mathcal{A}_r U_k) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that the exact solutions satiafy \begin{eqnarray}
U_{*} &=& U_{*} + \mathcal{L}^{-1} (f - B^T H_{*} - \mathcal{A}_r U_{*}) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray}
Thus, with the convention that $E_{k}^U = U_* - U_k$ and $E_{k}^H = H_* - H_k$, we obtain the error equations: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{L}^{-1} (- \mathcal{A}_r E^U_k - B^T E^H_k) \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray}
In the other direction, we have that 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{L}^{-1} (- \mathcal{A}_r  E^U_k - B^T E^H_k) \\
          &=& E^U_k - \mathcal{L}^{-1} \mathcal{A}_r E^U_k - \mathcal{L}^{-1} B^T E^H_k \\
          &=& (I - \mathcal{L}^{-1} \mathcal{A}_r) E^U_k - \mathcal{L}^{-1} B^T E_k^H \\ 
E^H_{k+1} &=& E^H_k + \omega B ((I - \mathcal{L}^{-1} \mathcal{A}_r) E^U_k - \mathcal{L}^{-1} B^T E_k^H ) \\
&=& (I - \omega B \mathcal{L}^{-1} B^T) E_k^H + \omega B (I - \mathcal{L}^{-1} \mathcal{A}_r)(E_k^U). 
\end{eqnarray} 
In the matrix form, we have 
\begin{equation}
\begin{pmatrix} 
I  & 0 \\
-\omega B & I
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k+1} \\
E^H_{k+1}
\end{pmatrix}  = 
\begin{pmatrix} 
I - \mathcal{L}^{-1} \mathcal{A}_r & - \mathcal{L}^{-1} B^T \\
0 & I
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k} \\
E^H_{k}
\end{pmatrix} 
\end{equation}
Or, we have 
\begin{equation}
\begin{pmatrix} 
E^U_{k+1} \\
E^H_{k+1}
\end{pmatrix}  = 
\begin{pmatrix} 
I - \mathcal{L}^{-1} \mathcal{A}_r & - \mathcal{L}^{-1} B^T \\
\omega B (I - \mathcal{L}^{-1} \mathcal{A}_r) & I - \omega B \mathcal{L}^{-1} B^T
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k} \\
E^H_{k}
\end{pmatrix}.  
\end{equation}

We shall denote $Q_B = 1/\omega$ and noticed that this expression is very similar to what Bramble has in his paper. Following Bramble, we shall assume there are two contraction paratmers, $\delta$ and $\gamma$ which are defined by the following inequality:
\begin{subeqnarray}
\|(\mathcal{A}_r^{-1} - \psi)(\phi)\|_{\mathcal{A}_r} \leq \delta \|\phi\|_{\mathcal{A}_r^{-1}} \\ 
\|(I - \omega \mathcal{S}_r)v\|_{Q_B} \leq \gamma \|v\|_{Q_B}. 
\end{subeqnarray}
Namely, $\delta$ is the convergence rate for Gauss-Seidel method for example and $\gamma$ is the convergence rate for the Schur complement solve. We assume that $\delta$ satisfies the following:
\begin{equation}
\delta < \frac{1-\gamma}{3-\gamma}. 
\end{equation} 
Bramble showed the following. We see that the following holds: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k - \psi (\mathcal{A}_r  E^U_k + B^T E^H_k) \\
          &=& (\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E^U_k + B^T E_k^H) - \mathcal{A}_r^{-1} B^T E^H_k \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray} 
By taking $\mathcal{A}_r$ norm, we arrive at 
\begin{eqnarray}
\|E^U_{k+1}\|_{\mathcal{A}_r} &=& \|(\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E^U_k + B^T E_k^H) - \mathcal{A}_r^{-1} B^T E^H_k \|_{\mathcal{A}_r}. 
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray} 
This gives that 
\begin{eqnarray}
\|E^U_{k+1}\|_{\mathcal{A}_r} &\leq& \delta \|E_k^U\|_{\mathcal{A}_r} + (1 + \delta) \|E_k^H\|_{Q_B}. 
\end{eqnarray}
We note that 
\begin{eqnarray}
E^H_{k+1} &=& (I - \omega B \psi B^T) E_k^H + \omega B (I - \psi \mathcal{A}_r)(E_k^U) \\ 
&=& (I - \omega B \mathcal{A}_r^{-1} B^T) E_k^H + \omega B (\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E_k^U + B^T E_k^H)  
\end{eqnarray} 
Thus we have that 
\begin{eqnarray}
\|E^H_{k+1}\|_{Q_B} &\leq& (\gamma + \delta) \|E_k^H\|_{Q_B} + \delta \|E_k^U\|_{\mathcal{A}_r}. 
\end{eqnarray} 
Therefore, we have that in a matrix form: 
\begin{equation}
\begin{pmatrix} 
\|E^U_{k+1}\|_{\mathcal{A}_r} \\
\|E^H_{k+1}\|_{Q_B}
\end{pmatrix} \leq M^{k+1}  
\begin{pmatrix} 
\|E^U_{0}\|_{\mathcal{A}_r} \\
\|E^H_{0}\|_{Q_B}
\end{pmatrix}  
\end{equation}
Here 
\begin{equation}
M = \begin{pmatrix} 
\delta & 1 + \delta \\ \delta & \gamma + \delta 
\end{pmatrix}. 
\end{equation}
This matrix is symmetric with respect to the inner product defined as follows: 
\begin{equation}
\left [ \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, \begin{pmatrix}  x_2 \\ y_2 \end{pmatrix} \right ] = \frac{\delta}{1+\delta} x_1 x_2 + y_1 y_2. 
\end{equation} 
Since $M$ is symmetric, its norm is bounded by its spectral radius and it is roots of 
\begin{equation}
\lambda^2 - (2\delta + \gamma)\lambda - \delta (1 - \gamma) = 0. 
\end{equation} 
It is observed that the spectral radius $\rho$ is an increasing function of $\delta$ for any fixed $\gamma \in [0,1]$. Moreover $\rho = 1$ for $\delta = (1-\gamma)/(3-\gamma)$. The convergence factor is then given by 
\begin{equation}
\rho = \frac{2\delta + \gamma + \sqrt{ (2\delta + \gamma)^2 + 4\delta(1-\gamma)}}{2}. 
\end{equation} 
Note that here $\gamma$ is the convergence rate for Richardson while $\delta$ is the convergence rate for Gauss-Seidel method. The convergence estimate is given as follows: 
\begin{eqnarray*}
\left ( \frac{\delta}{1 + \delta} (\mathcal{A}_r E_k^U, E_k^U) + (Q_B E_k^H, E_k^H) \right ) \leq \rho^{2k} \left ( \frac{\delta}{1 + \delta} (\mathcal{A}_r E_k^U, E_k^U) + (Q_B E_k^H, E_k^H) \right ). 
\end{eqnarray*}

\end{document} 

% Directly taking $A-$norm: 
% \begin{eqnarray}
% \| E^U_{k+1} \|_A &\leq& \| I - L^{-1} \nabla G \| \|E^U_k\|_A  + \| L^{-1} B^T \|_A \|E^H_k\|_A \\
% \| E^H_{k+1}\| &\leq& \|I - \omega B L^{-1} B^T \| \|E_k^H\| + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
% \end{eqnarray}

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

\begin{equation}
    \|M \|_A \leq \sqrt{ \kappa(A)} \| M \|_2
\end{equation}
We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \lambda_{max}(A) \| L^{-1} B^T H\| \\
   & =\lambda_{max}(A) (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}
We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}
\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}
\textcolor{red}{ Here !} 

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}

%\end{document} 

\section{Appendix} 

\begin{lemma} 
The GD three step can be shown to be convergent linearly if one chooses $\gamma = \frac{1}{r}$ with $\omega = r$, sufficiently small enough, then it converges linearly.
\end{lemma} 
\begin{proof} 
For GD three step case, we have that with $N = 3$: 
\begin{eqnarray*}
D_r^*(H_k + rKz_*) = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}
We also note that  
\begin{equation*}
D_r^*(H_k + rKz_k) = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{equation*}
Let us define 
\begin{eqnarray*}
D_{H_*,H_k}^{Z_*} &:=& D_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*) \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*) + \gamma H_* \\
&& \qquad - ((I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma H_k)  \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*) \\
&& \qquad - (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma (H_* - H_k) \\ 
&=& (I - \gamma A)(\xi_1) ((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - ((I - \gamma A)((I - \gamma A)(X_*)) + \gamma H_k)) \\
&& \qquad + \gamma (I + (I - \gamma A)(\xi_1)) (H_* - H_k) \\
&=& \gamma [(I - \gamma A)(\xi_1)(I - \gamma A)(\xi_2) + I + (I - \gamma A)(\xi_1))] (H_* - H_k) \\
&=& \gamma [I + A_\gamma (\xi_1) + A_\gamma(\xi_1) A_\gamma(\xi_2)] (H_* - H_k). 
\end{eqnarray*}
\begin{eqnarray*}
D_{Z_*,Z_k}^{H_k} &=& D_r^*(H_k + rKz_*) - D_r^*(H_k + rKz_k) \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) + \gamma H_k)) \\
&& \qquad - (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k)) \\ 
&=& A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k). 
\end{eqnarray*}
We now observe the following estimate holds: 
\begin{eqnarray*}
&& \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (D_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 \\ 
&& \quad = \|H_* - H_k - \omega (D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k})\|^2,  \\
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k} \rangle + \omega^2 \langle  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k},  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k}\rangle \\ 
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} \rangle + \omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{H_*,H_k}^{Z_*}\rangle \\ 
&& \qquad - 2 \omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2 \omega^2 \langle  
D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k} \rangle \\
&& \qquad + \omega^2 \langle D_{Z_*,Z_k}^{H_k},  D_{Z_*,Z_k}^{H_k} \rangle. 
\end{eqnarray*}
On the other hand, we see that 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad = -2\omega \langle H_* - H_k, A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle \\ 
&& \qquad + 2 \omega^2 \langle \gamma (I + A_\gamma (\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2)) (H_* - H_k), A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k)  \rangle \\
&& \qquad = - 2\omega \langle (I - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))(H_* - H_k), A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k) \rangle. 
%&& \qquad \leq 2\omega|\omega\gamma - 1| \sin^2 \theta (1 - \gamma \lambda)^2 \|H_* - H_k\| \|X_* - X_k\| \\
%&& \qquad + 2\omega^2 \gamma (1 - \gamma \lambda)^3 \sin^3 \theta \|H_* - H_k\|\|X_* - X_k\|.   
\end{eqnarray*}
Therefore, we have that with $\alpha + \beta = 1$, 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad \leq (1 - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))^{2\alpha} \rho_1^3 \|H_* - H_k\|^2 \\
&& + (1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))^{2\beta} \rho_1^3 \|X_* - X_k\|_\omega^2. 
\end{eqnarray*}
On the other hand, we have 
\begin{eqnarray*}
\|H_* - H_k - \omega D_{H_*,H_k}^{Z_*} \|^2 &\leq& 
\left (1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2} \|H_* - H_k\|^2 \\ 
\|D_{Z_*,Z_k}^{H_k}\|^2 &=& (1 - \gamma \lambda)^6 \|X_* - X_k\|^2.   
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} \leq \rho_{E^H} \|H_* - H_k\|^2  \\
&& \qquad + \rho_{E^X} \|X_* - X_k\|_{\omega}^2, 
\end{eqnarray*}
where 
\begin{eqnarray*}
\rho_{E^H} &=& \left ( 1 + \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2\alpha} \rho^3 \right ) \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2} \\
\rho_{E^X} &=& \left ( 1 + \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2\beta} \rho^3 \right ) (1 - \gamma \lambda)^6. 
\end{eqnarray*}
We now choose $\alpha = 0$ and $\beta = 1$. Further, we choose $\omega = 1/(3\gamma)$. Then, we see that 
\begin{eqnarray*}
I - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1) A_\gamma(\xi_2)) &=& I - (I + (I - \gamma A(\xi_1) + (I - \gamma A(\xi_1))(I - \gamma A(\xi_2))))/3  \\
&=& [2\gamma A(\xi_1) + \gamma A(\xi_2) - \gamma^2 A(\xi_1) A(\xi_2)]/3 = \gamma + O(\gamma^2). 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
\rho_{E^H} &=& \left ( 1 + (\gamma + O(\gamma^2))^{2\alpha} \rho^3 \right )  O(\gamma^2) \\
\rho_{E^X} &=& \left ( 1 + (\gamma + O(\gamma^2))^{2\beta} \rho^3 \right ) (1 - \gamma \lambda)^6. 
\end{eqnarray*}
We note that with $r_1 \rightarrow r - \lambda$, we have 
\begin{equation} 
1 - \gamma \lambda = 1 - \frac{\lambda}{r} = \frac{r - \lambda}{r} = \frac{r_1}{r_1 + \lambda}. 
\end{equation} 
We note that if $r$ is sufficiently large, then we can make sure these two parameters smaller than one. Thus, we have the linear convergence.  This completes the proof. 
\end{proof}


We now present the main theorem in this subsection. 
\begin{lemma} 
The GD two step can be shown to be convergent linearly if one chooses $\gamma = \frac{1}{r}$ with $\omega = r$, sufficiently small enough, then it converges linearly.
\end{lemma} 
\begin{proof} 
For GD two step case, we have that with $N = 2$
\begin{eqnarray*}
D_r^*(H_k + rKz_*) = (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}
We also note that  
\begin{equation*}
D_r^*(H_k + rKz_k) = (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k.  
\end{equation*}
Let us define 
\begin{eqnarray*}
D_{H_*,H_k}^{Z_*} &:=& D_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*) \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_* - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k)  \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma (H_* - H_k) \\ 
&=& (I - \gamma A)(\xi_1) ( (I - \gamma A)(X_*) + \gamma H_* - (I - \gamma A)(X_*) - \gamma H_k ) + \gamma (H_* - H_k) \\ 
&=& \gamma (I - \gamma A)(\xi_1)(H_* - H_k) + \gamma (H_* - H_k) = \gamma A_{\gamma_1} (H_* - H_k) + \gamma (H_* - H_k) \\
&=& \gamma (I +  A_\gamma(\xi_1)) (H_* - H_k). 
\end{eqnarray*}
and 
\begin{eqnarray*}
D_{Z_*,Z_k}^{H_k} &=& D_r^*(H_k + rKz_*) - D_r^*(H_k + rKz_k) \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) + \gamma H_k) - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) \\ 
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) \\ 
&=& A_{\eta_1} ((I - \gamma A)(X_*) - (I - \gamma A)(X_k)) = A_{\eta_1} A_{\eta_2} (X_*  - X_k). 
\end{eqnarray*}
We now observe the following estimate holds: 
\begin{eqnarray*}
&& \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (D_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 \\ 
&& \quad = \|H_* - H_k - \omega (D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k})\|^2,  \\
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k} \rangle + \omega^2 \langle  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k},  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k}\rangle \\ 
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} \rangle + \omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{H_*,H_k}^{Z_*}\rangle \\ 
&& \qquad - 2 \omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2 \omega^2 \langle  
D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k} \rangle \\
&& \qquad + \omega^2 \langle D_{Z_*,Z_k}^{H_k},  D_{Z_*,Z_k}^{H_k} \rangle. 
\end{eqnarray*}
On the other hand, we see that 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad = -2\omega \langle H_* - H_k, A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle \\ 
&& \qquad + 2 \omega^2 \langle \gamma (I + A_\gamma (\xi_1)) (H_* - H_k), A_{\eta_1} A_{\eta_2} (X_*  - X_k)  \rangle \\
&& \qquad = - 2\omega \langle (I - \omega \gamma (I + A_\gamma(\xi_1))(H_* - H_k), A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle. 
%&& \qquad \leq 2\omega|\omega\gamma - 1| \sin^2 \theta (1 - \gamma \lambda)^2 \|H_* - H_k\| \|X_* - X_k\| \\
%&& \qquad + 2\omega^2 \gamma (1 - \gamma \lambda)^3 \sin^3 \theta \|H_* - H_k\|\|X_* - X_k\|.   
\end{eqnarray*}
Therefore, we have that with $\alpha + \beta = 1$, 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad \leq (1 - \omega \gamma (I + A_\gamma(\xi_1))^{2\alpha} \rho_1^2 \sin 3 \theta \|H_* - H_k\|^2 + (1 - \omega \gamma (I +  A_\gamma(\xi_1))^{2\beta} \rho_1^2 \sin 3 \theta \|X_* - X_k\|_\omega^2. 
\end{eqnarray*}
On the other hand, we have 
\begin{eqnarray*}
\|H_* - H_k - \omega D_{H_*,H_k}^{Z_*} \|^2 &\leq& 
(1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \|H_* - H_k\|^2 \\ 
\|D_{Z_*,Z_k}^{H_k}\|^2 &=& (1 - \gamma \lambda)^4 \|X_* - X_k\|^2.  \\ 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} \\
&& \qquad \leq \left ( 1 + (1 - \omega \gamma (I + A_\gamma(\xi_1)))^{2\alpha}\rho^2 \sin 3\theta \right ) (1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \|H_* - H_k\|^2  \\
&& \qquad + \left (1 + (1 - \omega\gamma (I + A_\gamma(\xi_1))^{2\beta} \rho^2 \sin 3\theta \right ) (1 - \gamma \lambda)^4  \|X_* - X_k\|_{\omega}^2. 
%&& + \left ( (1 - \gamma \lambda)^4 + \rho^{2\beta_2} \sin^{2\mu_2} \theta \right ) \|Kz_* - Kz_k\|_\omega^2 \\ 
%&=& \left ( 1 + \sin^{2\mu_1} \right ) (1 - \gamma \lambda)^2  \|H_* - H_k\|^2 \\
%&& + \left ( 1 + \sin^{2\mu_2} \right ) (1 - \gamma \lambda)^4 \|Kz_* - Kz_k\|_\omega^2
\end{eqnarray*}
We note that with $r_1 \rightarrow r - \lambda$, we have that $\rho$ behaves like 
\begin{equation} 
1 - \gamma \lambda = 1 - \frac{\lambda}{r} = \frac{r - \lambda}{r} = \frac{r_1}{r_1 + \lambda}.  
\end{equation} 
On the other hand, with the choice of $\omega = 1/(2\gamma)$, we have that for sufficiently larger $r$, 
\begin{eqnarray*}
\sigma(I - \omega \gamma (I + A_\gamma(\xi_1))) &=& \sigma (I - (I + A_\gamma(\xi_1))/2) \\
&=& \sigma (I - (I + I - \gamma A(\xi_1))/2) = \sigma(\gamma A(\xi_2)/2) = \frac{\gamma}{2} L.    
\end{eqnarray*}
Thus, if we choose $\alpha = 0$ and $\beta = 1$, then we see that 
\begin{eqnarray*}
&& \left ( 1 + (1 - \omega \gamma (I + A_\gamma(\xi_1)))^{2\alpha}\rho^2 \right ) (1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \\
&& = \left ( 1 + \rho^2 \right ) \frac{\gamma^2 L^2}{4} \\
&& \left (1 + (1 - \omega\gamma (I + A_\gamma(\xi_1))^{2\beta} \rho^2 \right ) (1 - \gamma \lambda)^4 = \left ( 1 + \left ( \frac{\gamma L}{2} \right )^{2\beta}\rho^2 \right ) = \left ( 1 + \frac{c}{r^2} \right ) \left (\frac{r}{r + \lambda} \right )^4 \\ 
&& < 1 \Longleftrightarrow r^4 + c r^2 < (r + \lambda)^4. 
\end{eqnarray*}
This completes the proof. 
\end{proof}

We notice that the following monotonicity holds for $F_N$. 
\begin{lemma}
The function $F_N(X,H) : \Reals{d} \mapsto \Reals{d}$, defined by 
\begin{equation}
F_N(X,H) := \underbrace{W(\cdots (W(W}_{\textsf{N} \, \mbox{times}}(X, \underbrace{H), \cdots H)),H)}_{\textsf{N}\, \mbox{times}}
\end{equation}
is strongly monotone in the second variable. Namely, we have that for some $\lambda_N > 0$, which does not degenerate in $N$,  
\begin{equation}
\langle H_1 - H_2, F_N(X,H_1) - F_N(X,H_2) \rangle \geq \lambda_N \|H_1 - H_2\|^2, \quad \forall H_1, H_2 \in \Reals{d}.
\end{equation}
More precisely, we have that 
\begin{equation} 
\lambda_N = \gamma + (1 - \gamma L) \lambda_{N-1} = \gamma \sum_{\ell=1}^N (1 - \gamma L)^{\ell-1}.  
\end{equation} 
with $\lambda_1 = \gamma$. Furthermore, $F_N$ is uniformly continuous in the second variable in the following sense that
\begin{equation}
\|F_N(X,H_1) - F_N(X,H_2)\| \leq L_{N} \|H_1 - H_2\|,  
\end{equation}
where 
\begin{equation} 
L_{N} = \gamma \sum_{\ell = 1}^{N} (1 - \gamma \lambda)^{\ell-1}. 
\end{equation} 
\end{lemma}
\begin{proof}
We prove it by induction arguement. For $N = 1$, we have that 
\begin{eqnarray*}
\langle H_1 - H_2, F_1(X,H_1) - F_1(X,H_2) \rangle &=& \langle H_1 - H_2, W(X,H_1) - W(X,H_2) \rangle \\ 
&=& \langle H_1 - H_2, [(I - \gamma A)(X)  + \gamma H_1] - [(I - \gamma A)(X) + \gamma H_2] \rangle \\
&=& \langle H_1 - H_2, \gamma (H_1 - H_2) \rangle = \gamma \|H_1 - H_2\|^2. 
\end{eqnarray*}
Here $\lambda_1 = \gamma$. Now, we assume that there exists $\lambda_{N-1}$ such that 
\begin{eqnarray*}
\langle H_1 - H_2, F_{N-1}(X,H_1) - F_{N-1}(X,H_2) \rangle \geq r_{N-1} \|H_1 - H_2\|^2. 
\end{eqnarray*}
We then consider the following: 
\begin{eqnarray*}
&& \langle H_1 - H_2, F_N(X,H_1) - F_N(X,H_2) \rangle \\
&& \qquad = \langle H_1 - H_2, [W(F_{N-1}(X,H_1)) + \gamma H_1] - [W(F_{N-1}(X,H_2)) + \gamma H_2] \rangle = \gamma \|H_1 - H_2\|^2 \\
&& \qquad \geq \gamma \|H_1 - H_2\|^2 + (1 - \gamma L ) \langle H_1 - H_2, F_{N-1}(X,H_1) - F_{N-1}(X,H_2) \rangle \\
&& \qquad = (\gamma + (1 - \gamma L) \lambda_{N-1}) \|H_1 - H_2\|^2 = \lambda_N \|H_1 - H_2\|^2. 
\end{eqnarray*}
This completes the proof. 
\end{proof}


\subsection{Inexact Gauss-Seidel with a fixed number of GD steps for $X$ block} 
For given $(z_k, H_k)$, we consider the solution to the following equation: 
\begin{equation}
\nabla F(X) + r X - H_k - r Kz_k = 0. 
\end{equation}
This has been characterized as $X_{k+1}$ in the block Gauss-Seidel method in the previous section. However, we are interested in solving it using the Gradient Descent method. We notice that the corresponding functional is given as follows: 
\begin{equation} 
G(X) = F(X) - \langle H_k, X \rangle + \frac{r}{2} \|Kz_k - X\|^2. 
\end{equation} 
The optimality condition is that 
\begin{equation} 
0 = \nabla G(X_{k+1}) = \nabla F(X) - H_k -r Kz_k + rX. 
\end{equation}
Namely, we apply the gradient descent by the following procedure: 
\begin{equation} 
X_{k+1} = X_k - \gamma \nabla G(X_k) = X_k - \gamma (\nabla F(X_k) - H_k - rKz_k + rX_k). 
\end{equation} 
If we set $X_k = Kz_k$, then we have that
\begin{eqnarray*} 
X_{k+1} &=& X_k - \gamma \nabla G(X_k) = X_k - \gamma (\nabla F(X_k) - H_k - rKz_k + rX_k) \\
&=& Kz_k - \gamma(\nabla F(X_k) - H_k).   
\end{eqnarray*}
This is the choice of algorithm introduced in \cite{mishchenko2022proxskip}.  
We shall consider to apply the inexact block Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
N = \begin{pmatrix}
D_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad N^{*} = \begin{pmatrix}
D_r^{*} & 0\\
(r K^T K)^{-1} r K^T D_r^{*} & r^{-1} (K^T K)^{-1} 
\end{pmatrix}.  
\end{equation}
The inexact Uzawa iteration can then be defined as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + N^{*} (-B^T H_k - \nabla G(U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that 
\begin{equation}
-B^TH_k - \nabla G(U_k) = \begin{pmatrix} H_k - A_r(X_k) + rK z_k \\ rK^T X_k - r K^T K z_k \end{pmatrix} 
\end{equation}
and thus 
\begin{eqnarray*}
&& N^{*}(-B^TH_k - \nabla G(U_k)) = N^{*} \begin{pmatrix} H_k - A_r(X_k) + rK z_k \\ rK^T X_k - r K^T K z_k \end{pmatrix} \\
&& \quad = 
\begin{pmatrix} D_r^{*} (H_k - A_r(X_k) + rK z_k) \\ (K^TK)^{-1} K^T D_r^{*} (H_k - A_r(X_k) + rK z_k) + (K^T K)^{-1}K^T X_k - z_k \end{pmatrix}
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
X_{k+1} &=& X_k + D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
z_{k+1} &=& (K^TK)^{-1} K^T (X_{k} + D_r^{*} (H_k - A_r(X_k) + r K z_k)) \\ 
H_{k+1} &=& H_k + \omega B U_{k+1} = H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
We shall apply $K$ to $z$ equation to obtain: 
\begin{eqnarray*}
X_{k+1} &=& X_k + D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
Kz_{k+1} &=& P_Z (X_{k} + D_r^{*} (H_k - A_r(X_k) + r K z_k)) \\ 
H_{k+1} &=& H_k + \omega B U_{k+1} = H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& X_* + D_r^{*} (H_* - A_r(X_*) + r K z_*) \\
Kz_{*} &=& P_Z (X_* + D_r^{*} (H_* - A_r(X_*) + rK z_*)) \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_k + D_r^{*} (H_* - A_r(X_*) + r K z_*) - D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z \left \{ X_* - X_k + D_r^{*} (H_* - A_r(X_*) + rK z_*) - D_r^{*} (H_k - A_r(X_k) + rK z_k) \right \} \\
H_{*} - H_{k+1} &=& H_* - H_k + \omega (-(X_{*} - X_{k+1}) + K(z_{*} - z_{k+1})) \\ 
&=& H_* - H_k - \omega (X_{*} - X_{k+1}) + \omega K( z_{*} - z_{k+1}). 
\end{eqnarray*}

\subsubsection{The convergence analysis of a single step GD} In this section, we study the convergence analysis of the single step GD for the first block, i.e., $D_r^{-1} = \gamma$. We note that the main convergence analytical tool introduced in \cite{mishchenko2022proxskip} was the nonexpansiveness of the proximal operator. In this case, the error equation is given as follows: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_k + \gamma \left [(H_* - A_r(X_*) + r K z_*) - (H_k - A_r(X_k) + r K z_k) \right ] \\
&=& X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \\ 
Kz_{*} - Kz_{k+1} &=& P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \\
H_{*} - H_{k+1} &=& H_* - H_k + \omega (-(X_{*} - X_{k+1}) + K ( z_{*} - z_{k+1})). 
\end{eqnarray*}
The main convergence of interest lies in $z_k$ to $z_*$ and $H_k$ to $H_*$. Therefore, we shall not be much interested in the convergence of $X_k$ to $X_*$. This means that we are more or less interested in the convergence of $X_k$ in $P_Z$ norm, i.e., 
\begin{equation*}
\|E_k^X\|_{P_Z}^2 := \langle P_Z E_k^X, E_k^X \rangle \rightarrow 0 \quad \mbox{ as } \quad k \rightarrow \infty.
\end{equation*}
But, it is nothing else than the convergence of $z_k$ to $z_*$. Our plan is to establish the convergence of $H_k$ to $H_*$ and $z_k$ to $z_*$ and then use it to establish the convergence of $X_k$ to $X_*$. We first note that the following error bound holds. Under the condition on $\gamma$ given as follows: 
\begin{equation}
0 \leq \gamma \leq \frac{1}{L}, 
\end{equation}
Again, the trick is to rewrite $E_{k+1}^Z$ in a somewhat different form as follows: 
\begin{eqnarray*}
-\omega(Kz_{*} - Kz_{k+1}) = -\omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ).
\end{eqnarray*}
we have that 
\begin{eqnarray*}
\omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ) \| \\ 
&=& \|P_Z (H_* - H_k) - \omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ) \| \\
&=& \|P_Z (H_* - H_k) - \omega \left ( P_Z \left \{(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k) \right \} \right ) \| \\ 
&=& \|P_Z [(H_* - H_k) - \omega \left \{(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k) \right \}] \|. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|H_* - H_k - \omega Q_Z [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
&=& \|Q_Z \left \{ H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \right \} \| \\
&=& \|Q_Z \left \{ H_* - H_k - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma [H_* - H_k]] \right \} \| \\
\end{eqnarray*}
The main idea is not to use $A$, but to use the convexity of the functional for sufficiently small $\gamma > 0$:  
\begin{equation} 
V(x) = \frac{1}{2} \|x\|^2 - \gamma F(x).
\end{equation}
Therefore, we have for $\gamma \leq 1/L$, 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \omega \|Kz_* - Kz_{k+1}\| &\leq& \|H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
&\leq& |1 - \omega\gamma|\|H_* - H_k\| + \omega \|(I - \gamma A)(X_*) - (I - \gamma A)(X_k)\| \\
&\leq& |1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda) \omega \|X_* - X_k\| \\
&=& |1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda)  \|Kz_* - Kz_k\|_\omega. 
\end{eqnarray*}
\begin{remark}
The standard analysis presented in \cite{mishchenko2022proxskip} has used the fact that 
\begin{eqnarray*}
\|X_* - X_k - \gamma (A(X_*) - A(X_k)) \| \leq (1 - \gamma \lambda) \|X_* - X_k\|.
\end{eqnarray*}
This is indeed equivalent to the analysis presented in this paper. However, such a view can not be extended to multiple step GD case as will be discussed below. 
\end{remark}
This concludes that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq 
|1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda) \|Kz_* - Kz_k\|_\omega. 
%&\leq& \|H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
%&\leq& \| (I - \omega\gamma) (H_* - H_k) - \omega \{ X_* - X_k - \gamma (A(X_*) - A(X_k)) \} \| \\
%&\leq& {\rm abs}(1 - \omega \gamma) \|H_* - H_k\| + \omega \|X_* - X_k - \gamma (A(X_*) - A(X_k)) \| \\
%&\leq& {\rm abs}(1 - \omega \gamma) \|H_* - H_k\| + \omega (1 - \gamma \lambda)\|Kz_* - Kz_k\|. 
\end{eqnarray*}
Let us simply choose $r = \omega$ as has been done in \cite{mishchenko2022proxskip}. Then, we choose $r$ and $\gamma$ so that 
\begin{eqnarray*}
|1 - \omega \gamma| < 1 \quad \mbox{ or } \quad -1 < 1 - \omega \gamma < 1.  
\end{eqnarray*}
Furthermore, we can choose $\gamma$ so that 
\begin{equation}
1 - c_0 = \max \{ |1 - \omega \gamma|, 1 - \gamma \lambda \}. 
\end{equation}
Then, we have the linear convergence. Namely, there exists $c_0$ such that 
\begin{equation}
[\|Kz_k - Kz_*\|_\omega + \|H_k - H_*\|] \leq (1 - c_0) [ \|Kz_{k-1} - Kz_*\|_\omega + \|H_{k-1} - H_*\| ], \quad \forall k = 1,\cdots. 
\end{equation}
Lastly, we can obtain the convergence of $X_k$ to $X_*$ from this result: 
\begin{eqnarray*}
\|X_{*} - X_{k+1}\| &=& \|X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))] \| \\
&\leq& \|X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \| \\ 
&\leq& \|X_* - X_k - \gamma (A(X_*) - A(X_k))\| + \gamma \|H_* - H_k\| \\
&\leq& (1 - \gamma \lambda) \|X_* - X_k\| + \gamma \|H_* - H_k\|.
\end{eqnarray*}
Therefore, we can show that $\|X_* - X_{k}\|$ also converges.  This completes the proof. 

\subsubsection{The convergence analysis for nonlinear solver}
We now look at the general case for $D_r$. As presented in \cite{mishchenko2022proxskip}, we shall set $X_k = Kz_k$ when we begin the GD step for the variable $X$. This includes a total of $N$-step GD for finding zero of the following nonlinear equation for a given $z_k$ and $H_k$:
\begin{equation} 
\nabla F(X) + r X = rKz_k + H_k 
\end{equation} 
reads as follows: with $X_k = Kz_k$ and $\gamma > 0$,   
\begin{eqnarray*} 
X_{k+\frac{1}{N}} &=& X_{k} + \gamma (H_k - A(X_k)) = [(I - \gamma A)(X_k) + \gamma H_k] \\ 
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} + \gamma (H_k - A(X_{k+\frac{1}{N}})) = [(I - \gamma A)(X_{k+\frac{1}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} + \gamma (H_k - A(X_{k+\frac{N-2}{N}})) =  [(I - \gamma A)(X_{k+\frac{N-2}{N}}) + \gamma H_k] \\  \\
X_{k+\frac{N}{N}} &=& X_{k+\frac{N-1}{N}} + \gamma (H_k - A(X_{k+\frac{N-1}{N}})) = [(I - \gamma A)(X_{k+\frac{N-1}{N}}) + \gamma H_k] \\ 
\end{eqnarray*}
Therefore, we see that its action is independent of $Kz_k$ in this setting, but it depends on $H_k$. The action of $D_r^*$ is important for the analysis. We shall interpret it as follows: 
\begin{equation} 
X_{k+1} = X_k + D_r^*(H_k - A(X_k)).  
\end{equation} 
%where 
%\begin{eqnarray*} 
%D_r^* (H_k - A(X_k)) &:=& \gamma (H_k - A(X_k))  \\ 
%&+& \gamma (H_k - A((I - \gamma A)(X_k) + \gamma H_k)) \\ 
%&+& \gamma (H_k - A((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k))) \\
%&+& \gamma (H_k - A((I - \gamma A)^2((I - \gamma A)(X_k) + \gamma H_k))) \\ 
%&+& \cdots \\
%&+& \gamma (H_k - A((I - \gamma A)^{N-2}((I - \gamma A)(X_k) + \gamma H_k))) \\
%&=& \gamma N H_k - \gamma \sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_k )
%\end{eqnarray*} 
For example, if $N = 2$, then 
\begin{equation} 
X_{k+1} = (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k. 
\end{equation}
and for example, if $N = 3$, then 
\begin{eqnarray*} 
X_{k+1} = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}


We note that $A(X_*) - H_* = \nabla F(X_*) - H_* = 0$ and the 
error equation is given as follows:  
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_{k} + D_r^*(H_* - A(X_*)) - D_r^* (H_k - A(X_k)) \\ 
Kz_* - Kz_{k+1} &=& P_Z (X_{*} - X_{k} + D_r^*(H_* - A(X_*)) - D_r^* (H_k - A(X_k)) \\ 
H_* - H_{k+1} &=& H_* - H_k - \omega (X_* - X_{k+1}) + \omega K(z_* - z_{k+1}). 
\end{eqnarray*}
On the other hand, we have as for the GS or a single step GD case,
\begin{eqnarray*}
Kz_{*} - Kz_{k+1} = P_Z \left ( X_* - X_{k+1} \right ). 
\end{eqnarray*}
and thus
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z \left ( X_* - X_{k+1} \right ) \right ). 
\end{eqnarray*}
This leads to the following bound:  
\begin{eqnarray*}
\omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left ( P_Z [X_* - X_k + D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_k) + rKz_k)] \right ) \| \\
%&=& \|-\omega \left ( P_Z [X_* - X_k + D_r^* (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_*) + rKz_*) \right.  \\
%&& + D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*)   \\
%&& \left. + D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) ] \right ) \| \\
&=& \|P_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))].
%&& - \omega (X_* - X_k + \left [ D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*) \right ] ) \\ 
%&& - \omega \left ( D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) \right ) ) \|. 
%
%\left ( (H_* - H_k) -\omega [X_* - X_k + D_r^* (H_k - A_r(X_*) + rKz_k) - D_r^* (H_k - A_r(X_k) + rKz_k) \right.  \\
%&& + \left. D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_*) + rKz_k) \right )  \| \\
%&\leq& \|P_Z [X_* - X_k + D_r^* (H_k - A_r(X_*) + rKz_k) - D_r^* (H_k - A_r(X_k) + rKz_k)] \| \\
%&& + \|P_Z[ D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_*) + rKz_k)]\| \\
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|Q_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_k) + rKz_k)] \| \\  
&=& \|Q_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k)] \|. 
%&& - \omega (X_* - X_k + \left [ D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*) \right ] ) \\ 
%&& - \omega \left ( D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) \right ) ) \| \\ 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq \\ && \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))] \| \\
&=& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k)) + D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\ 
&=& \|H_* - H_k - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-1}{N}}) + \gamma (H_* - H_k)] \| \\ 
&=& \|(1 - \omega \gamma) (H_* - H_k) - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-1}{N}})] \|.  
\\
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|X_* - X_{k+\frac{N-1}{N}}\|  \\ 
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-2}{N}}) + \gamma (H_* - H_k)\|  \\
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda)^2 \|X_* - X_{k+\frac{N-2}{N}} \| + \omega \gamma (1 - \gamma \lambda) \|H_* - H_k\| \\
&\leq& (1 - \gamma\omega) \|H_* - H_k\| + \omega ( 1- \gamma \lambda)^N \|X_* - X_k\| + \omega\gamma ((1 - \gamma \lambda) + \cdots + (1 - \gamma \lambda)^{N-1}) \|H_* - H_k\|. \end{eqnarray*}
We shall choose $\omega = 1/\gamma$. Then, we have that \begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} &\leq& \omega ( 1- \gamma \lambda)^N \|X_* - X_k\| + \omega\gamma \sum_{\mu = 1}^{N-1} (1 - \gamma \lambda)^\mu \|H_* - H_k\|.  
\end{eqnarray*}


Therefore, the convergence boils down to establish that there exists $c_0 > 0$ such that 
\begin{eqnarray*}
&& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))] \| \\
&&\qquad \leq (1 - c_0) \left ( \|H_{*} - H_{k}\| + \|Kz_* - Kz_{k}\|_{\omega} \right ). 
\end{eqnarray*}
We observe that for $N = 2$, we have that 
\begin{eqnarray*}
&&  \|H_* - H_k - \omega [X_* - X_k + \{ \gamma ( H_* - A(X_*) ) + \gamma (H_* - A((I - \gamma A)(X_*) + \gamma H_*)) \} \\
&& 
- \{ \gamma ( H_k - A(X_k)) + \gamma (H_k - A((I - \gamma A)(X_k))) \}
\end{eqnarray*}
\textcolor{red}{The trick is to target the second expression of $A$ with input given as $(I - \gamma A)(X)$:} 
\begin{eqnarray*}
&&  \|H_* - H_k - \omega [(I - \gamma A)(X_*) + \gamma H_* +  \gamma (H_* - A((I - \gamma A)(X_*) + \gamma H_*)) \} \\
&& 
- \{(I - \gamma A)(X_k) + \gamma H_k + \gamma (H_k - A((I - \gamma A)(X_k))) \} \| \\ 
&=& \|(I - \omega \gamma) (H_* - H_k) - \omega [[(I - \gamma A)(X_*) + \gamma H_*] - [(I - \gamma A)(X_k) + \gamma H_k] \\
&& - \gamma ( A((I - \gamma A)(X_*) + \gamma H_*) - A((I - \gamma A)(X_k)) + \gamma H_k) \} \\ 
&\leq& |I - \omega \gamma |\|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k)\| \\ 
&\leq& (1 - \omega \gamma) \|H_* - H_k\| + (1 - \gamma \lambda)^2 \|X_* - X_k\|_\omega + \gamma \omega (1 - \gamma \lambda) \|H_* - H_k\| \\ 
&=& (1 - \gamma^2 \lambda) \|H_* - H_k\| + (1 - \gamma \lambda)^2 \|X_* - X_k\|_\omega. 
\end{eqnarray*}

%&& \qquad - \{ \gamma N H_k - \gamma \sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_k ) \}] \| \\
%&&  \|(I - \omega \gamma N ) (H_* - H_k) - \omega [X_* - X_k - \gamma \{\sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_*) + \gamma (I - \gamma A)^{\ell-2} H_* )\} \\ 
%&& \qquad - \{\sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_* ) \}] \| \\
%&&\qquad \leq (1 - c_0) \left ( \|H_{*} - H_{k}\| + \|Kz_* - Kz_{k}\|_{\omega} \right ). 
%\end{eqnarray*}



This leads that that there exists $c_0 > 0$ such that
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\|_\omega + \|H_* - H_{k+1}\| \leq (1 - c_0) \left [ \|Kz_* - Kz_k\|_\omega + \|H_* - H_k\| \right ]. 
\end{eqnarray*}
This completes the proof. 

We now consider the case $N = 3$. 
\textcolor{red}{The trick is to target the second expression of $A$ with input given as $(I - \gamma A)(X)$:} 
\begin{eqnarray*}
&&  \|(1 - 2\gamma \omega) (H_* - H_k) - \omega [(I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - \gamma A((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*)] \\
&& - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) - \gamma A((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) \| \\ 
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) \| \\
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda)^2 \|(I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)(X_k) - \gamma H_k) \| \\
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda)^3 \|X_* - X_k\| + \gamma \omega (1 - \gamma \lambda)^2 \|H_* - H_k\|. 
\end{eqnarray*}
This gives 
\begin{eqnarray*} 
\|H_* - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} &\leq& (1 - 2\gamma \omega + \gamma \omega (1 - \gamma\lambda)^2) \|H_* - H_k\| \\
&& \quad + (1 - \gamma \lambda)^3 \|Kz_* - Kz_k\|_{\omega}. 
\end{eqnarray*} 
and thus 
\begin{equation} 
\|H_* - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq (1 - \gamma \omega) \|H_* - H_k\| + (1 - \gamma \lambda)^3 \|Kz_* - Kz_k\|_{\omega}. 
\end{equation} 
This completes the proof. 

\begin{eqnarray*}
&& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k)) + D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\ 
&& \|H_* - H_k - \omega [D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\
&& + 
\omega \|X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k))] \|. 
\end{eqnarray*}
We shall analyze one by one as follows: 
\begin{eqnarray*}
\|H_* - H_k - \omega [D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| &=& \\  
\end{eqnarray*}
For $N = 2$, we get for $\gamma \leq 1/L$, 
\begin{equation} 
(1 - 2\gamma \omega) \|H_* - H_k\| + \omega \gamma^2 L \|H^* - H_k\| \leq (1 - 2\gamma \omega) \|H_* - H_k\| + \omega \gamma \|H^* - H_k\| = (1 - \gamma \omega) \|H_* - H_k\|. 
\end{equation} 

\bibliographystyle{plain}
\bibliography{mybib}

%\end{document} 


\subsubsection{The convergence analysis for multiple GD steps}
We now look at the general case for $D_r$. As presented in \cite{mishchenko2022proxskip}, we shall set $X_k = Kz_k$ when we begin the GD step for the variable $X$. A total of $N$-step GD for finding zero of the following nonlinear function: 
\begin{equation} 
\nabla F(X) + r X = rKz_k + H_k 
\end{equation} 
reads as follows: with $\gamma > 0$, 
\begin{eqnarray*} 
X_{k+1} = X_{k} + N \gamma H_k - \gamma \left ( \sum_{\mu = 0}^{N-1} A\left ( X_{k + \mu/N}\right ) \right ).
\end{eqnarray*}
We note that $A(X_*) - H_* = \nabla F(X_*) - H_* = 0$ and the 
error equation is given as follows:  
\begin{eqnarray*}
X_{*} - X_{k+(\ell+1)/N} &=& X_* - X_{k + \ell/N} + \gamma (H_* - A(X_*)) - \gamma(H_{k + \ell/N} - A(X_{k + \ell/N})) \\
&=& X_* - X_{k+\ell/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + \ell/N})),  
\end{eqnarray*}
where $\ell = 0,1,\cdots,N-1$. More precisely, we have that 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) \\ 
X_{*} - X_{k+(N-1)/N} &=& X_* - X_{k + (N-2)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-2)/N})) \\  
X_{*} - X_{k+(N-2)/N} &=& X_* - X_{k + (N-3)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-3)/N})) \\  
&\vdots& \\ 
X_{*} - X_{k+1/N} &=& X_* - X_{k} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k})).  
\end{eqnarray*}
On the other hand, we have as for the GS or a single step GD case,
\begin{eqnarray*}
Kz_{*} - Kz_{k+1} = P_Z \left ( X_* - X_{k+1} \right ). 
\end{eqnarray*}
This means that in order to find out the relationship between $E_{k+1}^Z$ and $E_k^Z$, we must analyze the relationship between $E_{k+1}^X$ and $E_{k}^X$, which boils down to the following estimate under the product of $P_Z$. For all $\ell = 0,1,\cdots,N-1$, we have for $0 < \gamma \leq 1/L$, 
\begin{eqnarray*}
\|X_{*} - X_{k+(\ell+1)/N}\|_{P_Z} &=& \|X_* - X_{k+\ell/N} + \gamma (H_* - A(X_*)) - \gamma(H_{k} - A(X_{k+\ell/N}))\|_{P_Z} \\
&\leq& \|X_* - X_{k+\ell/N} - \gamma (A(X_*) - A(X_{k+\ell/N}))\|_{P_Z} \\ 
&\leq& (1 - \gamma \lambda) \|X_* - X_{k+\ell/N}\|_{P_Z}.  
\end{eqnarray*}
\textcolor{red}{This has to be proven !} 
This gives that 
\begin{eqnarray*}
\|X_{*} - X_{k+1/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k}\|_{P_Z}   \\ 
\|X_{*} - X_{k+2/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k+1/N}\|_{P_Z}   \\ 
\|X_{*} - X_{k+3/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k+2/N}\|_{P_Z}  \\ 
&\vdots& \\
\|X_{*} - X_{k+1}\|_{P_Z} &\leq& (1 - \gamma \lambda)^N \|X_* - X_{k+(N-1)/N}\|_{P_Z}.  
\end{eqnarray*}
This gives that 
\begin{eqnarray*}
\|X_* - X_{k+1}\|_{P_Z} &\leq& (1 - \gamma \lambda)^N \|X_* - X_k\|_{P_Z} \\ 
&=& (1 - \gamma \lambda)^N \|X_* - X_k\|_{P_Z}.  
\end{eqnarray*}
Therefore, we have 
\begin{eqnarray*}
\|Kz_{*} - Kz_{k+1}\| = \|X_* - X_{k+1}\|_{P_Z} \leq (1 - \gamma \lambda)^N \|Kz_* - Kz_k\| 
\end{eqnarray*}
and we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|H_* - H_k - \omega Q_Z (X_* - X_{k+1})\| = \|Q_Z [(H_* - H_k) - \omega (X_* - X_{k+1})]\| \\ 
&=& \|Q_Z [(H_* - H_k) - \omega [ X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) ]] \| \\ 
&=& \|Q_Z [(H_* - H_k) - \omega [ X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) ]\}\| \\ 
&\leq& \|Q_Z [(I - \omega \gamma)(H_* - H_k) - \omega (X_* - X_{k + (N-1)/N} - \gamma (A (X_*) - A(X_{k + (N-1)/N}))) ]\| \\ 
&\leq& |1 - \omega \gamma| \|H_* - H_k\| + \omega \|Q_Z[X_* - X_{k + (N-1)/N} - \gamma (A (X_*) - A(X_{k + (N-1)/N}))]\|.
\end{eqnarray*}
Therefore, we have 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\|_{1/r} &\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + (1 - \gamma\lambda) \|X_* - X_{k + (N-1)/N}\| \\
&\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + (1 - \gamma \lambda) \|X_* - X_{k + (N-2)/N} - \gamma (A(X_*) - A(X_{k + (N-2)/N})) + \gamma(H_* - H_k)\| \\
&\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda) \left \{ (1 - \gamma \lambda) \|X_* - X_{k + (N-2)/N}\| + \gamma\|H_* - H_k\| \right \} \\
&=& [|1 - \omega \gamma| + \gamma (1 - \gamma \lambda) \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^2 \|X_* - X_{k + (N-2)/N}\| \\ 
&=& [|1 - \omega \gamma| + \gamma (1 - \gamma \lambda) + \gamma (1 - \gamma \lambda)^2) \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^3 \|X_* - X_{k + (N-3)/N}\| \\ 
&=& \cdots \\ 
&\leq& [|1 - \omega \gamma| + \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^N \|X_* - X_{k}\| \\ 
&=&  [|1 - \omega \gamma| + \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\|.  
\end{eqnarray*}
This gives that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &\leq& [|1 - \omega \gamma| + \omega \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\| + \omega ( 1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\| \\ 
&=& \left ( |1 - \omega \gamma| + \frac{\omega}{\lambda} ((1 - \gamma \lambda) - (1 - \gamma \lambda)^N) \right ) \|H_* - H_k\| + \omega (1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\|. 
\end{eqnarray*}
Therefore, we arrive at the conclusion that 
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\| + \|H_* - H_{k+1}\| \leq 
\left ( (1 + \omega) (1 - \gamma \lambda)^N \right ) \|Kz_* - Kz_k\| + \left (|1 - \omega \gamma| + \frac{\omega}{\lambda} (1 - (\gamma\lambda)^N) \right ) \|H_* - H_k\|_r. 
\end{eqnarray*}
We note that the coefficient $(1 + \omega) ( 1- \gamma \lambda)^N$ can be made to be smaller than one easily. However, the second term needs a care and our goal is to make 
\begin{equation} 
|1 - \omega\gamma| + \frac{\omega}{\lambda} < 1 
\end{equation} 
Note that if $|1 - \omega\gamma| = 1 - \omega\gamma$, then we do not have a chance. But $|1 - \omega\gamma| = \omega\gamma - 1$ leads that 
\begin{equation}
\omega \gamma - 1 + \frac{\omega}{\lambda} < 1 
\end{equation}
to satisfy that there exists $c_0 > 0$ such that
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\| + \|H_* - H_{k+1}\|_r \leq (1 - c_0) \left [ \|Kz_* - Kz_k\| + \|H_* - H_k\|_r \right ]. 
\end{eqnarray*}
This completes the proof. 
\begin{remark}
We should use $M_2$ to eliminate $H$ part to make tighter bounds. In fact, we can use the $M_2$ inner product here to eliminate $H$ part.  
\end{remark}

\begin{itemize}
\item Does the convergence rate reduce to exact Gauss-Seidel case when $N\rightarrow \infty$ ? 
\end{itemize}

%\end{comment} 

\bibliographystyle{plain}
\bibliography{mybib}

%\end{document}
\subsection{Linear Case handling $(X,z)$ and $H$ block} 
We now consider the standard approach to handle the following system: 
\begin{equation}
\begin{pmatrix}
A_r & - r K & -I \\
-r K^T& r K^TK & K^T\\
-I& K & 0\\
\end{pmatrix}
\begin{pmatrix}
X_*\\
z_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}. 
\end{equation}
For simplicity, we consider the following block system: 
\begin{equation}
\begin{pmatrix}
\nabla G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0
\end{pmatrix}
\end{equation}
where 
\begin{equation}
\nabla G = \begin{pmatrix}
A_r & -r K\\
-r K^T & rK^T K
\end{pmatrix}, \quad \mbox{ and } \quad 
B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
We note that the Schur complement is given as follows:   
\begin{equation}
S = -B \nabla G^{*} (-B^T) 
\end{equation}
We write an equivalent form of the above equation using the Schur complement system. We shall consider to apply the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
L = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad L^{-1} = \begin{pmatrix}
A_r^{-1} & 0\\
(rK^T K)^{-1} rK^T A_r^{-1} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}

% \begin{equation}
% \begin{aligned}
%       A^{-1} & = \begin{pmatrix}
%      A_r^{-1} + A_r^{-1} rK ( rK^TK - rK^T A_r^{-1} rK)^{-1} rK^T A_r^{-1} & -  A_r^{-1} rK (rK^TK - rK^T A_r^{-1} rK)^{-1}\\
%      (rK^TK - rK^T A_r^{-1} rK)^{-1} rK^T A_r^{-1}  & (rK^TK - rK^T A_r^{-1} rK)^{-1}
%     \end{pmatrix}, \\
%      & = \begin{pmatrix}
%      A_r^{-1} + r A_r^{-1} K (rP)^{-1} K^T rA_r^{-1} & -  r A_r^{-1} K (rP)^{-1} \\
%      (rP)^{-1} rK^T A_r^{-1}  & (rP)^{-1}
%      \end{pmatrix},
% \end{aligned}
% \end{equation}
% where $P = K^TK - K^T rA_r^{-1} K$ is symmetric positive definite and has spectral radius less than 1. 

% \begin{lemma}
% The Schur complement S is symmetric positive definite. 
% \end{lemma}
% \begin{proof}
% It is shown in Lemma \ref{lemma4} that $A^{-1}$ is symmetric positive definite.
% Since $B^T$ is onto, we must have $B A^{-1} B^T$ being symmetric positive definite. 
% \end{proof}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + L^{-1} (-B^T H_k - \nabla G( U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that the exact solutions satiafy \begin{eqnarray}
U_{*} &=& U_{*} + L^{-1} (-B^T H_{*}-\nabla G (U_{*})) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray}
Thus, with the convention that $E_{k}^U = U - U_k$ and $E_{k}^H = H - H_k$, we obtain the error equations: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + L^{-1} (-\nabla G(E^U_k) -B^T E^H_k) \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray}
\begin{comment} 
In the matrix form, we have 
\begin{equation}
    \begin{pmatrix} 
     I  & 0 \\
      -\omega B & I
    \end{pmatrix} 
        \begin{pmatrix} 
      E^U_{k+1} \\
       E^H_{k+1}
    \end{pmatrix}  = 
        \begin{pmatrix} 
     I - L^{-1} A & -L^{-1} B^T \\
    0 & I
    \end{pmatrix} 
            \begin{pmatrix} 
      E^U_{k} \\
       E^H_{k}
    \end{pmatrix} 
\end{equation}
\end{comment} 
In the other direction, we have that 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + L^{-1} (- \nabla G(E^U_k) - B^T E^H_k) \\
          &=& E^U_k - L^{-1} \nabla G(E^U_k) - L^{-1} B^T E^H_k \\
E^H_{k+1} &=& E^H_k + \omega \left [ B (E^U_k + L^{-1} (- \nabla G(E_k^U) - B^T E^H_k) \right ] \\
&=& E^H_k - \omega B L^{-1} B^T E_k^H + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
\end{eqnarray}

% Directly taking $A-$norm: 
% \begin{eqnarray}
% \| E^U_{k+1} \|_A &\leq& \| I - L^{-1} \nabla G \| \|E^U_k\|_A  + \| L^{-1} B^T \|_A \|E^H_k\|_A \\
% \| E^H_{k+1}\| &\leq& \|I - \omega B L^{-1} B^T \| \|E_k^H\| + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
% \end{eqnarray}

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

\begin{equation}
    \|M \|_A \leq \sqrt{ \kappa(A)} \| M \|_2
\end{equation}
We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \lambda_{max}(A) \| L^{-1} B^T H\| \\
   & =\lambda_{max}(A) (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}
We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}
\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}
\textcolor{red}{ Here !} 

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \langle A L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix},  L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix} \rangle  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}

\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}


\subsubsection{Nonlinear Case} 
For simplicity, we consider the following block system: 
\begin{equation}
\begin{pmatrix}
\nabla G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0
\end{pmatrix}
\end{equation}
where 
\begin{equation}
\nabla G = \begin{pmatrix}
A_r & -r K\\
-r K^T & rK^T K
\end{pmatrix}, \quad \mbox{ and } \quad 
B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
We shall consider to apply the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
L = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad L^{*} = \begin{pmatrix}
A_r^{*} & 0\\
(rK^T K)^{-1} rK^T A_r^{*} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}
We note that the contraction property requires the expansion of $L$, $\nabla G$ and $L^*$. Thus, we list their computation below: 
\begin{eqnarray*}
\nabla G(U) &=& 
\begin{pmatrix}
A_r(X) - r K z \\ 
-r K^T X + r K^T K z 
\end{pmatrix} \\ 
L(U) &=& 
\begin{pmatrix}
A_r(X) \\ 
-r K^T X + r K^T K z 
\end{pmatrix} \\ 
L^*(U) &=& 
\begin{pmatrix}
A_r^*(X) \\ 
(r K^TK)^{-1}r K^T A_r^{*}(X) + r^{-1} (K^T K)^{-1} z 
\end{pmatrix} 
\end{eqnarray*}
% where $P = K^TK - K^T rA_r^{-1} K$ is symmetric positive definite and has spectral radius less than 1. 

% \begin{lemma}
% The Schur complement S is symmetric positive definite. 
% \end{lemma}
% \begin{proof}
% It is shown in Lemma \ref{lemma4} that $A^{-1}$ is symmetric positive definite.
% Since $B^T$ is onto, we must have $B A^{-1} B^T$ being symmetric positive definite. 
% \end{proof}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray*}
U_{k+1} &=& U_k + L^{*} (-B^T H_k - \nabla G(U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray*}
We note that the exact solutions satiafy 
\begin{eqnarray*}
U_{*} &=& U_{*} + L^{*} (-B^T H_{*}-\nabla G (U_{*})) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray*}
Thus, with the convention that $E_{k}^U = U - U_k$ and $E_{k}^H = H - H_k$, we obtain the error equations: 
\begin{eqnarray*}
U_* - U_{k+1} &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B (U_* - U_{k+1}). 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
U_* - U_{k+1} &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B \left [U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) \right. \\
&& \left. - L^{*} (-\nabla G(U_k) -B^T H_k) \right ]. 
\end{eqnarray*}
This gives that 
Therefore, we have that 
\begin{eqnarray*}
\langle U_* - U_{k+1}, \nabla G(U_*) - \nabla G(U_{k+1}) \rangle &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B \left [U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) \right. \\
&& \left. - L^{*} (-\nabla G(U_k) -B^T H_k) \right ]. 
\end{eqnarray*}



A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\| &\leq& \rho_U \|E_k^U\| + \|L^{-1} B^T E_k^H\| \\ 
&\leq&  \rho_U \|E_k^U\| + \frac{r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}

 
\subsection{A New Proof based on Dual operator}

Under the Uzawa framework. 
For simplicity, we denote the nonlinear operator $\nabla F(\cdot) + r I$ as $A_r$, $[z, H]^T$ as $U$,  and rewrite the matrix as follows.
\begin{equation}
\label{optimality condition aug Lag matrix form 2 by 2}
    \begin{pmatrix}
    A_r & B^T \\
    B & C
    \end{pmatrix}
    \begin{pmatrix}
    X\\
    U 
    \end{pmatrix} = 
    \begin{pmatrix}
    0 \\
    0,
    \end{pmatrix}
\end{equation}
where 
\begin{equation}
A_r = \nabla F(\cdot) + r I, \quad B = \begin{pmatrix}
-r K^T\\ -I
\end{pmatrix}, \quad C = \begin{pmatrix}
r K^T K & K^T \\
K & 0
\end{pmatrix}.
\end{equation}
We view the ADMM as a type of Uzawa iteration that solves Schur complement operator with a different iterative method. Namely, we notice that the system can be given as follows: 
\begin{subeqnarray*}
0 &=& A_r X  + B^T U \\
0 &=& \left [ B A_r^* (-B^T) (U) + CU \right ] = S(U). 
\end{subeqnarray*}
Under this setting, we can understand that the Uzawa iteration with an iterative method for the Schur complement is given in the following form: 
\begin{equation}\label{UzawaADMM}
\begin{cases}
A_r X_{k+1} + B^T U_k = 0 \quad \mbox{ or equivalently } \quad X_{k+1} = A_r^*(-B^T)( U_k) \\
U_{k+1} = U_k + N^{-1} \left( - \left [ B A_r^*(-B^T)(U_k) + CU_k \right] \right) = U_k + N^{-1} \left( - (B X_{k+1} + C U_k)\right),
\end{cases}
\end{equation}
where $N$ is some approximation of the nonlinear Schur complement operator. Note that our choice of $N$ will be given as follows: 
\begin{equation}\label{shurH} 
N = \begin{pmatrix}
r K^T K & 0\\
K & - \frac{1}{r} I
\end{pmatrix} \quad \mbox{ and } \quad  N^{-1} = \begin{pmatrix}
(r K^T K)^{-1} &  0 \\
 r K (r K^T K)^{-1} & - r I
\end{pmatrix}. 
\end{equation}
Such an iterative method corresponds to a damped Gauss-Seidel type inexact solver for the Schur complement that solves the first variable $z_{k+1}$ and then using Richardon's iteration with step size $r$ for the second variable $H_{k+1}$. We now show that this is exactly the ADMM method given in Algorithm \ref{algADMM1}.

\begin{proposition}\label{prop: Uzawa iterative solver}
The ADMM method in Algorithm \ref{algADMM1} is equivalent to the Uzawa iterations \eqref{UzawaADMM} with the choice of $N$ defined as in \eqref{shurH}.
\end{proposition}
\begin{proof}
We begin by writing the Uzawa iterations \eqref{UzawaADMM} for each variable update. We note that the second iteration in \eqref{UzawaADMM}  can be expanded as follows.

\begin{equation}
    \begin{pmatrix}
    z_{k+1}\\
    H_{k+1}
    \end{pmatrix} = \begin{pmatrix}
    z_{k}\\H_{k}
    \end{pmatrix} + \begin{pmatrix}
    (r K^T K)^{-1} & 0 \\
   r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
     r K^T X_{k+1} - r K^T K z_k - K^T H_k\\
     X_{k+1} - Kz_k
    \end{pmatrix}
\end{equation}
We can then verify that the iterations indeed are the same as ADMM iterations. Namely, we have for $X_{k+1}$ update, that $A_r X_{k+1} - r Kz_{k} - H_k = 0 $, that is 
\begin{equation}
\nabla F(X_{k+1}) + r X_{k+1} - r K z_k - H_k = 0
\end{equation}
and for $z_{k+1}$ update, that \begin{equation}
\begin{aligned}
z_{k+1} &= z_k + (r K^T K)^{-1} (r K^T X_{k+1} - K^T H_k) - z_k \\
& = (r K^T K)^{-1} (r K^T X_{k+1} - K^T H_k)
\end{aligned}
\end{equation}
and for $H_{k+1}$ update, that  \begin{equation}
\begin{aligned}
H_{k+1} & = H_k +  r K(r K^T K )^{-1}\left( r K X_{k+1} - r K^T K z_k - K^T H_k \right) -r (X_{k+1} - Kz_k) \\
                & = H_k + r K(r K^T K )^{-1} (r K X_{k+1} - K^T H_k) - r K z_k - r (X_{k+1} - Kz_k) \\
                & = H_k +r K z_{k+1} -r X_{k+1}.
\end{aligned}
\end{equation}
Thus, we have 
\begin{equation}
H_{k+1} - H_k -r(Kz_{k+1} - X_{k+1})=0.
\end{equation}
This completes the proof. 
\end{proof}
We shall now set $E_k^U = U - U_k$ and $E_k^X = X - X_k$ and then shall drive the error equation for the above algorithm. First we note that 
\begin{equation}
X = A_r^* (-B^TH) \quad \mbox{ and } \quad X_{k} = A_r^* (-B^T H_{k-1})
\end{equation}
Thus, we have that 
\begin{eqnarray*}
\|X - X_k\|^2 = \|A_r^* (-B^TU) - A_r^* (-B^T U_{k-1})\|^{2} \leq \frac{1}{c_0 + r} \|B^T( U - U_{k-1})\|^2 
\end{eqnarray*}
Therefore, we shall only need to analyze the convergence of $E_k^U$ for the convergence of $E_k^X$. On the other hand, the error propagation operator for $E_k^U$ can be given as follows: 
\begin{equation}
E_{k+1}^U = E_k^U - N^{-1} (S(U) - S(U_k)). 
\end{equation}
We now introduce an inner product defined on the space $U = \{(z_k, H_k)_{k=1,\cdots}, (z_*, H_*) \}$ by 
\begin{equation}
\langle \overline{N} U_k, U_k \rangle = \left \langle 
\begin{pmatrix}
I & 0\\
0 & -I
\end{pmatrix}
\begin{pmatrix}
r K^T K & 0\\
K & -\frac{1}{r} I
\end{pmatrix} \begin{pmatrix}
z_k \\
H_k
\end{pmatrix},  \begin{pmatrix}
z_k \\
H_k
\end{pmatrix} \right \rangle = r \langle Kz, Kz \rangle + \frac{1}{r} \langle H, H \rangle  
\end{equation}
Therefore, it makes a norm. We now notice that
\begin{eqnarray} 
\overline{N} E_{k+1}^U = \overline{N} E_k^U - (\overline{S}(U) - \overline{S}(U_k)), 
\end{eqnarray} 
where 
\begin{equation} 
\overline{S} = \begin{pmatrix}
I & 0\\
0 & -I
\end{pmatrix} S.
\end{equation} 
This indicates that 
\begin{eqnarray*}
\|\overline{N} E_{k+1}^U\|^2 = \overline{N} E_k^U - (\overline{S}(U) - \overline{S}(U_k)), 
\end{eqnarray*}

\begin{comment} 
\begin{lemma}
For the error propagation matrix, we have $\rho(I - N^{-1}S) =\eta < 1$. 
\end{lemma}
\begin{proof}
\textcolor{red}{We found this part is not accurate and need to modify.
First, we observe that $I - N^{-1}S$ is given as follows:


where $E := K^T K = n I$, $Q_A = I - r A_r^{-1}$. We note that 

By $2\times 2$ block matrix decomposition, we have the error propagation matrix is spectrally equivalent to the following matrix
\begin{equation}\label{mat1}
    \begin{pmatrix}
   S1 & 0 \\
 0   &
 S_2 + S_3   \end{pmatrix},
\end{equation}
where $S_1 = I -   E^{-1} K^T Q_A K$, $S_2 = (I - M_2)Q_A $, $S_3 = (I- M_2) Q_A K (I - E^{-1} K^T Q_A K )^{-1} E^{-1} K^T Q_A $.

It is easy to see that $S_1$ is symmetric positive and has $\rho(S_1) < 1$.
We also have $\rho(S_2) \leq \frac{C_0}{r + C_0}$, $\rho(S_3) \leq \frac{C_0}{ r+C_0}\frac{r +C_0}{r} \frac{C_0}{r +C_0 }$. So $\rho(S_2 + S_3) \leq \frac{C_0}{r} \le 1 $ for sufficiently large $r$. 

We can then take $\eta = \min \{ \rho(S_1), \frac{C_0}{r} \}$. This completes the proof.}
\end{proof}
\end{comment} 

\subsubsection{linear case}
To analyze the above Uzawa iteration, the difficulty lies in the nonlinear operation $\nabla F(\cdot)$ since F is assumed to be strongly convex and L-smooth. We first consider a simpler case when $F(X) = \frac{1}{2} X^T A X - b^T X$ is a quadratic function, where $A$ is a symmetric positive definite matrix. In this case, we have a system of linear equations. 

\begin{equation}
\label{optimality condition in linear case}
    \begin{pmatrix}
    A_r & -r K & -I \\
    -r K^T& r K^TK & K^T\\
    -I& K & 0\\
    \end{pmatrix}
    \begin{pmatrix}
    X_*\\
    z_* \\
    H_*
    \end{pmatrix} = 
    \begin{pmatrix}
    f\\
    0\\
    0
    \end{pmatrix}
\end{equation}

\begin{lemma}
$I - rA_r^{-1}$ is symmetric positive definite, and $\rho(I - rA_r^{-1}) < \frac{c_{max}}{r + c_{max}}$, where $c_{max}$ is the largest eigenvalue of $A$. $\rho(A_r^{-1}) \leq \frac{1}{r + c_{min}}$, where $c_{min}$ is the smallest eigenvalue of $A$.
\end{lemma}

\begin{theorem}[Well-posedness]
The matrix \eqref{optimality condition in linear case} is invertible. 
\end{theorem}
\begin{proof}
We know that $A_r$ is invertible since it is positive definite. 
Using the same notation as in \eqref{optimality condition aug Lag matrix form 2 by 2}, it now suffices to prove that the Schur complement $S = C - B A_{r}^{-1} B^T$ is invertible. To see this, we write out the schur complement explicitly.
We have 
\begin{equation}
    B A_r^{-1} B^T = \begin{pmatrix}
    r^2 K^T A_r^{-1} K&  r K^T A_r^{-1}\\
    r A_r^{-1} K & A_r^{-1}
    \end{pmatrix}
\end{equation}
\begin{equation}
    S = \begin{pmatrix}
    r K^T \left(I - r A_{r}^{-1} \right)K  & K^T (I -  r A_{r}^{-1} ) \\
    (I -  r A_{r}^{-1} ) K  & - A_{r}^{-1}
    \end{pmatrix}
\end{equation}
Note that in $S$, the first diagonal block is positive definite because $I - r A_{r}^{-1}$ is positive definite. 

Now it suffices to again consider the Schur complement of $S$. We have 
\begin{equation}
    \tilde{S} = -A_r^{-1} - D \left[ r K^T \left(I -  r A_{r}^{-1} \right)K  \right] D^T  ,
\end{equation}
where $D = (I - (\frac{1}{r} A_r)^{-1}) K$.
Note that $D \left[ r K^T \left(I - (\frac{1}{r} A_{r})^{-1} \right)K  \right] D^T $ is positive semi-definite. Since $A_r$ is positive definite, we have $\tilde{S}$ is negative definite. 
\end{proof}

\subsubsection{Exact Uzawa for linear problems}
We consider the following exact Uzawa iteration,
\begin{equation}
    \begin{cases}
     X_{k+1} = A_r^{-1}f - A_r^{-1}B^T U_k, \quad A_r X_{k+1} +B^T U_k = f \\
     U_{k+1} = U_k + N^{-1} \left( B A_r^{-1} f - (C - B A_r^{-1}B^T)U_k \right) = U_k + N^{-1} \left( - B X_{k+1} - C U_k\right),
    \end{cases}
\end{equation}
where $N$ is some approximation of the Schur complement operator given by 
\begin{equation}
    N = \begin{pmatrix}
    r K^T K & 0\\
     K & - \frac{1}{r} I
    \end{pmatrix} = \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix}^{-1}.
\end{equation}

The exact solution satisfies 
\begin{equation}
    \begin{cases}
     X = A_r^{-1}f - A_r^{-1}B^T U \\
     U = U + N^{-1} \left( B A_r^{-1} f - (C - B A_r^{-1}B^T)U \right) = U + N^{-1} \left( - B X - C U\right),
    \end{cases}
\end{equation}
Denoting $E^X_{k} = X_{k} - X$, $E^U_{k} = U_k - U$, we write out the error equation as follows.
\begin{eqnarray}
E^X_{k+1} &=& - A_r^{-1}B^T (U_k - U) \\
E^U_{k+1} &=& E^U_{k} + N^{-1} \left( -(C - B A_r^{-1}B^T) E^{U}_k \right) = (I - N^{-1}S) E^U_{k}
\end{eqnarray}
With the above error equations, it is not hard to see that the convergence depend on the spectral radius of the error propagation matrix $I - N^{-1}S$. 

\begin{lemma}
The following is our goal: 
$\|I - N^{-1}S\| = \delta < 1$
for some norm. 
\end{lemma}
\begin{proof}
The error transfer operator is given as follows: 
\begin{eqnarray*}
I - N^{-1}S &=& 
\left ( \begin{matrix}
I & 0 \\ 
0 & I 
\end{matrix} \right ) - \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
    r K^T \left(I - r A_{r}^{-1} \right)K  & K^T (I -  r A_{r}^{-1} ) \\
    (I -  r A_{r}^{-1} ) K  & - A_{r}^{-1}
    \end{pmatrix}   \\
&=& \begin{pmatrix}
I - (K^TK)^{-1} K^T (I - r A_r^{-1}) K&  -\frac{1}{r} (K^TK)^{-1} K^T (I - r A_r^{-1}) \\
r(I - K (K^TK)^{-1} K^T) (I - r A_r^{-1}) K  & (I - K (K^TK)^{-1} K^T) (I - r A_r^{-1})
\end{pmatrix},
\end{eqnarray*}
%where $E := K^T K = n I$, $Q_A = I - r A_r^{-1}$. We note that 
%
%By $2\times 2$ block matrix decomposition, we have the error propagation matrix is spectrally equivalent to the following matrix
%\begin{equation}\label{mat1}
%    \begin{pmatrix}
%   S1 & 0 \\
% 0   &
% S_2 + S_3   \end{pmatrix},
%\end{equation}
%where $S_1 = I -   E^{-1} K^T Q_A K$, $S_2 = (I - M_2)Q_A $, $S_3 = (I- M_2) Q_A K (I - E^{-1} K^T Q_A K )^{-1} E^{-1} K^T Q_A $.

%It is easy to see that $S_1$ is symmetric positive and has $\rho(S_1) < 1$.
%We also have $\rho(S_2) \leq \frac{C_0}{r + C_0}$, $\rho(S_3) \leq \frac{C_0}{ r+C_0}\frac{r +C_0}{r} \frac{C_0}{r +C_0 }$. So $\rho(S_2 + S_3) \leq \frac{C_0}{r} \le 1 $ for sufficiently large $r$. 

%We can then take $\eta = \min \{ \rho(S_1), \frac{C_0}{r} \}$. This completes the proof.
\end{proof}

\begin{theorem}
For quadratic objective 
\end{theorem}

Now we extend our result to a nonlinear $\nabla F(X)$. 

\subsubsection{Inexact Uzawa for linear problems}


The inexact Uzawa iteration is given as follows. 

\begin{equation}
    \begin{cases}
     X_{k+1} = X_k + N_1^{-1}(-B^T U_k - A_r X_k), \quad A_r X_{k+1} +B^T U_k = f \\
     U_{k+1} = U_k + N^{-1} \left( - B X_{k+1} - C U_k\right),
    \end{cases}
\end{equation}
where $N_1^{-1}$ is the solver used for the $X$ part, e.g. one step of gradient descent or several steps. $N^{-1}$ is the same linear solver as before. 

We assume the solver $N_1^{-1}$ satisfies  $N_1^{-1}(0) = 0$. This is true for both linear solver and some nonlinear solvers, including several steps of gradient descent.


Under this assumption, the exact solution satisfies the following equations. 

\begin{equation}
    \begin{cases}
     X = X + N_1^{-1}(-B^T U - A_r X)\\
     U = U + N^{-1} \left( - B X - C U\right),
    \end{cases}
\end{equation}
Taking the difference, one can obtain the following error equations. 

\begin{eqnarray}
 E^X_{k+1} & = & E^X_{k} + N_1^{-1} (-B^T U_k - A_r X_k) - N_1^{-1} (-B^T U - A_r X) \\
E_{k+1}^U &=& E_{k}^U + N^{-1} (- B E^X_{k+1} - C E^U_k).
\end{eqnarray}

If $N_1^{-1}$ is a linear solver, we have the follow error equation due to linearity. 
\begin{eqnarray}
 E^X_{k+1} & = & E^X_{k} + N_1^{-1} (-B^T E^U_k - A_r E^X_k) \\
 E_{k+1}^U &=& E_{k}^U + N^{-1} (- B E^X_{k+1} - C E^U_k).
\end{eqnarray}
Writing it in the matrix form, we have 
\begin{equation}
    \begin{pmatrix}
     I & 0\\
     N^{-1} B & I
    \end{pmatrix} 
    \begin{pmatrix}
    E^X_{k+1} \\
    E^U_{k+1}
    \end{pmatrix} 
  = \begin{pmatrix}
    I - N_1^{-1} A_r & - N_1^{-1} B^T \\
    0 & I - N^{-1} C
    \end{pmatrix}
    \begin{pmatrix}
    E^X_{k} \\
    E^U_{k}
    \end{pmatrix}  . 
\end{equation}
Rearranging, we have 
\begin{equation}\begin{pmatrix}
    E^X_{k+1} \\
    E^U_{k+1}
    \end{pmatrix} = 
        \begin{pmatrix}
     I & 0\\
     - N^{-1} B & I
    \end{pmatrix} 
    \begin{pmatrix}
    I - N_1^{-1} A_r & - N_1^{-1} B^T \\
    0 & I - N^{-1} C
    \end{pmatrix}
    \begin{pmatrix}
    E^X_{k} \\
    E^U_{k}
    \end{pmatrix}  
\end{equation}
Now, we analyse the spectral radius of the error propagation matrix. 

We have for an iterative solver like Richardson's iteration (sufficiently small step size), 
$\rho(I - N_1^{-1} A_r) \leq \| I -N_1^{-1} A_r \| \leq 1 $. 

Furthermore, we compute 
\begin{equation}
\begin{aligned}
      I - N^{-1} C & = I - \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
    r K^TK & K^T \\
    K & 0
    \end{pmatrix} \\
    & = I - \begin{pmatrix}
    I &   (r K^T K)^{-1} K^T\\
     0 &  rK(rK^TK)^{-1}K^T 
    \end{pmatrix}\\
    & = \begin{pmatrix}
    0 & - (r K^T K)^{-1} K^T\\
    0 & I - K(K^TK)^{-1}K^T
    \end{pmatrix}
\end{aligned}
\end{equation}



\textcolor{red}{If $N^{-1}$ is a nonlinear solver, e.g. several steps of gradient descent or iterative solvers. 
}
 
\section{Appendix} 



 
\begin{algorithm}\label{alginexact4} \caption{Uzawa for $L_r$ with a single step Gradient Descent}
ADMM updates are as follows. 
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
Apply one step GD to find $X_{k+1}$ for $L_r$: 
\begin{equation}\label{gdADMM}
\nabla F(X_{k}) - H_k - r (Kz_k - X_{k+1}) = 0 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
\end{equation} }
\State{Update the Lagrange multiplier: \begin{equation} 
H_{k+1} - H_k - r (K z_{k+1} - X_{k+1}) = 0. 
\end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Assume that $K^TH_0 = 0$. Then Algorithm produces 
$Y_k = [\overline{z_k}; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [\overline{z_*}; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|H_{k+1} - H_* \|^2 \leq \frac{1}{1+\delta} \| H_{k} - H_*\|^2,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|H_{k} - H_* \|^2_C
\end{equation}
\end{theorem}

\begin{proof}
We have the following identity holds: 
\begin{eqnarray*}
\nabla F(X_{k}) - \nabla F(X_*) &=& r( K z_{k} - K {z_{k+1}}) + H_{k+1} - H_* \\
r M_1 (X_{k+1} - X_*) &=& - H_{k+1} + H_k \\
M_2 (X_{k+1} - X_*) &=& K(z_{k+1} - z_*)
\end{eqnarray*}
Let 
\begin{equation}
X = X_{k+1} - \frac{1}{r}H_k \quad \mbox{ and } \quad Y = X_* - \frac{1}{r}H_*. 
\end{equation}
We then see that 
\begin{eqnarray*}
\|Kz_{k+1} - X_* \|^2 &=& \|M_2(X) - M_2(Y)\|^2% \langle X_{k+1} - X_*, X_{k+1} - X_* \rangle \\ 
%&=& \left \langle Kz_k + \frac{1}{r} H_k - \frac{1}{r} \nabla F(X_k) - X_*, Kz_k + \frac{1}{r} H_k - \frac{1}{r} \nabla F(X_k)\right \rangle \\
%&=& \left \langle (X_k - X_*) - \frac{1}{r} (\nabla F(X_k) - \nabla F(X_*)), (X_k - X_*) - \frac{1}{r} (\nabla F(X_k) - \nabla F(X_*))\right \rangle \\
%&\leq& \left ( 1 - \frac{\lambda}{r} \right )  \|X_k - X_*\|^2. 
\end{eqnarray*}
On the other hand, we also observe that
\begin{eqnarray*}
&& \|H_{k+1} - H_* \|^2 = \langle H_{k} + r (Kz_{k+1} - X_{k+1}) - H_*, H_{k} + r (Kz_{k+1} - X_{k+1}) - H_* \rangle \\ 
&& = \left \langle r \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - H_*, r \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - H_* \right \rangle \\
&& = r^2 \left \langle \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \frac{1}{r} H_*, \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \frac{1}{r} H_* \right \rangle \\
&&= r^2 \left \langle \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \left ( X_* - \left (X_* - \frac{1}{r} H_* \right ) \right ), \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \left ( X_* - \left (X_* - \frac{1}{r} H_* \right ) \right ) \right \rangle \\
&&= r^2 \left \langle \left( M_2(X) - X \right ) - \left ( M_2(Y) - Y \right ),  \left( M_2(X) - X \right ) - \left ( M_2(Y) - Y \right ) \right \rangle \\
&&= r^2 \|(M_2(X) - X) - (M_2(Y) - Y)\|^2.  
%X_{k+1} - \frac{1}{r}H_k \right ) - \left (  X_* - \frac{1}{r} H_* \right ) \right \|^2 = r^2 \left \| \left ( X_{k+1} - X_* \right ) - \frac{1}{r} \left ( H_k - H_* \right ) \right \|^2 \\
%&&= r^2 \|X_{k+1} - X_*\|^2 - 2r^2 \left \langle X_{k+1} - X_*, \frac{1}{r} \left ( H_k - H_* \right ) \right \rangle + \|H_k - H_*\|^2.
%&&= r^2 \|X_{k+1} - X_*\|^2 - 2 \left \langle W_{k+1} - W_* - , r \left ( H_k - H_* \right ) \right \rangle + \|H_k - H_*\|^2. 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
\|X_{k+1} - X_*\|^2 + \frac{1}{r^2} \|H_{k+1} - H_*\|^2 &\leq& \|X_{k+1} - \frac{1}{r}H_k - (X_* - \frac{1}{r} H_*))\|^2 \\
&=& \| (X_{k+1} - X_*) - \frac{1}{r}(H_k - H_*))\|^2 \\
&=& \| W_k - W_*\|^2. 
%- 2 \left \langle W_{k} - W_* + \frac{1}{r} \left ( H_k - H_* \right ), \frac{1}{r} \left ( H_k - H_* \right ) \right \rangle + \frac{1}{r^2} \|H_k - H_*\|^2 \\ 
%&=& \|W_{k} - W_*\|^2, 
\end{eqnarray*} 
where $W_k = X_k - \frac{1}{r} \nabla F(X_k)$. 
\end{proof}
\begin{remark}
The use of nonexpansiveness is to obtain $\nabla F(\cdot)$ from two different $\nabla F - H_k$ and $\nabla F - H_*$. 
\end{remark}

\section{$2\times2$ block system}
In this section, we focus on the convergence analysis for the $2\times2$ block system involving the solution of $X, z$ with $H$ being fixed. This is a subproblem from the previous $3\times3$ system.  

We consider convergence analysis of algorithms for the following system.

\begin{equation} \label{2by2}
\begin{aligned}
\nabla F(X_{*}) - H - r (Kz_{*} - X_{*}) &= 0, \\
K^T H + r K^T (Kz_{*} - X_{*}) &= 0.
\end{aligned}
\end{equation}
In operator notation, we have 
\begin{equation}
\begin{pmatrix}\label{2by2abstract}
A_r & B^T\\
B & C
\end{pmatrix} 
\begin{pmatrix}
X_*\\
z_*
\end{pmatrix}= 
\begin{pmatrix}
f  \\
g 
\end{pmatrix},
\end{equation}
where $A_r = \nabla F(\cdot) + r I $, $B = -r K^T$, $C = rK^T K $ $f = H, g = -K^T H$. 


\begin{algorithm}
\caption{Block Gauss Seidel for $2 \times 2$ system \eqref{2by2}}
\label{algexact5}
\begin{algorithmic}
\State{Given initial $X_0, z_0$.}
\For{$k=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
\begin{equation}
\nabla F(X_{k+1}) - H - r (Kz_k - X_{k+1}) = 0 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
K^T H + r K^T (Kz_{k+1} - X_{k+1}) = 0,
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
For general $\lambda-$strongly convex and $L-$smooth objective function $F(X)$, we have for Algorithm \ref{algexact5}, 
\begin{equation}
    \|X_{k+1} - X_{*} \| \leq \frac{r}{r + \lambda} \| \bar{z}_k - \bar{z}_* \|.
\end{equation}
\end{theorem}

\begin{proof}
From the updates in Algorithm \ref{algexact5} and the optimality condition, we obtain the error equation as follows. 
\begin{eqnarray}
 \nabla F(x_{k+1}) - \nabla F(x_*) & = & r (\bar{z}_k - \bar{z}_*) - r(X_{k+1} - X_*)
%  a new line & = & continues
\end{eqnarray}
By $\lambda-$strong convexity, we have 
\begin{equation*}
\begin{aligned}
    \lambda \|X_{k+1} - X_* \|^2 &\leq \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*) \rangle \\
    & = \langle X_{k+1} - X_*, r(\bar{z}_k - \bar{z}_* )\rangle - \langle X_{k+1} - X_*, r (X_{k+1} - X_*) \rangle 
\end{aligned} 
\end{equation*}
Rearranging and applying Cauchy-Schwarz inequality, we can obtain the required inequality. 
\end{proof}

% \begin{lemma}
% For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the block Gauss Seidel iteration in Algorithm \ref{algexact5} corresponds to the exact Uzawa method with the following ierative method for the Schur complement: 
% \begin{equation}
%     z_{k+1} = z_{k} + N^{-1} (g - B X_{k+1} - Cz_{k}),
% \end{equation}
% where $N = C$. 
% \end{lemma}

\begin{theorem}
For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the block Gauss Seidel iteration in Algorithm \ref{algexact5} corresponds to the exact Uzawa method with the following ierative method for the Schur complement: 
\begin{equation}
    z_{k+1} = z_{k} + N^{-1} (g - B X_{k+1} - Cz_{k}),
\end{equation}
where $N = C$. 

Furthremore, we have the following convergence result
\begin{eqnarray}
     \| z_{k+1} - z_*\| &\leq& \left( \frac{r}{r +c_0}\right)^{k+1 } \| z_0 -z_*\|, \\
    \|X_{k+1} -X_* \| &\leq& \frac{r}{r + \lambda} \left( \frac{r}{r + c_0}\right)^k \|z_0 - z_* \|,
\end{eqnarray}
where $c_0$ is the smallest eigenvalue of $A$. 
\end{theorem}
\begin{proof}
Note that $C = rK^TK$ is invertible. A direct calculation can show that it is indeed same as the update in $z_{k+1}$ in Algorithm \ref{algexact5}. 

For simplicity, we absorb $b$ into the right hand side $f$. 

Konwing that 
\begin{equation}
    X_{k+1} = A_r^{-1} f - A_r^{-1} B^T z_{k},  
\end{equation}
we eliminate $X_{k+1}$ from the construction of $z_{k+1}$ to have the iteration 
\begin{equation}
    z_{k+1} = z_{k} + C^{-1} \left (g - B A_r^{-1}f - (C - B {A_r}^{-1} B^T) z_k \right),
\end{equation}
which is the iteration applied to the Schur complement system: 
\begin{equation}
    (C - B {A_r}^{-1} B^T) z = g - B A^{-1} f.
\end{equation}
The convergence in error $\| z_{k+1}-z_* \|$ depend on  $\rho( I - C^{-1} (C - B {A_r}^{-1} B^T))$.
\begin{equation*}
\begin{split}
\rho( I - C^{-1} (C - B {A_r}^{-1} B^T)) &=  \rho ( C^{-1} B {A_r}^{-1} B^T) \\
&\leq \rho( (rK^T K)^{-1} rK^T (A + rI)^{-1} rK )   \\ 
& \leq  \frac{r}{r + c_0}.
\end{split}
\end{equation*}
Therefore, we have 
\begin{equation}
    \| z_{k+1} - z_*\| \leq \left( \frac{r}{r +c_0}\right)^{k+1 } \| z_0 -z_*\|. 
\end{equation}

\end{proof}

\begin{algorithm}
\caption{Inexact Uzawa for $2 \times 2$ system \eqref{2by2abstract} }
\label{algexact6}
\begin{algorithmic}
\State{Given initial $X_0, z_0$.}
\For{$k=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
\begin{equation}
X_{k+1} = X_k + \omega I (f - A_r X_k - B^T z_k), 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
z_{k+1} = C^{-1} (g - B X_{k+1} - Cz_k)
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

For Algorthm \ref{algexact6}, we define $E^X_{k} = X_k - X_*$, $e^z_{k} = z_k - z_*$ the error equations are given by 
\begin{eqnarray}
E^X_{k+1} &=& E^X_{k} + wI\left( -A_rE^X_{k} - B^Te^z_k \right) \label{erroreqn1}\\
e^z_{k+1} &=& -C^{-1}B E^X_{k+1} \label{erroreqn2}
\end{eqnarray}
    
\begin{theorem}
For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the Algorithm \ref{algexact5} has the following convergence results for some $\eta<1$ if $\omega$ is sufficiently small

\begin{equation}
    \|X_{k+1} - X_*\| \leq \eta^{k+1} \|X_{0} - X_* \| 
\end{equation}

\begin{equation}
    \|Kz_{k+1} - K z_*\| \leq \eta^{k+1} \|X_{0} - X_* \| 
\end{equation}
\end{theorem}

\begin{proof}
Substituting \eqref{erroreqn2} into \eqref{erroreqn1}, we have 
\begin{equation}
    E^X_{k+1} = E^X_{k} + wI\left( -A_rE^X_{k} + B^T C^{-1} B E^X_{k} \right)
\end{equation}
Then convergence of $\|E^X_{k+1} \|$ depends on $\rho(I  - w ( A_r - B^T C^{-1}B) )$. 

Note that $\lambda_{\text{min}}(A_r) = c_0 + r$, and $B^T C^{-1} B = r K (K^TK)^{-1} K^T$, which is symmetric and has spectral radius $r$. We have $A_r - B^T C^{-1} B$ is symmetric positive definite. Therefore, by choosing sufficiently small step size $\omega$ for the Richardson's iteration, we have for some $\eta < 1$
\begin{equation}
    \|E^X_{k+1}\| \leq \eta^{k+1} \|E^X_{0} \| 
\end{equation}

Furthermore, multiplying \eqref{erroreqn2} by $K$ and considering the norm, we have 
\begin{equation}
    \begin{aligned}
       \|Ke^z_{k+1} \| &= \|KC^{-1}B E^X_{k+1}\| \\
        & = \| K(K^T K)^{-1} K^T  E^X_{k+1}\| \\
        & \leq \|E^X_{k+1}\|  \leq \eta^{k+1} \|E^X_{0} \| 
    \end{aligned}
\end{equation}
\end{proof}




\begin{algorithm}\label{alg:inexactADMM2}
\caption{ADMM for $L_r$ with GD with for the first step}
ADMM updates are as follows. 
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_{t+1}$ update: 
    solve $\nabla F(X_{t+1}) - H_t - r (Kz_t - X_{t+1}) = 0$ using gradient descent as follows.
    \For{$ k = 0,..., K-1$}
      \begin{equation}\label{gd for ADMM}
        \begin{split}
            X_{t+\frac{k+1}{K}} & = X_{t+\frac{k}{K}} - \lambda \left(\nabla F(X_{t+\frac{k}{K}})  - H_k - r(K z_k - X_{t + \frac{k}{K}}) \right) \\
            & = (1 - \lambda r)X_{t+\frac{k}{K}} + \lambda r K z_k - \lambda \left(\nabla F(X_{t+\frac{k}{K}})  - H_k \right)
        \end{split}
    \end{equation}
    \EndFor
    }
    \State{$z_{t+1}$ update: 
        \begin{equation} 
        K^T H_k + r K^T (Kz_{t+1} - X_{t+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} 
        H_{t+1} - H_t - r (K z_{t+1} - X_{t+1}) = 0. 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}



\textcolor{red}{How to analyze the convergence rate in the following two cases?}

\textcolor{red}{If we consider $X$ part, and consider $z$, $H$ part together, then this is an exact Uzawa, with an iterative method for the Schur complement. See Propostition \ref{prop: Uzawa iterative solver}. }

\textcolor{red}{If we consider the $X$, $z$ together, then it is an inexact Uzawa with the first block solved by one step of block GS and the second part solved by Richardson iteration.}


\newpage 

\begin{section}{Uzawa iterations}
This section contains mathematical theory on Uzawa iterations that are related to our problem. 
\end{section}

\section{Introduction}

Data becomes increasingly decentralized and the privacy of individual data is an utmost importance in the digital age \cite{house2012consumer, cai2021deepstroke,chen2020ai,luo2020arbee,wang2020panel}. Unlike standard machine learning approaches, \textit{Federated learning} (FL) encourages each client to have a local training data set, which will not be stored to the server and  to update the local correction of the current global model maintained by the main server via the local data and local gradient descent method. {FL} has been used successfully in many different areas, which include Internet of Things (IoT) applications \cite{hwang2015iot, ferrag2021federated}. {FL} can be modeled as the optimization problem given as 
\begin{equation}\label{FL}
\min_{x \in X} \left \{ E(x) = \sum_{k=1}^N f_k (x) \right \},
\end{equation} 
where $X$ is a parameter space, $N$ is the number of clients or devices, and $f_k \colon X \rightarrow \mathbb{R}$, $1 \leq k \leq N$, is a local objective function for the $k^{\rm th}$ worker. The local objective function $f_k$ depends on the data of the $k^{\rm th}$ worker, but not on those of the other clients. The standard Federated Avgerage ({\textit{FebAvg}}) algorithm consists of three steps
\begin{enumerate}
\item  the central server broadcasts the latest model $x_t$, to all the clients;
\item every worker, say $k^{\rm th}$ worker, lets $x_t^k = x_t$ and then performs one or few local updates with learning rate $\gamma$ 
  $$x_{t+1}^k \leftarrow x_t^k - \gamma \nabla f_k(x_t^k)$$
\item the server then aggregates the local models, $x_{t+1}^1, \cdots x_{t+1}^N$, to produce the new global model $x_{t+1}$ \cite{konevcny2016federated}.
  \end{enumerate}

Figure \ref{fig:scheme} illustrates a simple single local gradient descent algorithm (Local GD) and the arrows indicate the communications, which poses a bottleneck of the algorithm because it is generally orders of magnitude more expensive than the local computations and more communications make the algorithm more vulnerable for cybersecurity. Recent methods, therefore, aim at enhancing the privacy of FL by using reduced model or even sacrificing system efficiency. However, {\textbf{providing privacy has to be carefully balanced with system efficiency}} \cite{li2020federated}. Figure \ref{fig:scheme2} shows that the Local GD without applying the shifted gradient can reach the lower accuracy faster, but it does not converge eventually. One recent algorithm, called Scaffnew or ProxSkip, is shown to achieve the best communication efficiency, without sacrificing the convergence property, until today \cite{mishchenko2022proxskip}. Scaffnew reformulates ~\cref{FL} as follows:
\begin{equation}\label{cp}
\min_{x_1, \cdots, x_N \in X} \left \{ \frac{1}{N} \sum_{i=1}^N f_i(x) + \psi(x_1,\cdots,x_N) \right \},
\end{equation} 
where $\psi$ is a proper closed convex function introduced for a consensus reformulation, which can be interpreted as some average of parameters $x_1,\cdots,x_N$, from each client. Then it applies the proximal gradient descent method. The novelty of Scaffnew is at the introduction of certain shift, denoted by $h_t$, leading to the modified grandient, i.e., $\widetilde{\nabla}f(x_t) = \nabla f(x_t) - h_t$. This can be considered as a type of the preconditioner, which leads to the solution for the consensus reformulation even with applying the proxy operator once in a while. 

%The focus of our tasks is to develop mathematical foundation of FL and to design effective training algorithms for FL using ideas from numerical analysis and mathematical optimization so as to push the boundaries of the current state of the art in FL \cite{li2019convergence,zhou2022convergence,haddadpour2019convergence,mitra2021linear}. 

The focus of this proposal is to push the boundary of theoretical convergence analysis for the currently available for {\textit{FebAvg}}. The convergence theory of Scaffnew was established for strongly convex problems \cite{mishchenko2022proxskip}. We also present improved FL models that can provide better privacy without sacrificing convergence property of scheme. These two goals will be achieved by viewing \textit{FebAvg} within the framework of subspace correction methods.  

To expedite the success of the proposed studies, we will team up with people of expertises in numerical analysis, mathematical optimization, and machine learning. The project leader Jinchao Xu will closely collaborate with Young Ju Lee (Texas State) and other close collaborators such as Qingguo Hong (Penn State), Xiaofeng Xu (Penn State) and Jongho Park (KAIST).  J. Xu's research interests include mathematical foundation of machine learning,  approximation theory for deep neural networks, design of convolutional neural networks from multigrid viewpoint, and development of training algorithms based on subspace correction --- a general framework containing a large class of optimization algorithms such as coordinate descent method and federated learning. Lee has worked on successive subspace corrections for nontrivial problem, such as nearly or singular problem, which fits in the current project since objective functionals are typically nearly singular \cite{chen2020robust,lee2009robust,LWXZ:2007}. Hong has an expertise in analysis of preconditioners for coupled nonlinear systems \cite{hong2016uniformly,hong2016robust,chen2020robust}. 
Park has several interesting results on parallel subspace correction methods for mathematical optimization problems~\cite{Park:2020,Park:2021,Park:2022}.



We consider to solve 
\begin{equation} 
\min_{x \in \Reals{d}} f(x) + \psi(x), 
\end{equation}
where $f : \Reals{d} \rightarrow \Reals{}$ is a smooth function and $\psi : \Reals{d} \rightarrow \Reals{} + \{+\infty\}$ is a proper, closed and convex regularizer. 

We consider the constrained optimization. Namely, for some $\mathcal{C} \subset \Reals{d}$, we consider to minimize  
\begin{equation}\label{main:eq}  
\min_{x \in \mathcal{C}} f(x) \quad \mbox{ or equivalently, } \quad \min_{x \in \Reals{d}} f(x) \quad \mbox{ subject to } x \in \mathcal{C}.  
\end{equation}
By defining $\psi : \Reals{d} \mapsto \Reals{}$ by 
\begin{equation}
\psi(x) :=  \left \{ \begin{array}{cc} 
0, & \mbox{ if } x \in \mathcal{C} \\
+\infty, & \mbox{ otherwise } 
\end{array} \right . 
\end{equation}
the problem \eqref{main:eq} can be formulated into 
\begin{equation} 
\min_{x \in \Reals{d}} f(x) + \psi(x).  
\end{equation}

\newpage 


\section{Federated learning and ADMM}
Notation:

$x \in \mathbb{R}^d$, $X = (x_1, x_2 ,\dots,x_n) \in \mathbb{R}^{d\times n}$, where $x_i \in \mathbb{R}^d$.

In federated learning, the objective function is $f(x) = \frac{1}{n} \sum_{i = 1}^n f_i(x)$,where each $f_i$ is the local objective function for each client $i$. 

We have the following formulations of the minimization problem in federated learning. 

\begin{itemize} 
    \item Original formulation \begin{equation}\label{Fed: original}
        \min_{x \in \mathbb{R}^d} f(x) = \frac{1}{n} \sum_{i = 1}^n f_i(x)
    \end{equation} 
    \item 
    \begin{equation}
        \min_{\substack{X \in \mathbb{R}^{dn} \\x1 = x2 = \cdots = x_n }} F(X) = \frac{1}{n} \sum_{i = 1}^n f_i(x_i)
    \end{equation}
    \item Consensus formulation
        \begin{equation}  \label{Fed: concensus}
        \min_{X \in \mathbb{R}^{dn}} F(X) + \psi(X),
    \end{equation}
    where 
    \begin{equation}\label{psi}
    \psi(x) =  \left \{ \begin{array}{cc} 
    0, & \mbox{ if }  x_1 = x_2 = \cdots = x_n \\
    +\infty, & \mbox{ otherwise } 
    \end{array} \right . 
    \end{equation}
\item 
    \begin{equation}
    \min_{ \substack{X,Z \in \mathbb{R^{dn}} \\ X = Z}} F(X) + \psi(Z)
    \end{equation}
\item Lagrange multiplier method
    \begin{equation} \label{Fed: Lagrange}
        \min_{X,Z \in \mathbb{R}^{dn}} \max_{h \in \mathbb{R}^{dn} } F(X) + \psi(Z) + \langle H, Z- X \rangle
    \end{equation}
\item Introduce the $K = [I,...,I]^T$, where $I \in \mathbb{R}^{d\times d}$ is the identity matrix.
\begin{equation}
    \min_{ \substack{ X \in \mathbb{R}^{dn}, z \in \mathbb{R}^d \\X = Kz }} F(X)
\end{equation}
\item Using tensor notation, $\mathds{1} = [1,1,\dots, 1]^T $
\begin{equation}
    \min_{ \substack{X \in \mathbb{R}^{dn}, z \in \mathbb{R}^d \\ X = \mathds{1} \otimes z  } } F(X)
\end{equation}
\item Lagrange multiplier method 
\begin{equation}
    \min_{X \in \mathbb{R}^{dn},z \in \mathbb{R}^d} \max_{H \in \mathbb{R}^{dn}} F(X) + \langle H,Kz - X \rangle 
\end{equation}
\end{itemize}
\newpage

\begin{algorithm}
\caption{Federated Learning for $f(x)$}\label{alg:FedLearing}
Given a stepsize $\gamma > 0$, initial iterate $x_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following ($x_i,t$ is the local copy of the parameter for client i):  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{For each client i: $x^0_{i,t} = x_t$}
    \For{$k = 0, 1,2, \cdots, K-1$}
    \State{$x_{i,t +\frac{k+1}{K}} = x_{i,t + \frac{k}{K}} - \gamma \nabla f_i(x_{i,t+\frac{k}{K}})$}
    \EndFor
    \State{$x_{t+1} =\frac{1}{n} \sum_{i = 1}^n x_{i,t+1} $} 
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{remark}
For K = 1, this simply reduces to the usual GD for f(x).
\end{remark}

\begin{lemma}\label{lemma: prox psi}
Given $X = (x_1, x_2, \dots, x_n)$, we have 
\begin{equation}
    {\rm prox}_{a\psi}(X) = (\bar{x},\dots, \bar{x} ),
\end{equation}
where $\bar{x} =\frac{1}{n} \sum_{i = 1}^n x_i$, $\psi$ is given in \eqref{psi}, $a$ is a nonzero constant. 
\end{lemma}
\begin{proof}
By definition, we have 
\begin{equation*}
\begin{split}
        \text{prox}_{a\psi} (X) &= \mathop{\arg \min}_{Y} \psi(Y) + \frac{1}{2a} \| X - Y \|^2 \\
         & = \mathop{\arg \min}_{Y : y_1 = \cdots =y_n = \alpha } \sum_{i = 1}^n \| x_i - \alpha\|^2 = (\bar{x},\dots , \bar{x}),
\end{split}
\end{equation*}
where $\bar{x} =\frac{1}{n}\sum_{i=1}^n x_i$. 
\end{proof}

Thanks to Lemma \ref{lemma: prox psi}, we have the following consensus form of federated learning.

\begin{algorithm}
\caption{Consensus Federated Learning for $F(X) + \psi(X)$}\label{alg:FedLearing consensus}
Given a stepsize $\gamma > 0$, initial iterate $X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    % \State{$X^0_t = X_t$}
    \For{$k = 0, 1,2, \cdots, K-1$}
    \State{$X_{t +\frac{k+1}{K}} = X_{t + \frac{k}{K}} - \gamma \nabla F(X_{t + \frac{k}{K}})$}
    \EndFor
    \State{$X_{t+1} = \text{prox}_{\gamma \psi} (X_{t+1})$} 
\EndFor
\end{algorithmic}
\end{algorithm}

Using our notation, we now introduce the ProxSkip algorithm. See algorithm~\ref{alg:ProxSkip}. 

\begin{algorithm}
\caption{ProxSkip}\label{alg:ProxSkip}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, $h_0$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_{t+1} = Z_{t} - \gamma (\nabla F(Z_{t}) -h_t ) $}
    \State{Flip a coin $\theta_t$, $P(\theta_t = 1) = p $}
    \If{$\theta_t = 1$} 
    \State{$Z_{t+1} = {\rm prox}_{\frac{\gamma}{p}\psi } 
    \left ( X_{t+1} - \frac{\gamma}{p} h_t \right )$} 
    \Else
        \State{$Z_{t+1} = X_{t+1}$}
    \EndIf 
    \State{$h_{t+1} = h_t + \frac{p}{\gamma} (Z_{t+1} - X_{t+1})$} 
\EndFor
\end{algorithmic}
\end{algorithm}

A deterministic version of ProxSkip is given in algorithm \ref{alg:ProxSkip deterministic}. 

\begin{algorithm}
\caption{ProxSkip Deterministic (SCAFFOLD)}\label{alg:ProxSkip deterministic}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \For{$k = 0, 1,\dots, K-1$}
    \State{$X_{t + \frac{k+1}{K}} = Z_{t + \frac{k}{K}} - \gamma ( \nabla F(Z_{t + \frac{k}{K}} ) - h_t)$ }
    \State{$Z_{t + \frac{k+1}{K}} =X_{t + \frac{k+1}{K}} $}
    \EndFor    
    \State{$Z_{t+1} = \text{prox}_{K\gamma \psi}(X_{t+1} - K\gamma h_t)$}
    \State{$H_{t+1} = H_t + \frac{\gamma}{K} (Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}

Now we look at the Lagrange multiplier formulation~\eqref{Fed: Lagrange}. The Lagrangian is given by 
\begin{equation}\Label{lagrangian}
    L(X,Z,h) = F(X) +\psi(Z) + \langle H,Z-X \rangle
\end{equation}
The augmented Lagrangian is given by 
\begin{equation}\Label{Augmented lagrangian}
    L_r(X,Z,h) = F(X) +\psi(Z) + \langle H,Z-X \rangle + \frac{r}{2} \|Z-X \|^2
\end{equation}

The famous ADMM method performs the following three steps
\begin{itemize} 
    \item ADMM step 1
    \begin{equation}
        X_{t+1} = \mathop{ \arg \min }_{X} L_r({X,Z_{t}, H_t})
    \end{equation} 
    
    \begin{equation}
    L_r(X,Z_t,H_t) = F(X) +\psi(Z_t) + \langle H_t,Z_t-X \rangle + \frac{r}{2} \|Z_t-X \|^2
    \end{equation}
    
    \begin{equation}
        \nabla F(X_{t+1}) - H_t + r (X_{t+1} - Z_t)
    \end{equation}

    \item ADMM step 2
    \begin{equation}
        Z_{t+1} = \mathop{ \arg \min }_{Z} L_r({X_{t+1},Z, H_t})
    \end{equation}
    
    \begin{equation}
    L_r(X,Z,H) = F(X_{t+1}) +\psi(Z) + \langle H_t,Z-X_{t+1} \rangle + \frac{r}{2} \|Z-X_{t+1} \|^2
    \end{equation}
    
    \item ADMM step 3
    \begin{equation}
        H_{t+1} = H_t + r (Z_{t+1} - X_{t+1})
    \end{equation}
    
    \begin{equation}
    L_r(X,Z,H) = F(X) +\psi(Z) + \langle H,Z-X \rangle + \frac{r}{2} \|Z-X \|^2
\end{equation}
\end{itemize}

From this viewpoint, we can derive various algorithms for the Federated learning. 

\begin{algorithm}
\caption{Exact ADMM for $L_r$}\label{alg:ADMM exact}
Given a stepsize $\gamma > 0$, initial iterate $X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{Solve using exact solver $X_{t+1} = \mathop{ \arg \min }_{X} L_r({X,Z_{t}, H_t})$, namely $X_{t+1}$ satisfies\\
    \begin{equation}
        \nabla F (X_{t+1}) - H_t +r(X_{t+1} - Z_t) = 0
    \end{equation} }
    \State{Solve $Z_{t+1} = \mathop{ \arg \min }_{Z} L_r({X_{t+1},Z, H_t})$ using exact solver , namely\\
    \begin{equation}
        Z_{t+1} = \text{prox}_{\frac{\psi}{r}} (X_{t+1} - \frac{1}{r} h_t)
    \end{equation} }
    
    \State{Update the Lagrange multiplier: $H_{t+1} = H_t + r (Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{lemma}\label{lemma: equivalence of admm step 2 and prox}
The second minimization problem in the ADMM method 
\begin{equation}
    Z_{t+1} = \mathop{\arg \min}_Z L_r (X_{t+1})
\end{equation}
is equivalent to the following proximal map 
\begin{equation}
    Z_{t+1} = \text{prox}_{\frac{\psi}{r}}(X_{t+1} - \frac{1}{r} h_t) 
\end{equation}
\end{lemma}
\begin{proof}
\begin{equation}
    \begin{split}
         \mathop{\arg \min}_Z L_r (X_{t+1},Z,H_t)
         &=  \mathop{\arg \min}_Z \psi(Z) + \langle H_t, Z - X_{t+1}\rangle+ \frac{r}{2} \| Z- X_{t+1} \|^2 \\
         & =  \mathop{\arg \min}_Z  \psi(Z) + \frac{r}{2} \| Z - X_{t+1} + \frac{1}{r}H_t  \|^2 \\
         & = \text{prox}_{\frac{\psi}{r}} (X_{t+1} - \frac{1}{r} H_t)
    \end{split}
\end{equation}
\end{proof}

\begin{algorithm}
\caption{Inexact ADMM for $L_r$}\label{alg:ADMM inexact}
Given a stepsize $\gamma > 0$, initial iterate $X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{Solve $X_{t+1} = \mathop{ \arg \min }_{X} L_r({X,Z_{t}, h_t})$ approximated, e.g. using  K iterations of GD: }
    \For{$k = 0,1,\dots, K-1$}
    \State{$X_{t + \frac{k+1}{K}} = X_{t+\frac{k}{K}} - \gamma \bigg( \nabla F(X_{t+\frac{k}{K}}) -H_t - r(Z_{t} -X_{t+\frac{k}{K}}) \bigg)$}
    \State{\textcolor{blue}{$X_{t + \frac{k+1}{K}} = Z_t - \frac{1}{r} \bigg( \nabla F(X_{t+\frac{k}{K}}) -H_t \bigg)$ ~~(if we a stepsize $\gamma = \frac{1}{r}$)} }
    \EndFor
    \State{Solve $Z_{t+1} = \mathop{ \arg \min }_{Z} L_r({X_{t+1},Z, h_t})$ using exact solver, namely 
    \begin{equation}
        Z_{t+1} = \text{prox}_{\frac{\psi}{r}} (X_{t+1} - \frac{1}{r} h_t).
    \end{equation}
    }
    \State{Update the Lagrange multiplier: $h_{t+1} = h_t + r (Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}




\newpage
\subsection{Proximal gradient descent: ProxGD} 

ProxGD, known as forward-backward algorithm is the basis. For the forward step, we apply the gradient descent algorithm for the objective functional $f(x)$, i.e., 
\begin{equation}
\widehat{x}_{t+1} = x_t - \gamma_t \nabla f(x_t). 
\end{equation}
Here $\gamma_t > 0$ is a suitably chosen stepsize at time $t$, while $s_t = -\nabla f(x_t)$ is the search direction. 
On the other hand, due to the other functional $\psi(x)$, we need to adjust it. Namely, we need to apply certain projection. This generates the following:
\begin{equation}
x_{t+1} = {\rm prox}_{\textcolor{red}{\gamma_t}  \psi} (\widehat{x}_{t+1}) = {\rm prox}_{\textcolor{red}{\gamma_t} \psi} (x_t - \gamma_t \nabla f(x_t)). 
\end{equation}
 and 
\begin{equation}
{\rm prox}_{\gamma \psi(\cdot)} : \Reals{d} \rightarrow \Reals{d}    
\end{equation}
is the proximity operator of $\psi$, defined via 
\begin{equation}
{\rm prox}_{\gamma \psi}(x) = {\rm arg}\min_{y \in \Reals{d}} \left [ \frac{1}{2} \|y - x\|^2 + \gamma \psi(y) \right ]. 
\end{equation}
In summary, we have write the ProxGD algorithm as follows: 

\begin{algorithm}
\caption{ProxGD}\label{alg:maingd}
Given a stepsize $\gamma > 0$, initial iterate $x_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=1,2,\cdots,T-1$}

\State{$\widehat{x}_{t+1} = x_t - \gamma (\nabla f(x_t))$} 

%\State{Flip a coin $\theta_t \in \{0,1\}$ where ${\rm Prob}(\theta_t = 1) = p$}
%
%\If{$\theta_t = 1$} 
\State{$x_{t+1} = {\rm prox}_{\gamma \psi}  
\left ( \widehat{x}_{t+1} \right )$} %
%\Else
%    \State{$x_{t+1} = \widehat{x}_{t+1}$}
%    
%\EndIf 
%
%\State{$h_{t+1} = h_t + \frac{p}{\gamma}(x_{t+1} - \widehat{x}_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{remark}
Typically, the proximity operator is assumed to be evaluated in closed form. ProxGD is most suited to situations when the proximity operator is relatively cheap to evaluate. Therefore, the bottleneck is at the computation of $\nabla f$, than at the computation of the proximity operator. 
\end{remark}


\section{The update of the Lagrange multiplier for the federated learning algorithm} 

We denote the vector $X \in \mathbb{R}^{nd}$ as follows: 
\begin{equation}
X = \left ( \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_d \end{array} \right ) \in \mathbb{R}^{nd}, \quad \mbox{ where } x_j \in \mathbb{R}^{n}, \mbox{ and } \forall i=1,\cdots,d. 
\end{equation}
We now introduce a matrix $K \in \mathbb{R}^{d} \rightarrow \mathbb{R}^{nd \times d}$ defined by the following relation: 
\begin{equation} 
K = \left ( \begin{array}{c} I_d \\ I_d \\ \vdots \\ I_d \end{array} \right ), 
\end{equation}
where $I_d \in \mathbb{R}^{d\times d}$ is the identity matrix. Therefore, we see that for $z \in \mathbb{R}^{d}$, we have 
\begin{equation} 
K z = \left ( \begin{array}{c} z \\ z \\ \vdots \\ z \end{array} \right ) \in \mathbb{R}^{nd}. 
\end{equation} 
With $V = \mathbb{R}^{nd} \times \mathbb{R}^d$, we now consider to solve 
\begin{equation} 
\min_{X \in V } F(X) + H^T(Kz - X) + \frac{r}{2} \|Kz - X\|^2. 
\end{equation} 
This problem can be formulated as a saddle problem as follows: 
\begin{equation}\label{main:eq} 
\max_{H \in \mathbb{R}^{nd}}  \min_{X \in V} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}
\end{equation}
We define $g(H)$ as follows: 
\begin{equation}\label{gfunction}
g(H) = \min_{X} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}. 
\end{equation}
We shall now show how the Lagrange multiplier should be updated in the following Lemma. 
\begin{lemma}
Let $F$ be closed and convex and for a fixed $H_{k-1}$ and $z = z_k$, let 
\begin{equation} 
X_k = {\rm arg}\min_{X} ( F(X) + H_{k-1}^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2).  
\end{equation} 
Then, it holds that 
\begin{equation} 
\partial g(H_{k-1}) = Kz_k - X_k,  
\end{equation}
where $g$ is defined in \eqref{gfunction}. 
Furthermore, we have that
\begin{equation} 
\partial F(X_k) - (H_{k-1} + r (K z_k - X_k)) = 0. 
\end{equation} 
Therefore, the Lagrange multiplier has to be updated via the following formular: 
\begin{equation}
H_k = H_{k-1} + r (Kz_k - X_k). 
\end{equation}
\end{lemma}
\begin{proof} 
We define $G(X) = F(x) + \frac{r}{2} \|Kz - X\|^2$ and recall that the conjugate of $G$ denoted by $G^*$ is defined as follows: 
\begin{equation}
G^*(Y) := \max_{X} Y^T X - G(X). 
\end{equation}
Therefore, we see that 
\begin{eqnarray*} 
g(H) &=& -\max_X \left \{ -G(X) - H^T (Kz - X)  \right \} \\ 
&=&  -\max_X \left \{ -G(X) - (K^T H)^T z + H^TX \right \} \\
&=&  - \max_X \left \{ H^T X - G(X) \right \} + H^T K z \\
&=& - G^*(H) + H^TKz. 
\end{eqnarray*} 
The dual variable is then to satisfy the following maximum optimization: 
\begin{equation} 
\max_H g(H). 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
H_k = H_{k-1} + t_k \partial g(H_{k-1}), 
\end{equation} 
where $t_k \geq 0$. We note that $H_{k-1}$ produces $X_k$ as the minimizer given as follows: 
\begin{equation} 
X_k = {\rm arg}\min_Y \left ( G(Y) + H_{k-1}^T (K z_k - X) \right ).  
\end{equation} 
We now recall the well-known fact that if $G$ is closed and convex, then 
\begin{equation}
Y \in \partial G(X) \quad \Leftrightarrow \quad X \in \partial G^*(Y) \quad \Leftrightarrow \quad X \in {\rm arg}\min_Z G(Z) - Y^T Z.
\end{equation}
Using this fact, we shall show that $\partial g(H_{k-1}) = Kz_k - X_k$. First, we note that 
\begin{equation}
\partial g(H) = \partial G^*(H) + Kz. 
\end{equation}
Therefore,  if $X \in \partial G^*(H)$, then $X = {\rm arg}\min_Z G(Z) - H^TZ$. Namely, we have that 
\begin{eqnarray*}
H_{k-1}^T Kz_k + {\rm arg}\min_Z \left \{ G(Z) + H_{k-1}^T Z \right \} &=& G(X_k) + H_{k-1}^T (Kz_k - X_k) \\
&=& F(X_k) + (H_{k-1}^T (Kz_k - X_k) + \frac{r}{2}\|Kz_k - X_k\|^2. 
\end{eqnarray*}
Thus, we have that
\begin{equation}
\partial g(H_{k-1}) = Kz_k - X_k. 
\end{equation}
This completes the proof. 
\end{proof} 

\newpage 
\subsection{ProxSkip} 
Generally, the proximity operator is difficult to apply and thus, we want to skip this procedure and apply it only every once in a while. Therefore, the convergence of the ProxSkip requires some additional trick, which is basically the modification of the $\nabla f$ term. 

\begin{algorithm}
\caption{ProxSkip}\label{alg:mainskip}
Given a stepsize $\gamma > 0$, probability $p > 0$, initial iterate $x_0 \in \Reals{d}$, initial control variate $h_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=1,2,\cdots,T-1$}

\State{$\widehat{x}_{t+1} = x_t - \gamma (\nabla f(x_t) \textcolor{red}{- h_t})$} 

\State{Flip a coin $\theta_t \in \{0,1\}$ where ${\rm Prob}(\theta_t = 1) = p$}

\If{$\theta_t = 1$} 
    \State{$x_{t+1} = {\rm prox}_{\frac{\gamma}{p}} \psi 
\left ( \widehat{x}_{t+1} \textcolor{red}{- \frac{\gamma}{p} h_t} \right )$} 
\Else
    \State{$x_{t+1} = \widehat{x}_{t+1}$}
    
\EndIf 

\State{$\textcolor{red}{h_{t+1} = \textcolor{blue}{h_t} + \frac{p}{\gamma}(x_{t+1} - \widehat{x}_{t+1})}$}
\EndFor
\end{algorithmic}
\end{algorithm}
 

\begin{remark}
The term $h_t$ is called the control variate and it plays a role of shiting the gradient $\nabla f(x_t)$ when the forward step is performed. We note that at the convergence, we have that $x_* = \widehat{x}_*$ and thus, the consensus is achieved and it holds that 
\begin{equation}
x_* = x_* - \gamma (\nabla f(x_*) - h_*).
\end{equation}
Therefore, $h_* = \nabla f(x_*).$
\end{remark}

\subsection{ProxSkip vs ADMM for $p=1$} 

For $p = 1$, we shall now prove that this is nothing else than ADMM. We consider the following minimization problem: 
\begin{equation} 
\min_{x,z} f(x) + \psi(z) \quad \mbox{ subject to } \quad z - x = 0. 
\end{equation}
This problem is equivalent to the original problem. The augmented Lagrangian can be defined as follows for a given parameter $\delta > 0$: 
\begin{equation} 
L_\rho(x,z,h) = f(x) + \psi(z) + h (z - x) + \frac{\rho}{2} \|x-z\|^2. 
\end{equation}
The alternating direction method of multiplier reads as follows: 
\begin{algorithm}
\caption{ADMMSkip}\label{alg:ADMMSkip}
Given a stepsize $\gamma > 0$, probability $p > 0$, initial iterate $x_0 = z_0 \in \Reals{d}$, initial control variate $h_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=1,2,\cdots,T-1$}

\State{$x_{t+1} = {\rm arg}\min_{x} L_\rho(x,z_t,h_t)$} 

\State{Flip a coin $\theta_t \in \{0,1\}$ where ${\rm Prob}(\theta_t = 1) = p$}

\If{$\theta_t = 1$} 
    \State{$z_{t+1} = {\rm arg}\min_z L_\rho(x_t, z, h_t) = {\rm Prox}_{\rho \Psi}(x_{t+1})$} 
\Else
    \State{$z_{t+1} = x_{t+1}$}
    
\EndIf 

\State{$\textcolor{red}{h_{t+1} = \textcolor{blue}{h_t} + \frac{p}{\gamma}(z_{t+1} - x_{t+1})}$}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{remark} 
For $p = 1$, we see that {\rm ADMMSkip} is {\rm ADMM}. 
\end{remark} 

\subsection{Linear Convergence of ProxSkip} 

We define 
\begin{equation}
x_* = {\rm arg}\min_x \left ( f(x) + \psi(x) \right ).
\end{equation}

For the convergence measure, we introduce the so-called Lyapunov function:  
\begin{equation} 
\Psi_t := \|x_t - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2.
\end{equation} 
We further define
\begin{subeqnarray}
w_t &=& x_t - \nabla f(x_t) \\ 
w_* &=& x_* - \nabla f(x_*). 
\end{subeqnarray}

Under these settings, we shall then show that ProxSkip generates iterates 
$\{x_t\}_{t = 1,\cdots}$ converges linearly in the sense that 
\begin{equation}
\mathbb{E}(\Psi_T) \leq (1 - \zeta)^T \Psi_0. 
\end{equation}
To achieve this theorem, we state a couple of assumptions on $f$ and $\psi$. 
\begin{assump}\label{ass1} 
$f$ is $L-$smooth and $r-$strongly convex. 
\end{assump}
By $L-$smoothness, we mean that 
\begin{equation}
\|\nabla f(x) - \nabla f(y) \| \leq L \|x - y\|, \quad \forall x, y \in \Reals{d}. 
\end{equation}
By $r-$strongly convex, we mean that 
\begin{equation}
f(y) \geq f(x) + \nabla f(x) ( y - x) +\frac{1}{2} r \|y - x\|^2, \quad \forall x, y \in \Reals{d}.  
\end{equation}
The regularizer $\psi$ satisfies
\begin{assump}\label{ass2} 
$\psi$ is proper, closed, and convex. 
\end{assump}
\begin{lemma} 
Under the above two Assumptions \ref{ass1} and \ref{ass2}, there exists a unique minimizer for the following problem: 
\begin{equation}
\min_{x} f(x) + \psi(x). 
\end{equation}
\end{lemma}
We also note that the Bregman divergence of a differentiable function $f: \Reals{d} \mapsto \Reals{}$ is defined by
\begin{equation} 
D_f(x,y) := f(x) - f(y) - \langle \nabla f(y), x - y \rangle. 
\end{equation} 
and the symmetrized Bregman divergence is given as 
\begin{equation} 
D_f(x,y) + D_f(y,x) = \langle \nabla f(x)  - \nabla f(y), x - y \rangle.  
\end{equation} 
For an $L-$smooth and $r-$strongly convex function, we have 
\begin{equation} 
\frac{r}{2} \|x - y\|^2 \leq D_f(x,y) \leq \frac{L}{2} \|x - y\|^2.
\end{equation} 
Furthermore, we have 
\begin{equation} 
\frac{1}{2L} \|\nabla f(x) - \nabla f(y)\|^2 \leq D_f(x,y) \leq \frac{1}{2r} \|\nabla f(X) - \nabla f(y)\|^2. 
\end{equation} 
%
%Given $\psi : \Reals{d} \mapsto \Reals{}$, we define $\psi^*(y) := \sup_{x \in \Reals{d}} \{ \langle x, y \rangle - \psi(x)\}$ to be its Fenchel conjugate.  
%
%The proximity operator of $\psi^*$ satisfies for any $\tau > 0$. 
%\begin{equation} 
%u = {\rm prox}_{\tau \psi^*} (y) \mbox{ implies } u \in y - \tau %\partial \psi^*(u). 
%\end{equation} 
We now present three technical lemmas, which will be needed to establish the main theorem. First of all, we shall need the following firm non-expansiveness of the proximity operator. The proof can be found at \cite{xxx}.  
\begin{lemma}
Let $\psi$ be proper, closed and convex and let $P(x):= {\rm prox}_{\frac{\gamma}{p}\psi}(x)$ and $Q(x) = x - P(x)$. Then, it holds that
\begin{equation}
\|P(x) - P(y)\|^2 + \|Q(x) - Q(y)\|^2 \leq \|x - y\|^2.    
\end{equation}
\end{lemma}

\begin{lemma}\label{3.3}
Under the assumptions \ref{ass1} and \ref{ass2}, for $\gamma > 0$ and $0 < p \leq 1$, we have 
\begin{equation} 
\mathbb{E}(\Psi_{t+1}) \leq \|w_t - w_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|h_t - h_*\|^2, 
\end{equation} 
where the expectation is taken over the $\theta_t$ in the Algorithm 1. 
\end{lemma} 
\begin{proof} 
We let $P(x) = {\rm prox}_{\frac{\gamma}{p}\psi}(x)$ and 
\begin{equation}
x := \widehat{x}_{t+1} - \frac{\gamma}{p} h_t \quad y = x_* - \frac{\gamma}{p} h_*. 
\end{equation}
The optimality conditin is that 
\begin{equation} 
x_* = P(y). 
\end{equation} 
The method reads as follows: 
\begin{equation}
x_{t+1} = \left \{ \begin{array}{ll} P(x) & \mbox{ with probability } p \\ \widehat{x}_{t+1} & \mbox{ with probability } 1 - p \end{array}  \right. 
\end{equation}
Furthermore, we have that 
\begin{equation}
h_{t+1} = h_t + \frac{p}{\gamma} (x_{t+1} - \widehat{x}_{t+1}) = \left \{ \begin{array}{ll} h_t + \frac{p}{\gamma} \left ( P(x) - \widehat{x}_{t+1} \right ) & \mbox{ with probability } p \\ h_t & \mbox{ with probability } 1 - p \end{array}  \right. 
\end{equation}
Now, we compute the expected value of the Lyapunov function 
\begin{equation}
\Psi_t := \|x_t - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 
\end{equation}
at the time step $t+1$, with respect to the coin toss at iteration $t$, which is 
\begin{eqnarray}
\mathbb{E}(\Psi_{t+1}) &=& p \left ( \|P(x) - x_*\|^2 + \frac{\gamma^2}{p^2} \left \|h_t + \frac{p}{\gamma} (P(x) - \widehat{x}_{t+1})  - h_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&=&   p \left ( \|P(x) - P(y) \|^2 + \left \| \frac{\gamma}{p} h_t + (P(x) - \widehat{x}_{t+1})  - \frac{\gamma}{p} h_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&=& p \left ( \|P(x) - P(y) \|^2 + \left \| P(x) - (\widehat{x}_{t+1} - \frac{\gamma}{p}h_t )  + ( y - x_*) \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&=& p \left ( \|P(x) - P(y) \|^2 + \left \| (P(x) - x) + ( y - P(y) ) \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&\leq& p \left \|\left ( \widehat{x}_{t+1} - \frac{\gamma}{p} h_t \right ) - \left ( x_* - \frac{\gamma}{p} h_* \right ) \right \|^2 + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ). 
\end{eqnarray}
A simple algebra shows that 
\begin{eqnarray}
\mathbb{E}(\Psi_{t+1}) &\leq& \|w_t - w_*\|^2 -\gamma^2\|h_t - h_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2. 
\end{eqnarray}
This completes the proof. 
\end{proof} 
We shall now need the last lemma: 
\begin{lemma}\label{3.4} 
Let Assumption \ref{ass1} hold with any $r \geq 0$. If $0 < \gamma \leq \frac{1}{L}$, then 
\begin{equation} 
\|w_t - w_*\|^2 \leq (1 - \gamma r) \|x_t - x_*\|^2. 
\end{equation} 
\end{lemma}
\begin{proof}
Recall the definition of $w_t$ and $w_*$ and thus, 
\begin{eqnarray}
\|w_t - w_*\|^2 &=& \|x_t - x_* - \gamma (\nabla f(x_t) - \nabla f(x_*))\|^2 \\
&=& \|x_t - x_*\|^2 + \gamma^2\|\nabla f(x_t) - \nabla f(x_*)\|^2 - 2\gamma \langle \nabla f(x_t) - \nabla f(x_*), x_t - x_* \rangle \\ 
&\leq& (1 - \gamma r) \|x_t - x_*\|^2  - 2\gamma D_f(x_t,x_*) + \gamma^2 \|\nabla f(x_t) - \nabla f(x_*)\|^2 \\ 
&=& (1 - \gamma r) \|x_t - x_*\|^2  - 2\gamma \left ( D_f(x_t,x_*) - \frac{\gamma}{2} \|\nabla f(x_t) - \nabla f(x_*)\|^2 \right ) \\ 
&\leq& (1 - \gamma r) \|x_t - x_*\|^2,  
\end{eqnarray}
where the last inequality is due to $0 \leq \gamma \leq 1/L.$ This completes the proof. 
\end{proof}
Using the above lemmas, we can conclude that 
\begin{theorem}
Let Assumptions \ref{ass1} and \ref{ass2} hold. Let $0 < \gamma \leq 1/L$ and 
$0 < p \leq 1$. Then the ProxSkip converges with the following property: 
\begin{equation} 
\mathbb{E}(\Psi_T) \leq (1 - \zeta)^T \Psi_0, 
\end{equation} 
where $\zeta:= \min\{\gamma r, p^2\}$. 
\end{theorem}
\begin{proof} 
By Lemma \ref{3.3} and Lemma \ref{3.4}, we have that 
\begin{eqnarray}
\mathbb{E}(\Psi_{t+1}) &\leq& \|w_t - w_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 \\ 
&\leq& (1 - \gamma r) \|x_t - x_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 \\
&\leq& (1 - \zeta) \left ( \|x_t - x_*\|^2 +  \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 \right ) \\
&\leq& (1 - \zeta) \Psi_t. 
\end{eqnarray}
By the recurrence relation, we arrive at the conclusion. This completes the proof.  
\end{proof}

\newpage 

\section{Optimization of the average of $n-$functions} 

We consider to minimize the average of $n$ functions using a cluster of $n$ compute nodes
\begin{equation}
\min_{x \in \Reals{d}} \left \{ f(x) := \frac{1}{n} \sum_{i=1}^n f_i(x) \right \}, 
\end{equation}
where $f_i : \Reals{d} \mapsto \Reals{}$ and it is owned by and stored by client $i \in [n] := \{1,\cdots,n\}.$ This problem is currently the dominant paradigm for training supervised machine learning models. 

This problem can be formulated in a somewhat different form given as follows. By cloning the model $x$ into $n$ independent copies $x_1,\cdots,x_n$ and casting them into the consensus form, we have  
\begin{equation}
\min_{x_1,\cdots,x_n \in \Reals{d}} \frac{1}{n} \sum_{i=1}^n f_i(x_i) + \psi (x_1,\cdots,x_n), 
\end{equation}
where the regualizer $\psi : \Reals{nd} \mapsto \Reals{}$, 
\begin{equation}
\psi(x_1,\cdots,x_n) :=  \left \{ \begin{array}{cc} 
0, & \mbox{ if } x_1 = x_2 = \cdots = x_n \\
+\infty, & \mbox{ otherwise }. 
\end{array} \right . 
\end{equation}
The evaluation of the proximity operator of $\psi$ given by, is 
\begin{equation}
{\rm arg}\min_{y \in C} \|y - x\|. 
\end{equation}

\begin{equation}
C := \{(x_1,\cdots,x_n) \in \Reals{nd} : x_1 = x_2 = \cdots = x_n\}.     
\end{equation}
We note that with $\overline{x} = \frac{1}{n} \sum_{i=1} x_i,$
\begin{equation}
{\rm prox}_{\gamma \psi}(x_1, \cdots, x_n) = (\overline{x},\cdots,\overline{x}) \in \Reals{nd}. 
\end{equation}
Basically, we observe that the proximity operator is to achieve the consensus. 

%\subsection{Decentralized training} 
%Given a graph $G = (V,E)$ with nodes $V$ and edges $E$, we assume that every communication node $i$ receives a weighted average of its neighbors' vectors with weights $W_{i,1},\cdots,W_{in} \in [0,1]$. We also assume that nodes $i$ and $j$ communicate if and only if $W_{ij} \neq 0$. This is basically equivalent to say that $(i,j) \in E$. %
%Therefore, the weights $W_{ij}$ define the mixing matrix $\tenq[2]{W}$. 
%
%The optimization problem with decentralized communication %can be cast into 
%\begin{equation} 
%\min_{x \in \Reals{d}} f(x) \quad \mbox{ subject to } %(\tenq[2]{I} - \tenq[2]{W}) x = 0. 
%\end{equation} 
%We can reformulate it into 
%\begin{equation} 
%\min_{x \in \Reals{d}} f(x) + \psi(Lx),  \quad \mbox{ %subject to } (\tenq[2]{I} - \tenq[2]{W}) x = 0. 
%\end{equation} 
%where 
\subsection{Federated Learning (FL) : } 

Federated learning is basically an optimization for some functional $f(x)$. However, data are assigned to each client and using such data set can enhance the accuracy of the constructed parameters. Due to the use of different data sets for any client, all the local functions $f_i$ are typically different. Therefore, the local steps can introduce a drift in the updates of each client. This leads to convergence issues. The main task for the FL community is to propose algorithm that could mitigate the client drift issue. 
 
The ProxSkip applied to FL can be summarized as follows: 
\begin{algorithm}
\caption{Scaffnew}\label{alg:mainscaff}
Given a stepsize $\gamma > 0$, probability $p > 0$, initial iterate $\{x_{i0}\}_{i=1,\cdots,n} \in \Reals{nd}$, initial control variate $\{h_{i0}\}_{i=1,\cdots,n} \in \Reals{nd}$ such that $\sum_{i=1}^n h_{i0} = 0$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}

\State{Server: flip a coin, $\theta_t \in \{0,1\}$, $T$ times, where ${\rm Prob}(\theta_t = 1) = p$ and send the sequence $\theta_0, \cdots, \theta_{T-1}$ to all workers} 

\For{$t=1,2,\cdots,T-1$, \mbox{ \textbf{in parallel, all workers $i \in [n]$}} }

\State{$\widehat{x}_{i,t+1} = x_{i,t} - \gamma (\nabla f_i(x_{i,t}) \textcolor{red}{- h_{i,t}})$} 

\If{$\theta_t = 1$} 
    \State{$x_{i,t+1} = {\rm prox}_{\frac{\gamma}{p}} \psi 
\left ( \widehat{x}_{i,t+1} \textcolor{red}{- \frac{\gamma}{p} h_{i,t}} \right ) = \frac{1}{n} \sum_{j=1}^n \widehat{x}_{j,t+1}$} 
\Else
    \State{$x_{i,t+1} = \widehat{x}_{i,t+1}$}
    
\EndIf 

\State{$\textcolor{red}{h_{i,t+1} = \textcolor{blue}{h_{i,t}} + \frac{p}{\gamma}(x_{i,t+1} - \widehat{x}_{i,t+1})} = \textcolor{blue}{h_{i,t}} + \frac{p}{\gamma}(\sum_{j=1}^n \widehat{x}_{j,t+1}- \widehat{x}_{i,t+1}) $}
\EndFor
\end{algorithmic}
\end{algorithm}
\begin{remark}
I consider that we may remove $h_{i,t}$ for the update of $h_{i,t+1}$. 
\end{remark}

We can mimick the convergence proof of ProxSkip algorithm to achieve the convergence of the Scaffnew algorithm. We now provide an assumption for each $f_i$ for all $i=1:n$ as follows: 
\begin{assump}\label{scaffnewass1} 
Each $f_i$ is $L-$smooth and $r-$strictly convex. 
\end{assump}
\begin{corollary}[Federated Learning]
Let Assumption \ref{scaffnewass1} hold and let $\gamma = 1/L$, $p = 1/\sqrt{\kappa}$, and $g_{i,t}(x_{i,t}) = \nabla f_i(x_{i,t})$. Then the iteration complexity of Scaffnew is $O(\kappa \log 1/\epsilon)$ and its communication complexity is $O(\sqrt{\kappa} \log 1/\epsilon)$.   
\end{corollary}

\newpage 

\section{Interpretation of Scaffnew Algorithm} 
We consider the total of $n-$particle whose location is given as 
\begin{equation}
\partial_t \tenq[2]{x} = \tenq[2]{v}, 
\end{equation}
We consider the following time dependent system of $n$ equations:  
\begin{equation} 
\partial_t \tenq[2]{v} = \Delta_p \tenq[2]{v} + \tenq[2]{h}(\tenq[2]{v}), 
\end{equation} 
subject to $\tenq[2]{v}(x,0) = \tenq[2]{v_0}$, $\tenq[2]{x_0} = \tenq[2]{x_0}$, where 
\begin{equation}
\tenq[2]{h}(\tenq[2]{s}) = \left ( \begin{array}{c} 
(\overline{s} - s_1 ) |\overline{s} - s_1|^{q-2} \\ (\overline{s} - s_2 ) |\overline{s} - s_2|^{q-2} \\ \vdots \\ ( \overline{s} - s_n ) |\overline{s} - s_n|^{q-2} \end{array} \right )
\end{equation} 
% |\tenq{s}|^{q-2}$
In a recent work, it was proved that 
\begin{theorem}
If $q = 2$, then the exponential decay to consensus. If $1 < q < 2$, then the finite time arrival to consensus, depending on the initial condition.  
\end{theorem}
I am currently working to extend it to replace $p-$Laplacian to obtain similar results. If we view the GD as the time integration to reach the steady state solution to $-\nabla f(v) + h(v) = 0$ or the time evolution or Jacobi method to solve the time dependent problem: 
\begin{equation}
\partial_t v = -\nabla f(v) + h(v), 
\end{equation}
as the iteration is stated as 
\begin{equation}
v_{t+1} = v_t + \gamma(-\nabla f(v_t) + h_t). 
\end{equation}
Now, we observe the time discrete form of $h_t$ given as follows: 
\begin{equation}
h_{t+1} - h_t = \frac{p}{\gamma}(\overline{v} - v). 
\end{equation}
This can be interpreted as 
\begin{equation}
\partial_t h = \overline{v} - v. 
\end{equation}
If we view $f(v) = \frac{1}{p} \int_\Omega |\nabla v|^p \, dx$, then we have that 
\begin{equation}
-\nabla f(v) = \Delta_p v.     
\end{equation}
Therefore, 
\begin{equation}
h_t = \frac{p}{\gamma} \int_0^t \overline{v} - v \, dt.  
\end{equation}
This can be interpreted as to the iterate we see in Algorithm 1, ProxSkip. 
This analoguous relation can be used to understand why $h$ is introduced to shift the gradient of $f$. 
\begin{remark}
We can speed up further the consensus, then the communication can be lowered.
\end{remark}

\section{Proposed Projects} 

In this section, we shall discuss two main subtasks. 



\subsection{Convergence analysis of Scaffnew-type methods in a framework of composite optimization} 

The theory of Scaffnew was originally developed for strongly convex problems \cite{mishchenko2022proxskip}. Our first target goal is, therefore, to extend such a theory to general convex loss functions. We will then proceed to extend Scaffnew algorithm to non-convex loss functions and perform numerical investigation for deep learning problems such as convolutional neural networks. 

As discussed in \S \ref{intro}, Scaffnew~\cite{mishchenko2022proxskip} constructs the modified, but equivalent formulation \cref{cp} of \cref{FL} as a block-separable structure of~\cref{FL} via introducing auxiliary variables. In \cref{FL}, each $f_k$ possesses its own parameter $x_k$ and the equalities among $x_k$'s are imposed by the constraint function $\psi$. This is in fact a very special case of a large class of mathematical optimization problems called composite optimization~\cite{Nesterov:2013}:
\begin{equation}
    \label{composite}
    \min_{x \in X} \left\{ F(x) + G(x) \right\},
\end{equation}
where $X$ is a solution space, $F \colon X \rightarrow \mathbb{R}$ is a smooth function, and $G \colon X \rightarrow \overline{\mathbb{R}}$ is a proper, convex, and lower semicontinuous function that is possibly nonsmooth.
Indeed, setting
\begin{equation*}
    x = (x_1, \dots, x_N), \quad
    F(x) = \sum_{k} f_k (x_k), \quad
    G(x) = \chi (x_1, \dots, x_N)
\end{equation*}
in~\cref{composite} yields~\cref{cp}. In the past decade, there have been numerous literature on efficient numerical solvers for composite optimization~\cref{cp}; see~\cite{CP:2016,Nesterov:2013,Teboulle:2018} and references cited therein. In particular, there have been several notable results on efficient subspace correction methods for composite optimization~\cite{Park:2021,Park:2022}. Exploiting the composite optimization structure of~\cref{cp}, we design an efficient subspace correction solver for~\cref{cp} that can effectively reduce the amount of communications. In~\cref{cp}, the constraint function $\psi$ was used to enforce the equalities among $x_k$'s. Meanwhile, there are various mathematical tools to synchronize the parameters in the clients, such as penalty methods and/or Lagrange multipliers. Using these tools, numerous variational problems equivalent to~\cref{FL} or relaxed version of models, which might be more relevant can be generated. Each of these variational problems has its own properties and there lies a possibility of designing efficient numerical solvers based on such properties. For example, in~\cite{GM:2012}, a multiple splitting algorithm to solve problems of the form~\cref{FL} was proposed based on an equivalent variational problem
\begin{equation*}
    \min_{\substack{x_k \in X,\\ 1 \leq k \leq N}} \sum_k f_k (x_k) \quad
    \textrm{ subject to } x_k = x_{k+1}, \textrm{ } 1 \leq k \leq N-1.
\end{equation*} 
By carefully studying the properties of variational formulations, we will develop efficient training algorithms or build a new shift operator in a framework of subspace correction strategies~\cite{Park:2020,Park:2021,Park:2022,Xu:1992,LWXZ:2007,LWXZ:2008}. On a separate issue, objective functionals that arise in FL are semi-coersive or nearly semi-coersive convex, \cite{lee2009robust,LWXZ:2007,hong2016uniformly,hong2016robust,chen2020robust}. In particular, Xu, Lee, and Park have collaborative research topics in progress on designing efficient subspace correction methods for various convex optimization problems~\cite{LPX:un2,LPX:un1}. These works are critical to design fast optimization algorithms for machine learning, such as logistic equation and they can be applied directly for FL.  

\subsection{Design and analysis of improved FL models using duality} 

%Our second proposed project is to apply the method of subspace correction for numerical solutions of partial differential equations \cite{Xu:1992}. 
A challenging aspect of solving~\cref{FL} is that every local objective function $f_k$ shares the same parameter $x$. Since there does not exist a choice for $x$ that minimizes every $f_k$ simultaneously in general, it is not very straightforward to design a numerical solver for~\cref{FL} that deals with each $f_k$ in parallel. On the other hand, we can consider various variational formulations equivalent to~\cref{FL} by using well-established tools in mathematical optimization, suitable for applying the idea of subspace correction, and design efficient subspace correction-based training algorithm for FL. Namely, FL can be connected to the framework of subspace correction method, based on the dual formulation of the objective functional \cite{zhang2002dual}. 

We assume that each $f_k$ is convex and that $E$ is strongly convex with parameter $r > 0$. Reallocating the strongly convex term in $E$ appropriately,~\cref{FL} can be rewritten as follows:
\begin{equation}\label{primal}
\min_{x \in X} \left\{ E(x) = \sum_k \tilde{f}_k (x) + \frac{r}{2} \|x\|^2 \right\},
\end{equation}
where each $\tilde{f}_k \colon X \rightarrow \mathbb{R}$, $1 \leq k \leq N$, is a convex function. Invoking Fenchel-Rockafellar duality~\cite{CP:2016}, one can obtain the following dual formulation of~\cref{primal}:
\begin{equation}\label{dual}
\min_{\substack{y_k \in X, \\ 1\leq k\leq N}} \left\{ \sum_k \tilde{f}_k^* (y_k) + \frac{1}{2r} \left\| \sum_k y_k \right\|^2 \right\},
\end{equation}
where $\tilde{f}_k^*$ is the convex conjugate of $\tilde{f}_k$ defined by
\begin{equation*}
    \tilde{f}_k^* (y) = \sup_{x \in X} \left\{ \left< y, x \right> - \tilde{f}_k (x) \right\}.
\end{equation*}
For example, if $\tilde{f}_k$ is given by the logistic loss function, $\tilde{f}_k (x) = \log (1 + e^{ax})$, then we have $\tilde{f}_k^* (y) = (y/a) \log (y/a) + (1 - y/a) \log (1 - y/a)$. We note that the cost function of~\cref{dual} has a block-separable structure. Therefore, it is possible to apply block coordinate descent methods~\cite{CP:2015}, or more generally, subspace correction methods. The convergence of subspace correction is well-known to generally accelerate when the local correction can be done in a more accurate manner unlike the Local DG, based on the operator splitting techniques. Such a formulation has been applied in limited cases in machine learning, such as SVM \cite{hsieh2008dual} and LR \cite{yu2011dual}. 

%
%The general guideline we propose is to consider the dual formulation of the objective functional \cite{zhang2002dual} via Fenchel-Rockafella Duality\cite{nachum2020reinforcement}. The advantage of this approach is that the unknown is not the parameter space, rather its dual variable is the unknown, which resides in the data points and thus, the large scale subspace correction framework can be naturally applied within FL framework. Furthermore, the convergence of subspace correction is well-known to generally accelerate when the local correction can be done in a more accurate manner unlike the Local DG which is based on the operator splitting techniques. Such a formulation has been applied in limited cases in machine learning, such as SVM \cite{hsieh2008dual} and LR \cite{yu2011dual}. In our approach, we first present various variational formulations that are equivalent to the original FL formulation such as the one considered in \cite{mishchenko2022proxskip} or modified FL models. By carefully studying the properties of variational formulations, we will develop efficient training algorithms or new shift operator, using subspace correction strategies in both the data space and the parameter space~\cite{Park:2020,Park:2021,Park:2022,Xu:1992,LWXZ:2007,LWXZ:2008}. Lastly, we note that generally, objective functionals that arise in FL are semi-coersive or nearly semi-coersive convex, \cite{lee2009robust,LWXZ:2007,hong2016uniformly,hong2016robust,chen2020robust}. The current ongoing collaborative effort in designing efficient subspace correction methods for various convex optimization problems \cite{LPX:un2,LPX:un1} will be applied to understand these open issues. These works are critical to design fast optimization algorithms for machine learning, such as logistic equation. %Xu is well-known for his studies in developing, designing, and analyzing fast methods for the solution of large-scale systems of equations, which include the BPX-preconditioner \cite{bramble1990parallel} and the HX-preconditioner \cite{hiptmair2007nodal}. Among others, his work on subspace correction methods in his SIAM Review paper \cite{xu1992iterative} is mostly relevant to the current proposal.


%The current major research effort on FL is devoted to the following three issues; (1) Convergence study of {\textit{FebAvg}} or its variants, (2) Improvement of Communication speeds between local and global servers, and (3) Development of models that take into account heterogeneity or unbalanced and non-i.i.d. data. Thus far, there have been a number of convergence theories that justify the use of {\textit{FebAvg}} as presented in \cite{li2019convergence,zhou2022convergence,haddadpour2019convergence,mitra2021linear}. Clearly, the main motivation of the FL is to encourage the local computations by each worker. Such local update for global model improvement has to be transferred to the server and thus algorithm developments are more or less motivated to improve communications between local and global server. 

%The most recent algorithm called ProxSkip, is shown to achieve the best communication efficiency until today \cite{mishchenko2022proxskip}. Figure \ref{xxx} shows the algorithm. Basically, this introduces proximal gradient with specially designed constraint operator for averaging or communication step. 

%This is randomized method, which apply the global communication with some probability. The communication rounds is not too much needed for the convergence. 

%The main idea of ProxSkip is to use the communication between local and global communication only once in a while, still achieving the convergence. However, we note that all the convergence theory rely on the assumption that the functional is strictly convex. Therefore, the state of the art theory lacks in handling realistic objective functionals, i.e., convex functional or non-convex functional \cite{haddadpour2019convergence}.
%
%
%However, we note that all the convergence theory rely on the assumption that the functional is strictly convex. Therefore, the state of the art theory lacks in handling realistic objective functionals, i.e., convex functional or non-convex functional \cite{haddadpour2019convergence}.




%We have expertises in subspace correction methods, randomized solvers and convex optimization.

%
%Specifically, the following two tasks will be accomplished in this project. %Michalis Kalitsis (Merit Network, Inc at Univ of Michigan)'s expertise is at algorithm design, ML, and security and privacy, especially for IoT applications. Prasenjit Mitra (Penn State)'s interest lies in AI, ML on Web and Social media, Hadi Hosseini (Penn State)'s research expertise is AI and Multiagent Systems.
 
%{\textbf{In this proposal, we shall establish the convergence for {\textit{FebAvg}} only under $F$ being convex, which is an open issue, and also suggest strategies to design efficient numerical solvers for~\cref{FL}, which include sampling and averaging techniques. We shall also present an improved model that can adequately deal with unbalanced and non-i.i.d. clients' data.}}

%\begin{description}
%\item[Task 1:] To design client-level parallel subspace correction methods for Federated Learning by identifying block-separable structures of the problem.
%\item[Task 2:] To design efficient numerical methods for FL via constructing equivalent %reformulation or relaxed variational formulations of FL. % which are derived by using the %optimization theory 
%\end{description}
 

%Specifically, the following two tasks will be accomplished in this project. %Michalis Kalitsis (Merit Network, Inc at Univ of Michigan)'s expertise is at algorithm design, ML, and security and privacy, especially for IoT applications. Prasenjit Mitra (Penn State)'s interest lies in AI, ML on Web and Social media, Hadi Hosseini (Penn State)'s research expertise is AI and Multiagent Systems.
 
%{\textbf{In this proposal, we shall establish the convergence for {\textit{FebAvg}} only under $F$ being convex, which is an open issue, and also suggest strategies to design efficient numerical solvers for~\cref{FL}, which include sampling and averaging techniques. We shall also present an improved model that can adequately deal with unbalanced and non-i.i.d. clients' data.}}

%\begin{description}
%\item[Task 1:] To design client-level parallel subspace correction methods for Federated Learning by identifying block-separable structures of the problem.
%\item[Task 2:] To design efficient numerical methods for FL via constructing equivalent reformulation or relaxed variational formulations of FL. % which are derived by using the optimization theory 
%\end{description}

%\section{Proposed Projects}
%In this section, we present some description of the proposed projects. 
\subsection{Subspace correction methods}
Subspace correction methods are prominent numerical solvers for large-scale problems because they can efficiently utilize massively parallel computer architectures. More generally, various iterative methods such as multigrid methods and domain decomposition methods can be interpreted as subspace correction methods~\cite{Xu:1992}. Algebraic multigrid methods~\cite{XZ:2017} provides a general framework to design a multilevel training algorithm without the limitation of geometric grids. There have been many important developments on subspace correction methods and algebraic multigrid methods for linear and nonlinear problems, but nonlinear preconditioning is much less explored than linear preconditioning. In the context of domain decomposition, a seminal contribution for nonlinear preconditioning was made by~\cite{CKY:2001}, namely the Additive Schwarz Preconditioned Inexact Newton method~(ASPIN), see also~\cite{CK:2002}. The idea is: The nonlinear system is transformed into a new nonlinear system, which has the same solution as the original system. For certain applications the nonlinearities of the new function are more balanced and, as a result, the inexact Newton method converges more rapidly. For solving nonlinear equations and optimization problems, a good nonlinear preconditioner can sometimes drastically improve the robustness of the nonlinear convergence by reducing the impact of certain parameters. 
Recently, some acceleration schemes that can be applied to subspace correction methods for general convex optimization problems were proposed~\cite{Park:2021,Park:2022}. We also mention that the speed up has been observed with specially designed randomized choices for mini batch \cite{li2019convergence,zhou2022convergence}, which is expected to be analyzed and understood in the framework of randomized subspace correction method studied in \cite{hu2019randomized} as well. We expect that novel training algorithms for FL with improved efficiency in the senses of both communication and computation can be designed using subspace correction strategies explained above. We propose subspace correction-based training algorithm for FL that can can significantly reduce the amount of communications between the server and clients by reducing the number of required epochs.

% Section: Variational formulations for federated learning
\subsection{Variational formulations for federated learning}
A challenging aspect of solving~\cref{FL} is that every local objective function $f_k$ shares the same parameter $x$. Since there does not exist a choice for $x$ that minimizes every $f_k$ simultaneously in general, it is not very straightforward to design a numerical solver for~\cref{FL} that deals with each $f_k$ in parallel.

Meanwhile, various variational formulations equivalent to~\cref{FL} can be obtained by using well-established tools in mathematical optimization. We find variational formulations equivalent to~\cref{FL} that are suitable for applying the idea of subspace correction, and design efficient subspace correction-based training algorithm for FL.

A possible formulation to apply subspace correction strategies is a dual formulation of~\cref{FL}, which is more suitable for designing a client-wise parallel algorithm than the original formulation~\cref{FL}. We assume that each $f_k$ is convex and that $E$ is strongly convex with parameter $r > 0$. Reallocating the strongly convex term in $E$ appropriately,~\cref{FL} can be rewritten as follows:
\begin{equation}
    \label{primal}
    \min_{x \in X} \left\{ E(x) = \sum_k \tilde{f}_k (x) + \frac{r}{2} \|x\|^2 \right\},
\end{equation}
where each $\tilde{f}_k \colon X \rightarrow \mathbb{R}$, $1 \leq k \leq N$, is a convex function. Invoking Fenchel--Rockafellar duality~\cite{CP:2016}, one can obtain the following dual formulation of~\cref{primal}:
\begin{equation}
    \label{dual}
    \min_{\substack{y_k \in X, \\ 1\leq k\leq N}} \left\{ \sum_k \tilde{f}_k^* (y_k) + \frac{1}{2r} \left\| \sum_k y_k \right\|^2 \right\},
\end{equation}
where $\tilde{f}_k^*$ is the convex conjugate of $\tilde{f}_k$ defined by
\begin{equation*}
    \tilde{f}_k^* (y) = \sup_{x \in X} \left\{ \left< y, x \right> - \tilde{f}_k (x) \right\}.
\end{equation*}
For example, if $\tilde{f}_k$ is given by the logistic loss $\tilde{f}_k (x) = \log ( 1 + e^{ax})$, then we have $\tilde{f}_k^* (y) = (y/a) \log (y/a) + (1 - y/a) \log (1 - y/a)$.
As the cost function of~\cref{dual} has a block-separable structure, it is suitable to apply block coordinate descent methods~\cite{CP:2015}, or more generally, subspace correction methods.

An alternative way to create a block-separable structure in~\cref{FL} is to introduce some auxiliary variable. More precisely, we introduce $N$ copies $x_1$, \dots, $x_N$ of $x$ that play roles of the parameters of $f_1$, \dots, $f_N$, respectively, as follows:
\begin{equation}
    \label{constrained}
    \min_{\substack{x_k \in X,\\1\leq k\leq N}} \left\{ \sum_k f_k (x_k) + \chi (x_1, \dots, x_N) \right\},
\end{equation}
where
\begin{equation*}
    \chi(x_1, \dots, x_N) = \begin{cases}
    0, & \textrm{ if } x_1 = \dots = x_N, \\
    \infty, & \textrm{ otherwise.}
    \end{cases}
\end{equation*}
That is, each $f_k$ possesses its own parameter $x_k$ and the equalities among $x_k$'s are imposed by the constraint function $\chi$. The formulation~\cref{constrained} was considered in several existing works, e.g., ProxSkip~\cite{mishchenko2022proxskip}. Here, we focus on the fact that~\cref{constrained} is an instance of a large class of mathematical optimization problems called composite optimization~\cite{Nesterov:2013}:
\begin{equation}
    \label{composite}
    \min_{x \in X} \left\{ F(x) + G(x) \right\},
\end{equation}
where $X$ is a solution space, $F \colon X \rightarrow \mathbb{R}$ is a smooth function, and $G \colon X \rightarrow \overline{\mathbb{R}}$ is a proper, convex, and lower semicontinuous function that is possibly nonsmooth.
Indeed, setting
\begin{equation*}
    x = (x_1, \dots, x_N), \quad
    F(x) = \sum_k f_k (x_k), \quad
    G(x) = \chi (x_1, \dots, x_N)
\end{equation*}
in~\cref{composite} yields~\cref{constrained}. In the past decade, there have been numerous literature on efficient numerical solvers for composite optimization~\cref{composite}; see~\cite{CP:2016,Nesterov:2013,Teboulle:2018} and references therein. In particular, there have been several notable results on efficient subspace correction methods for composite optimization~\cite{Park:2021,Park:2022}. Exploiting the composite optimization structure of~\cref{constrained}, we design an efficient subspace correction solver for~\cref{constrained} that can effectively reduce the amount of communications. In~\cref{constrained}, the constraint function $\chi$ was used to enforce the equalities among $x_k$'s. Meanwhile, there are various mathematical tools to synchronize the parameters in the clients, such as penalty methods and Lagrange multipliers. Using these tools, numerous variational problems equivalent to~\cref{FL} or relaxed version of models, which might be more relevant can be generated. Each of these variational problems has its own properties and there lies a possibility of designing efficient numerical solvers based on such properties. For example, in~\cite{GM:2012}, a multiple splitting algorithm to solve problems of the form~\cref{FL} was proposed based on an equivalent variational problem
\begin{equation*}
    \min_{\substack{x_k \in X,\\ 1 \leq k \leq N}} \sum_k f_k (x_k) \quad
    \textrm{ subject to } x_k = x_{k+1}, \textrm{ } 1 \leq k \leq N-1.
\end{equation*}
We shall investigate various variational models equivalent to the original formulation~\cref{FL} for FL, and then try to find efficient numerical strategies to solve the models. 

\section{The update of the Lagrange multiplier} 
We consider to solve
\begin{equation} 
\min_{\{x \in \Reals{n} : Ax = b\}} f(x) 
\end{equation} 
This problem can be formulated as a saddle problem as 
\begin{equation}
\max_u \min_{x} \left \{ f(x) + u^T (Ax - b) \right \}
\end{equation}
We define $g(u)$ by 
\begin{equation} 
g(u) = \min_x \left \{ f(x) + u^T (Ax - b) \right \}. 
\end{equation} 
We recall that the conjugate of $f$ denoted by $f^*$ is defined as follows: 
\begin{equation}
f^*(y) := \max_{x} y^T x - f(x). 
\end{equation}
Therefore, we see that 
\begin{eqnarray*} 
g(u) &=& - \max_x \left \{ -f(x) - u^T (Ax - b)  \right \} \\ 
&=&  - \max_x \left \{ -f(x) - (A^T u)^T x + u^Tb \right \} \\
&=&  - \max_x \left \{ - (A^T u)^T x - f(x) \right \} - u^Tb = - f^*(-A^Tu) - u^Tb. 
\end{eqnarray*} 
The dual variable is then to satisfy the following maximum optimization: 
\begin{equation} 
\max_u g(u). 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
u_k = u_{k-1} + t_k \partial g(u_{k-1}), 
\end{equation} 
where $t_k \geq 0$. We note that $u_{k-1}$ produces $x_k$ as the minimizer given as follows: 
\begin{equation} 
x_k = {\rm arg}\min_x \left ( f(x) + u_{k-1}^T (Ax - b)\right ) 
\end{equation} 
We recall the well-known fact that if $f$ is closed and convex, then 
\begin{equation}
y \in \partial f(x) \quad \Leftrightarrow \quad x \in \partial f^*(y) \quad \Leftrightarrow \quad x \in {\rm arg}\min_z f(z) - y^T z.
\end{equation}
Using this fact, we shall show that $\partial g(u_{k-1}) = Ax_k - b$. First, we note that 
\begin{equation}
\partial g(u) = A \partial f^*(-A^T u) - b. 
\end{equation}
Therefore,  if $x \in \partial f^*(-A^Tu)$, then $x = {\rm arg}\min_z f(z) - (-A^Tu)^T z$. Namely, we have that 
\begin{equation}
{\rm arg}\min_z f(z) - (-A^Tu)^T z = f(x) + u^T A x. 
\end{equation}
Thus, we have that
\begin{equation}
\partial g(u_{k-1}) = A \partial f^*(-A^T u_{k-1}) - b = Ax_{k} - b.  
\end{equation}

\subsection{Augmented Lagrangian Method} 

We note that the choice of the step size $t_k$ in the ADMM is unclear. To remedy this issue, we consider the augmented Lagrangian method. This reads as follows: 
\begin{equation} 
\min_{\{x \in \Reals{n} : Ax = b\}} f(x) + \frac{\rho}{2} \|Ax - b\|^2. 
\end{equation} 
This problem can be formulated as a saddle problem as 
\begin{equation}
\max_u \min_{x} \left \{ f(x) + u^T (Ax - b) + \frac{\rho}{2} \|Ax - b\|^2 \right \}
\end{equation}
We define $g(u)$ by 
\begin{equation} 
g(u) = \min_x \left \{ f(x) + u^T (Ax - b) + \frac{\rho}{2} \|Ax - b\|^2 \right \}. 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
u_k = u_{k-1} + \rho \partial g(u_{k-1}), 
\end{equation} 
where $t_k = \rho$ is chosen. The reason why it makes sense is that 
\begin{equation} 
x_k = {\rm arg}\min_x \left ( f(x) + u_{k-1}^T (Ax - b) + \frac{\rho}{2} \|Ax - b\|^2 \right ). 
\end{equation} 
Namely, we have that
\begin{equation} 
\partial f(x_k) + A^T(u_{k-1} + \rho (A x_k - b)) = 0. 
\end{equation} 
This is the stationary condition for original primal problem if $u_k = u_{k-1} + \rho (A x_k - b)$.   



\section{The update of the Lagrange multiplier for the federated learning algorithm} 

We denote the vector $X \in \mathbb{R}^{nd}$ as follows: 
\begin{equation}
X = \left ( \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_d \end{array} \right ) \in \mathbb{R}^{nd}, \quad \mbox{ where } x_j \in \mathbb{R}^{n}, \mbox{ and } \forall i=1,\cdots,d. 
\end{equation}
We now introduce a matrix $K \in \mathbb{R}^{d} \rightarrow \mathbb{R}^{nd \times d}$ defined by the following relation: 
\begin{equation} 
K = \left ( \begin{array}{c} I_d \\ I_d \\ \vdots \\ I_d \end{array} \right ), 
\end{equation}
where $I_d \in \mathbb{R}^{d\times d}$ is the identity matrix. Therefore, we see that for $z \in \mathbb{R}^{d}$, we have 
\begin{equation} 
K z = \left ( \begin{array}{c} z \\ z \\ \vdots \\ z \end{array} \right ) \in \mathbb{R}^{nd}. 
\end{equation} 
With $V = \mathbb{R}^{nd} \times \mathbb{R}^d$, we now consider to solve 
\begin{equation} 
\min_{X \in V } F(X) + H^T(Kz - X) + \frac{r}{2} \|Kz - X\|^2. 
\end{equation} 
This problem can be formulated as a saddle problem as follows: 
\begin{equation}\label{main:eq} 
\max_{H \in \mathbb{R}^{nd}}  \min_{X \in V} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}
\end{equation}
We define $g(H)$ as follows: 
\begin{equation}\label{gfunction}
g(H) = \min_{X} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}. 
\end{equation}
We shall now show how the Lagrange multiplier should be updated in the following Lemma. 
\begin{lemma}
Let $F$ be closed and convex and for a fixed $H_{k-1}$ and $z = z_k$, let 
\begin{equation} 
X_k = {\rm arg}\min_{X} ( F(X) + H_{k-1}^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2).  
\end{equation} 
Then, it holds that 
\begin{equation} 
\partial g(H_{k-1}) = Kz_k - X_k,  
\end{equation}
where $g$ is defined in \eqref{gfunction}. 
Furthermore, we have that
\begin{equation} 
\partial F(X_k) - (H_{k-1} + r (K z_k - X_k)) = 0. 
\end{equation} 
Therefore, the Lagrange multiplier has to be updated via the following formular: 
\begin{equation}
H_k = H_{k-1} + r (Kz_k - X_k). 
\end{equation}
\end{lemma}
\begin{proof} 
We define $G(X) = F(x) + \frac{r}{2} \|Kz - X\|^2$ and recall that the conjugate of $G$ denoted by $G^*$ is defined as follows: 
\begin{equation}
G^*(Y) := \max_{X} Y^T X - G(X). 
\end{equation}
Therefore, we see that 
\begin{eqnarray*} 
g(H) &=& -\max_X \left \{ -G(X) - H^T (Kz - X)  \right \} \\ 
&=&  -\max_X \left \{ -G(X) - (K^T H)^T z + H^TX \right \} \\
&=&  - \max_X \left \{ H^T X - G(X) \right \} + H^T K z \\
&=& - G^*(H) + H^TKz. 
\end{eqnarray*} 
The dual variable is then to satisfy the following maximum optimization: 
\begin{equation} 
\max_H g(H). 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
H_k = H_{k-1} + t_k \partial g(H_{k-1}), 
\end{equation} 
where $t_k \geq 0$. We note that $H_{k-1}$ produces $X_k$ as the minimizer given as follows: 
\begin{equation} 
X_k = {\rm arg}\min_Y \left ( G(Y) + H_{k-1}^T (K z_k - X) \right ).  
\end{equation} 
We now recall the well-known fact that if $G$ is closed and convex, then 
\begin{equation}
Y \in \partial G(X) \quad \Leftrightarrow \quad X \in \partial G^*(Y) \quad \Leftrightarrow \quad X \in {\rm arg}\min_Z G(Z) - Y^T Z.
\end{equation}
Using this fact, we shall show that $\partial g(H_{k-1}) = Kz_k - X_k$. First, we note that 
\begin{equation}
\partial g(H) = \partial G^*(H) + Kz. 
\end{equation}
Therefore,  if $X \in \partial G^*(H)$, then $X = {\rm arg}\min_Z G(Z) - H^TZ$. Namely, we have that 
\begin{eqnarray*}
H_{k-1}^T Kz_k + {\rm arg}\min_Z \left \{ G(Z) + H_{k-1}^T Z \right \} &=& G(X_k) + H_{k-1}^T (Kz_k - X_k) \\
&=& F(X_k) + (H_{k-1}^T (Kz_k - X_k) + \frac{r}{2}\|Kz_k - X_k\|^2. 
\end{eqnarray*}
Thus, we have that
\begin{equation}
\partial g(H_{k-1}) = Kz_k - X_k. 
\end{equation}
This completes the proof. 
\end{proof} 



\bibliographystyle{plain}
\bibliography{mybib}

\end{document} 

