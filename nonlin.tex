
\section{Federated Learning for $F$ being a nonlinear convex functional} 

In this section, we shall provide a couple of conditions on the objective functional and discuss the convergence of ProxSkip as discussed in \cite{mishchenko2022proxskip}. We shall assume that $F : \Reals{N_x} \mapsto \Reals{}$ be proper, lower semi-continuous and a strongly convex functional and also that $F$ is $\lambda_F$-strongly convex and $L_F$-smooth. Namely, with $L_F, \lambda_F > 0$, we have 
\begin{subeqnarray*}
F(X) - F(Y) - \langle \nabla F(Y), X-Y \rangle &\geq& \frac{\lambda_F}{2}\|X-Y\|^2, \quad \forall X, Y \in \Reals{N_x} \\
\|\nabla F(X) - \nabla F(Y) \| &\leq& L_F \|X - Y\|, \quad\forall X, Y \in \Reals{N_x},   
\end{subeqnarray*}
We note that the condition number of $F$, denoted by $\kappa(F)$ is then given as
\begin{equation}
\kappa(F) = \frac{L_F}{\lambda_F}. 
\end{equation}

\subsection{Review of the convergence result available in literature and open problem} 
The Stochastic ProxSkip algorithm \cite{mishchenko2022proxskip} is proven to converge with optimal rate under the assumption of an appropriate choice of parameters. We recall that the Stochastic ProxSkip algorithm is given in Algorithm \ref{Prox}. 
\begin{algorithm}
\caption{Stochastic ProxSkip \cite{mishchenko2022proxskip}}\label{Prox} 
Given $\gamma = 1/L_F > 0$, and probability $p > 0$, initial iterate $X_0 = Kz_0$ and initial control variate $H_0$, such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots, N_{tot}$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$),  
    \begin{equation}
    X_{k+1} = X_k - \gamma (\nabla F(X_k) - H_k), 
    \end{equation}}
    \State{Flip a coin $\theta_k \in \{0,1\}$ where ${\rm Prob}(\theta_k = 1) = p$}
    \If{$\theta_k = 1$}
    \State{
    \begin{equation}
        Kz_{k+1} = \frac{1}{N} \sum_{i=1}^N X_{k+1}^i
    \end{equation} }
    \Else{}
    \State{
    \begin{equation}
        Kz_{k+1} = X_{k+1}         
    \end{equation} }
    \EndIf
    \State{   
    \begin{equation} \label{Hupdate}
        H_{k+1} = H_k + \frac{p}{\gamma} (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}
To describe the convergence of the Stochastic ProxSkip, established in \cite{mishchenko2022proxskip}, we let $(X_*, Kz_*)$ be the optimal solution to the equation \eqref{main:prob} and $H_*$ be the optimal control variate such that 
\begin{equation}
H_* = \nabla F(X_*). 
\end{equation}
We then let 
\begin{equation} 
\Psi_k := \|Kz_k - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2.
\end{equation} 
%We further define
%\begin{subeqnarray}
%w_k &=& Kz_k - A (Kz_k) \\ 
%w_* &=& Kz_* - A (X_*). 
%\end{subeqnarray}
%We consider to choose 
%\begin{equation}
%\omega = \frac{1}{\gamma n} \, \mbox{ and } \, p = \frac{1}{n}.
%\end{equation}
%Therefore, $\omega^2 = p^2/\gamma^2$. Thus, the weights in the Lyapunov function is %nothing else than, 
%\begin{equation} 
%\omega^2 \Psi_k := \omega^2 \|Kz_k - X_*\|^2 + \|H_k - H_*\|^2.
%\end{equation}
Under these settings, it was prove that Algorithm \ref{Prox} generates iterates 
$\{(X_k, Kz_k, H_k)\}_{k = 1,\cdots}$, which converges with the following property: 
\begin{theorem} 
For $\gamma = \frac{1}{L_F} > 0$ and $0 < p \leq 1$, the Algorithm \ref{Prox} we have 
\begin{equation} 
\mathbb{E}(\Psi_{N_{\rm tot}}) \leq \left (1 - \min \left \{ \frac{1}{\kappa(F)}, p^2 \right \} \right )^{N_{\rm tot}} \Psi_0. 
\end{equation} 
where the expectation is taken over the $\theta_k$ in the Algorithm 1. 
\end{theorem} 
We note that $N_{\rm tot}$ is the total iterations, i.e., $N_{\rm tot} \approx n \times N_c$, where $N_c$ is the number of communications and $n$ is the average number of local Gradient descent steps in each communication. Thus, if we interpret $N_c$ as the outer iteration count, then we have the following heuristic convergence rate estimate. For $\gamma = \frac{1}{L_F}$ and $p = \frac{1}{n}$ with $n = \sqrt{\kappa(F)}$, the Algorithm \ref{Prox} converges with the rate given as \begin{eqnarray*} 
\mathbb{E}(\Psi_{N_{\rm tot}}) &\leq& \left (1 - \frac{1}{\kappa(F)} \right )^{N_{\rm tot}} \Psi_0 \approx \exp \left \{ \frac{-N_{\rm tot}}{\kappa} \right \} \Psi_0 \\
&\approx& \exp \left \{ \frac{-n N_c}{\kappa(F)} \right \} \Psi_0 \approx \left ( 1 - \frac{1}{\kappa(F)/n} \right )^{N_c} \Psi_0. 
\end{eqnarray*} 
Therefore, with $p = 1/\sqrt{\kappa(F)}$, we arrive at 
\begin{equation}
\mathbb{E}(\Psi_{N_{\rm tot}}) \approx \left ( 1 - \frac{1}{\sqrt{\kappa(F)}} \right )^{N_c} \Psi_0. 
\end{equation}
This means that we have the improvement with the local steps. Furthermore, we are guided how to state the convergence rate in a way that the number of local steps improve the overall convergence rate. The key shall be found at the parameter $\omega$. 
%
%Namely, if $n = \sqrt{\kappa}$, then we achieve the optimal convergence. On the other hand, if it is smaller or larger than $\sqrt{\kappa(A)}$, then the convergence becomes worse. Therefore, if we are able to achieve that $n = \sqrt{\kappa}$, then we have the desired convergence rate in every communications. \textcolor{red}{The local iteration counts can not be too large in this setting !} \textcolor{blue}{This is to ask if we can make $\rho_1$ small effortlessly, i.e., if it is possible to make $\rho_1$ negligible for $n = \sqrt{\kappa}$, namely, if we can design a local solve which can converge very fast with small local iterations like multigrid method, which converges independent of the problem size and with small number of sweep, then by choosing $\omega = \lambda_F/n$, we can make } 
%\begin{equation}
%\rho_2(r) = 1 - \frac{1}{n}. 
%\end{equation}
%Now, by choosing $n = \sqrt{\kappa}$, we can obtain the similar result. %We need to show that under the assumption that the orthogonality holds: 
%\begin{equation}
%\rho = \max \{\rho_1(r) , \rho_2(r) \} = \rho_2(r)  
%\end{equation}
%and we can choose $\rho_2(r) = 1 - 1/n$ for any $n$ and thus we can choose $n = \sqrt{\kappa}$. 
%
%But, it is kind of contradictory since $n$ should be large enough to make sure $\rho_1$ is negligible. So, our goal is to make $\rho_1$ negligible regardless of the choice of $n$. On the other hand, to make $\rho_1$ small, if we choose $r$ large, then GD part will go to zero quickly, but $r$ factor can slow down the process. It is kind of contradictory. Namely, we have the convergence rate given as below:
%\begin{equation}
%\rho_1 = \frac{r}{r + \lambda} + \left ( \frac{L - \lambda}{2r + L + \lambda} \right %)^2. 
%\end{equation}
%
%\begin{proof} 
%We let $Kz = P_Z(X)$.   
%\begin{equation}
%X := X_{k+1} - \frac{\gamma}{p} H_k \quad \mbox{ and } \quad Y = X_* - \frac{\gamma}{p} H_*. 
%\end{equation}

\subsection{Linear Convergence Anaysis of FL in the framework of inexact Uzawa formulation}\label{flalgorithm} 

In this section, we shall formulate the Federated learning as an inexact Uzawa method \cite{bramble1997analysis} and discuss the convergence analysis for linear and nonlinear case. We begin with some preliminaries. 

\subsubsection{Convex Functions and Some  Preliminaries}\label{problem} 
In this section, we shall formally define problem and introduce a couple of important facts about convex functional. We begin by noting that the problem \eqref{main:prob} can be reformulated to solve the following optimization problem: 
\begin{equation}\label{main:eq2} 
\mathop{\arg \min }_{\substack{X \in \Reals{ N_x} \\ K z - X = 0, \,\, z \in \Reals{d}}} F(X), 
\end{equation}
where $Kz$ is $N_c$ copies of $z$. We note that for the constraint $Kz = X$, we can introduce the Lagrange multiplier $H$ and reformulate the problem \eqref{main:eq2} in the following form: 
\begin{equation}
\min_{X \in \Reals{N_x}, z \in \Reals{d}} \max_{H \in \Reals{N_x}} F(X) + \langle H, Kz - X \rangle,  
\end{equation}
which can be further modified to be an Augmented Lagrangian method given as follows: 
\begin{equation}
\min_{X \in \Reals{N_x}, z \in \Reals{d}} \max_{H \in \Reals{N_x}} L_r(X,z,H),   
\end{equation}
where with $r > 0$ being a positive parameter, 
\begin{equation}\label{lagrange} 
L_r(X,z,H) = F(X) + \langle H, K z - X \rangle + \frac{r}{2} \|Kz - X\|^2. 
\end{equation}
Note that the optimality conditions for Lagrangian and the Augmented Lagrangian are given as follows. 
\begin{equation}\label{Lag}
\begin{cases}
\nabla F(X_*) - H_* = 0,\\
K^T H_* = 0, \\
Kz_* - X_*= 0. 
\end{cases}
\end{equation}
On the other hand, the optimality condition for the Augmented Lagrangian formulation is given as follows: 
\begin{equation}
\label{augLag}
\begin{cases}
\nabla F(X_*) - H_* - r (Kz_* - X_*) = 0, \\
K^T H_*  + r K^T(Kz_* -X_*) = 0,\\
K z_* - X_* = 0
\end{cases}
\end{equation}  
Throughout the paper, we shall denote $A = \nabla F$ and among others, it is important to keep in mind that it holds
\begin{equation} 
A(X_*) = H_*. 
\end{equation}
%We shall state and prove a simple, but important result: \begin{theorem}
%The following system admits a unique solution: 
%\begin{equation}
%\begin{pmatrix}
%\nabla F(\cdot) & 0 & -I \\
%0 & 0 & K^T\\
%-I& K & 0\\
%\end{pmatrix} \begin{pmatrix}
%X \\ z \\ H
%\end{pmatrix} = \begin{pmatrix}
%f \\ g \\ 0
%\end{pmatrix}
%\end{equation}
%\end{theorem}
%\begin{proof}
%We apply the inf-sup condition with nonlinear first $(1,1)$ block, which is stated in \cite{howell2011inf, xu2003some}. This completes the proof. 
%\end{proof}
%\begin{proposition}
%The optimality conditions for the Lagrangian and the augmented Lagrangian are equivalent.
%\end{proposition}
%\begin{proof}
%Multiplying the third equation in \eqref{optimality for Lag} and subtracting it from the first equation yields the first equation in \eqref{optimality for aug Lag}. Multiplying the third equation by $r K^T$ in \eqref{optimality for Lag} and adding it to the second equation yields the second equation in \eqref{optimality for aug Lag}.
%\end{proof}
%
%\begin{theorem}
%For strongly convex and L-smooth objective function $F(X)$ in \eqref{new formulation}, we have 
%\begin{enumerate}
%    \item The global minimizer $X_* = K z_* $ always exists. 
%    \item There exists $H_*$ that solves the optimality condition \eqref{optimality for aug Lag} along with the global minimizer $z_*$, $X_*$. Furthermore, the solution to the optimality conditions gives the global minimizer $z_*$ and $X_*$. 
%\end{enumerate}
%\end{theorem}
%\begin{proof}
%Later. 
%This requires the theory of Lagrange multiplier method for strongly convex and $L-$smooth objective with linear constraints. 
%\end{proof}
We shall interpret the Scaffnew algorithm as to find $(X_*, z_*, H_*)$ for the system describing the optimality condition of the Augmented Lagrangian. Namely, we are intereted in solving the following nonlinear system: \begin{equation}
\label{completesystem}
\begin{pmatrix}
\nabla F(\cdot)+ r I & - r K & -I \\
-r K^T& r K^TK & K^T\\
-I& K & 0\\
\end{pmatrix}
\begin{pmatrix}
X_*\\
z_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}
\end{equation}

We shall now introduce a couple of important fact about convex functional, some of which can be found at \cite{kakade2009duality}. First of all, we introduce the  Legendre-Fenchel convex conjugate of the functional $F$, denoted by $F^* : \Reals{N_x} \rightarrow \Reals{}$, which is defined by the following:
\begin{equation}
F^*(S) = \max_{X \in \Reals{N_x}} \{\langle S,X \rangle - F(X) \}.    
\end{equation}
Among others, we recall the following important lemma, stated for the dual of $F$: 
\begin{lemma}
Let $F : \Reals{N_x} \rightarrow \Reals{}$ be a proper, convex, and lower semi-continuous. Then, $F$ is $\lambda_F-$strongly convex if and only if $F^*$ is $\frac{1}{\lambda_F}-$smooth. 
\end{lemma}
We also hereby note that the gradient of $F^*$ is basically the inverse of $\nabla F$. Namely, we have that 
%\begin{definition} 
%The subgradient of a convex function $F : X \rightarrow (-\infty,\infty]$ is the operator $\partial F$ which maps $x \in X$ to the set (possibly empty)  
%\begin{equation*}
%\partial F(x) = \{ p \in X : F(y) \geq F(x) + \langle p, y- x\rangle, \quad \forall y \in X \}.
%\end{equation*}
%\end{definition} 
%We remark that $\partial F(x)$ is the set of supporting slopes at $x$. If $F$ is differentiable at $x$, then $\partial F(x) = \{\nabla F(x)\}$. We are in a position to prove a simple, but important lemma \cite{kakade2009duality}.
%\begin{proposition} 
%For any $F$ in $\Gamma_0(X)$, $p \in \partial F(x)$ if and %only if $x \in \partial F^*(p)$. 
%\begin{equation} 
%\langle p, x \rangle - F(x) = F^*(p)
%\end{equation} 
%\end{proposition} 
%\begin{proof} 
%We note that for any $p, x$, it holds true that
%\begin{equation} 
%\langle p, x\rangle \leq F(x) + F^*(p). 
%\end{equation}
%But, if $p \in \partial F(x)$, then it means 
%\begin{equation} 
%\langle p, x\rangle - F(x) \geq \langle p, y \rangle - F(y) %\quad \forall y \in X. 
%\end{equation} 
%Therefore, 
%\begin{equation}
%\langle p, x \rangle -F(x) \geq F^*(p). 
%\end{equation}
%This means $x \in \partial F^*(p)$. This completes the %proof. 
%\end{proof} 
%\begin{remark} 
%The result shows that in fact, 
\begin{equation}
\nabla F^* ( \nabla F (X)) = X \quad \mbox{ and } \quad \nabla F (\nabla F^*(S)) = S.
\end{equation}
%
%\end{remark} 
%
%We note the following lemma
%\begin{lemma}
%If $F$ is $\lambda-$strongly convex, then so is $L_r - %\frac{r}{2}\|Kz-X\|^2$ as a function of $X$. 
%\end{lemma}
%
%
% \begin{remark}
% For a convex function, we may simply drop the absolute %sign in \eqref{L-smooth2} since it is always greater than %or equal to 0.
% \end{remark}
 %The Bregman divergence of a differentiable function $F : \Reals{N_x} \rightarrow \Reals{}$ is defined by 
%\begin{equation}
%D_F(x,y) := F(x) - F(y) - \langle \nabla F(y), x-y\rangle, %\quad \forall x, y \in \Reals{N_x}. 
%\end{equation}
%\begin{equation}
%D_F(x,y) + D_F(y,x) := \langle \nabla F(x) - \nabla F(y), %x-y\rangle, \quad \forall x, y \in \Reals{d}. 
%\end{equation}
%For an $L_F-$smooth and $\lambda_F-$strongly convex function $F: \Reals{N_x} \rightarrow \Reals{}$, we have 
%\begin{equation} 
%\frac{\lambda_F}{2} \|x - y\|^2 \leq D_F(x,y) \leq %\frac{L_F}{2} \|x - y\|^2, \quad \forall x, y \in \Reals{d}
%\end{equation} 
%and 
%\begin{equation} 
%\frac{1}{2L_F} \|\nabla F(x) - \nabla F(y)\|^2 \leq D_F(x,y) \leq \frac{1}{2\lambda_F} \|\nabla F(x) - \nabla F(y)\|^2, \quad \forall x, y \in \Reals{d}. 
%\end{equation} 
We have only assumed that $F$ is $L_F-$smooth and thus it may not be twice differentiable. On the other hand, its Hessian is well-defined almost everywhere due to Rademacher theorem \cite{o2006metric}. $\lambda_F$ convexity and $L_F$ smoothness is equivalent to the fact that the Hessian of $F$ has the following property: 
\begin{equation}
\lambda_F I \leq \mathcal{H}_F \leq L_F I.  
\end{equation}
The mean value theorem will be useful in our analysis. Thus, we shall invoke this in many different places. On the other hand, $F$ is not sufficiently smooth, i.e., not twice continuously differentiable, then, we shall use the smoothed version of $F$, denoted by $F_\delta$, which is obtained by the mollifier applied to $F$. Namely,  
\begin{equation}
F_\delta = F \star e_\delta,  
\end{equation}
where $e_\delta$ is the standard mollifying function \cite{ghomi2002problem}. Note that it will be only used for analysis. Note that it holds true that \begin{equation}
\partial_j F_\delta(x) := \int_{\Reals{N_x}} \partial_j F(x - y) e_\delta(y) \, dy, \quad \forall j = 1,\cdots,N_x. 
\end{equation}
In practice, it will not be needed at all. We shall list a couple of important fact about $F_\delta$. 
\begin{lemma}
Let $F$ be $\lambda_F-$strongly convex and $L_F-$smooth. Then, $F_\delta$ satisfies the following three properties: 
\begin{enumerate} 
\item[(a)] $F_\delta$ is convex 
\item[(b)] $F_\delta$ inherits $\lambda_F-$strong convexity as well as $L_F-$smoothness independent of $\delta$. 
\item[(c)] $\nabla F_\delta$ converges to $\nabla F$ as $\delta \rightarrow 0$. 
\end{enumerate}
\end{lemma} 
As a consequence of the lemma, we have that $H_{F_\delta}$ is symmetric positive definite with the following property that for all $\delta > 0$, we have 
\begin{equation}\label{hessian} 
\lambda_F \leq \min \left \{ \sigma (\mathcal{H}_{F_\delta}(x)) : x \in \Reals{N_x} \right \}
\quad \mbox{ and } \quad \max \left \{ \sigma(\mathcal{H}_{F_\delta}(x)) : x \in \Reals{N_x} \right \} \leq L_F.
\end{equation} 
Note that it is well-known that if $F$ is $\lambda_F-$strongly convex and $L_F$ smooth, then for $\gamma \leq 1/L_F$, the functional $G(x) = \|x\|^2/2 - \gamma F(x)$ is $1 - \gamma \lambda_F$ smooth and $1 - \gamma L_F$ strongly convex. We shall demonstrate the usefulness of the mollifier to prove some important fact, which will be used frequently later in this paper. 
\begin{lemma}\label{lemmaGD}
Let $w_k = x_k - \omega \nabla F(x_k)$ and $w = x - \omega \nabla F (x)$, where $F$ is $\lambda_F$-strongly convex and $L_F$ smooth. Then it holds that if $\omega$ is such that $0 < \omega \leq 1/L_F$, then it holds:  
\begin{equation}
\|w_k - w\| \leq (1 - \omega \lambda_F) \|x_k - x\|.  
\end{equation}
On the other hand, if $\omega = \frac{2}{\lambda_F + L_F}$, then we have  
\begin{equation}
\|w_k - w\| \leq \frac{\kappa(F) - 1}{\kappa(F) + 1} \|x_k - x\|,  
\end{equation}
where $\kappa(F) = \frac{L_F}{\lambda_F}$. 
\end{lemma}
\begin{proof}
Since $F$ is $\lambda_F-$strongly convex and $L_F$ smooth, we have that
\begin{equation}
\lambda_F \|x - y\| \leq \|\nabla F(x) - \nabla F(y)\| \leq L_F \|x - y\|, \quad \forall x, y \in \Reals{N_x}.  
\end{equation}
We consider $F_\delta$ and define $w_\delta = x - \omega \nabla F_\delta(x)$ and $w_{k,\delta} = x_k - \omega \nabla F_\delta (x_k)$. We then see that 
\begin{eqnarray*} 
\|w_{k,\delta} - w_\delta\| &=& \|x_k - x - \omega (\nabla F_\delta(x_k) - \nabla F_\delta(x)) \| \\ 
&=& \|(I - \omega \nabla F_\delta)(x_k) - (I - \omega \nabla F_\delta)(x)\| \\
&\leq& \sup_{\xi} \|I - \omega \mathcal{H}_{F_\delta}(\xi)\|\|x_k - x\| \leq \frac{\kappa(F_\delta) - 1}{\kappa(F_\delta) + 1} \|x_k - x\| \\
&\leq& \frac{\kappa(F) - 1}{\kappa(F) + 1} \|x_k - x\|.
\end{eqnarray*}
By taking limit $\delta \rightarrow 0$, we complete the first part of the result. We have also used the fact that the following function when $t \geq 1$, takes the maximum when $t$ is the largest, i.e.,   
\begin{eqnarray}
\frac{L_F}{\lambda_F} = {\rm arg}\max_{t \geq 1} \left \{\frac{t - 1}{t + 1} \right \}. 
\end{eqnarray}
The first result can be done similarly, and easier. Thus, we skip the proof. This completes the proof. 
\end{proof}
%\begin{remark}
%Such a result is interesting since it looks new in the %convex optimization. 
%\end{remark}
%\begin{lemma}
%Assume $G(x) = \frac{1}{2}\| x\|^2 - \gamma F(x) $, where $\gamma > 0$ and $F$ is $\lambda$-strongly convex, and $L$-smooth, we have for $\gamma < \frac{1}{L}$ , the function $G$ is $\lambda_G$-strongly convex and $L_G$-smooth, with $\lambda_G = 1 - \gamma L$, $L_G = 1- \gamma \lambda $.
%\end{lemma}
%
%\begin{proof}
%Denote $h(x) = \frac{1}{2}\|x \|^2$.
%
%We first show $G(x)$ is $\lambda_G$-strongly convex. 
%By strong convexity of $h(x)$: 
%\begin{equation} \label{h-convexity}
%    h(x) - h(y) - \langle \nabla h(y), x-y \rangle \geq %\frac{1}{2} \| x - y\|^2 
%\end{equation}
%
%By $L$-smoothness of $F(x)$, we have
%\begin{equation}\label{F-L-smooth}
%    F(x) - F(y) - \langle \nabla F(y), x -y\rangle \leq %\frac{L}{2} \|x- y \|^2 
%\end{equation}
%Multiplying \eqref{F-L-smooth} by $-\gamma$ and adding it to %\eqref{h-convexity}, we obtain 
%\begin{equation}
%    G(x) - G(y) - \langle \nabla G(y), x-y \rangle \geq %\frac{1 - \gamma L }{2} \|x- y \|^2.
%\end{equation}
%Therefore, for $\gamma < \frac{1}{L}$, $G$ is %$\lambda_G$-strongly convex, with  $\lambda_G = 1 - \gamma L$. %
%
%Now we show $G(x)$ is also $L_G$-smooth.
%
%By smoothness of $h(x)$: 
%\begin{equation} \label{h-smooth}
%    h(x) - h(y) - \langle \nabla h(y), x-y \rangle \leq %\frac{1}{2} \| x - y\|^2 
%\end{equation}
%
%By $\lambda$-strong convexity of $F(x)$, we have
%\begin{equation}\label{F-convexity}
%    F(x) - F(y) - \langle \nabla F(y), x -y\rangle \geq %\frac{\lambda}{2} \|x- y \|^2 
%\end{equation}
%Multiplying \eqref{F-convexity} by $-\gamma$ and adding it to %\eqref{h-smooth}, we obtain 
%\begin{equation}
%    G(x) - G(y) - \langle \nabla G(y), x-y \rangle \leq %\frac{1 - \gamma \lambda }{2} \|x- y \|^2.
%\end{equation}
%Since $G(x)$ is also convex, for $\gamma < \frac{1}{L} $, we %immediately have that $G$ is $L_G$-smooth, with  $L_G = 1 - %\gamma \lambda$. 
%
%\end{proof}
\begin{comment} 
\section{Exact Uzawa Method for Two Blocks $(X,z)$ and $H$} 

We also look at the convergence analysis for solving $X_{k+1}$ and $z_{k+1}$ exactly in a coupled way. 
\begin{algorithm}
\caption{Exact Uzawa Algorithm}\label{algADMM2} 
ADMM updates are as follows. 
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,K-1$}
    \State{$X_{k+1}$ and $t_{k+1}$ update: 
    \begin{equation} 
    \nabla F(X_{k+1}) - H_k - r (Kz_{k+1} - X_{k+1}) = 0,
    \end{equation} 
    \begin{equation} 
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation}
        H_{k+1} - H_k - r (K z_{k+1} - X_{k+1}) = 0. 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Assume that $K^TH_0 = 0$. Then Algorithm \ref{algADMM2} produces $Y_k = [\overline{z_k}; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [\overline{z_*}; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|H_{k+1} - H_* \|^2 \leq \frac{1}{1+\delta} \| H_{k} - H_*\|^2,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|H_{k} - H_* \|^2 
\end{equation}
\end{theorem}
\begin{proof}
We have
\begin{align}
\nabla F(X_{k+1}) - \nabla F(X_*) &= H_{k+1} - H_* \label{gradF-H-relation} \\
r M_1 (X_{k+1} - X_*) &= - H_{k+1} + H_k  \\
M_2 (X_{k+1} - X_*) &= K(z_{k+1} - z_*)   
\end{align}

By strong convexity, we have 
\begin{eqnarray*}
\lambda \| X_{k+1} - X_* \|^2 &\leq& \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*)\rangle \\
&\leq& \langle X_{k+1} - X_*, H_{k+1} - H_* \rangle = \langle X_{k+1} - X_*, M_1(H_{k+1} - H_*) \rangle \\
&=&  \langle M_1 (X_{k+1} - X_*), H_{k+1} - H_* \rangle  \\
&=& \frac{1}{r} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle \\
&=& \frac{1}{2r} \big(  \| H_k - H_* \|^2- \|H_{k} -H_{k+1} \|^2 - \| H_{k+1} - H_*\|^2  \big),
\end{eqnarray*}
which immediately leads to 
\begin{equation}
     \| X_{k+1} - X_* \|^2 \leq \frac{1}{2\lambda r} \|H_k - H_* \|^2. 
\end{equation}

Rearranging, we have
\begin{equation}
    2 r \lambda \| X_{k+1} -X_* \|^2 + \|H_{k} -H_{k+1}\|^2 + \|H_{k+1} - H_* \|^2 \leq  \|H_k - H_* \|^2.
\end{equation}
Now it suffices to show for some $\delta > 0$, we have 
\begin{equation}\label{claim}
\delta \|H_{k+1} - H_* \|^2 \leq 2 \lambda r  \|X_{k+1} - X_* \|^2 + \| H_k - H_{k+1}\|^2,
\end{equation}
which will imply
\begin{equation}
    \| H_{k+1} - H_* \|^2 \leq \frac{1}{1 + \delta} \| H_{k} - H_* \|^2. 
\end{equation}
From \eqref{gradF-H-relation} and L-smoothness, we can see that 
\begin{equation}
\begin{aligned}
\delta \|H_{k+1} - H_* \|^2 & = \delta L^2 \| X_{k+1} - X_* \|^2 \\
& \leq 2 \lambda r  \|X_{k+1} - X_* \|^2 + \| H_k - H_{k+1}\|^2
\end{aligned}
\end{equation}
by making $\delta$ sufficiently small. This completes the proof.
\end{proof}

\subsection{Second Approach using Dual Operator} 
In this subsection, we develop a new framework for analyzing the Exact Uzawa algorithm for a nonlinear problem $\eqref{completesystem}$. We first define $U = [X,z]^T$, and 
\begin{equation}
G(U) = G(X,z) = F(X) + \frac{r}{2} \|Kz - X \|^2
\end{equation}
To find out the parameter for the strong convexity and the smoothness, we investigate the spectrum of the Hessian of $G$. Note that the Hessian of $G(U)$ is given as follows: 
\begin{equation}\label{hessianG}
\nabla^2G(X,z) = \begin{pmatrix}
\nabla^2 F(X)+ r I & - r K \\
-r K^T& r K^TK 
\end{pmatrix}
\end{equation}
First, we denote $\nabla^2 F(X) + rI $ as $A_r(X)$ and notice that the Hessian of $F$ has the uniformly $\lambda_F$ strongly convex and thus, $A_r(X)$ has the smallest eigenvalue which is larger than or equal to $\lambda_F + r$. On the other hand, $F$ has $L_F$ smoothness and thus, the largest eigenvalue of $A_r$ will be $L_F + r$ as well. 
%We are now in a position to obtain the spectrum of %the Hessian of $G$. Since it is symmetric, we can %apply the LU factorization and obtain that it is %spectrally equivalent to the following generally %nonlinear matrix: 
%\begin{equation}
%\begin{pmatrix}
%A_r & 0 \\
%0   & r K^T K - rK^T A_r^{-1} r K  
%\end{pmatrix} = \begin{pmatrix}
%A_r & 0 \\
%0   & r K^T ( I - r A_r^{-1}) K  
%\end{pmatrix} 
%\end{equation}
%$
%We also note that $K^TK = NI$ and further note that %for any $v$, we have 
%\begin{equation}
%\frac{(K^T (I - r A_r^{-1}) K v , v)}{(v,v)} = n %\frac{((I - r A_r^{-1}) K v , Kv)}{(K v, Kv)}.   
%\end{equation}
%Thus, we have that 
%\begin{equation}\label{lambdaG}
%\lambda_G = n \left ( 1 - \frac{r}{r + \lambda} %\right )\leq n \frac{((I - r A_r^{-1}) K v , Kv)}{(K %v, Kv)} \leq n \left ( 1 - \frac{r}{r + L} \right ) %= L_G.       
%\end{equation}
%This gives the following result: 
%\begin{lemma}\label{lemma4}
%The functional $G$ is $\lambda_G$ strongly convex while it is $L_G$ %smooth. 
%\end{lemma}
%\begin{proof}
% We have 
% \begin{equation}
%     K^T K - K^T r A_r^{-1} K = K^T (I - r A_r^{-1}) K.
% \end{equation}
% Let $c_0$, $c_1$ be the smallest and largest eigenvalues of $A_r$, we have 
% \begin{equation}
%     \rho(r A_r^{-1}) = \frac{r }{r + c_0}.
% \end{equation}
% Therefore $\lambda_{min} (I - r A_r^{-1})  = \frac{c_0}{r + c_0}$, $(I - r A_r^{-1})$ is symmetric positive definite. Since $K$ is full rank, we have $K^T (I - r A_r^{-1}) K $ is symmetric positive definite. This completes the proof.
% \end{proof}

We begin our discussion by stating a simple result:
\begin{lemma}\label{lemma5}
Given that the objective function $F(X)$ is $\lambda_F-$strongly-convex and $L_F-$smooth, then function G(U) is also $\lambda_G-$strongly-convex $L_G-$smooth, where $\lambda_G$ and $L_G$. 
\end{lemma}
\begin{proof}
By $\lambda-$strongly convexity of $F(X)$, we have that $\nabla^2 F(X)$ is symmetric positive definite for all $X$. \textcolor{red}{proof is needed for nonlinear case !}

By analyzing the Schur complement of \eqref{hessianG}, it is easy to show that the Schur complement of $\nabla^2G(U) $ is also symmetric positive definite. This implies that $\nabla^2G(U) $ is indeed symmetric positive definite for all $U$. Therefore, we must have that the function $G$ is $\lambda_G-$strongly-convex and $L_G-$smooth for some $\lambda_G$ and $L_G$. 
This completes the proof.
\end{proof}
We shall now write the equations \eqref{completesystem} using $G$ as follows: 
\begin{equation}\label{main:AB} 
\begin{pmatrix}
\nabla G(\cdot) & B^T \\
B & 0 
\end{pmatrix} \begin{pmatrix}
U \\
H
\end{pmatrix}  =  
\begin{pmatrix}
0\\
0
\end{pmatrix}，
\end{equation}
where 
\begin{equation} 
B ^T = \begin{pmatrix}
-I \\
K^T
\end{pmatrix} 
\end{equation} 
% Then the usual exact Uzawa can be formulated as follows.
% \begin{equation}
% \begin{aligned}
%     \nabla G(U_{k+1}) + B^T H_k = 0 \\
%     \nabla 
% \end{aligned}
% \end{equation}
We state the following important formulation for exact Uzawa, which is the key idea of our framework.
\begin{theorem}
The exact Uzawa iteration to solve \eqref{main:AB} is given as follows:  
\begin{equation}
\begin{aligned}
U_{k+1} &= \nabla G^{*} (-B^T H_k) \\
H_{k+1} &= H_k + \omega \left( B \nabla G^*(-B^T H_K) \right), 
\end{aligned}, 
\end{equation}
where $G^*$ is the Dual of $G$ such that it is $\lambda^*-$strongly convex with $\lambda^* = \frac{1}{L_G}$ and $L^*$-smoothness with $L^* = \frac{1}{\lambda_G}$. This iterative method converges linearly if $\omega$ is chosen sufficiently small. 
\end{theorem}
\begin{proof}
From the equation \eqref{main:AB}, we can write out solution using the Dual of $G$ operator as follows:  
\begin{subeqnarray}
U & = & \nabla G^*(-B^T H) \\
-B \nabla G^*(-B^T H) &=& 0. ，
\end{subeqnarray}
The idea of exact Uzawa is to apply the Richardson iterative method for the unknown $H$. The property of $G^*$ is from the standard result. For the convergence, we construct the error equation as follows: 
\begin{subeqnarray}
H_{k+1} - H & = & H_{k} - H + \omega \left( B \nabla G^*(-B^T H_k) - B\nabla G^*(-B^T H) \right)  \\ 
U_{k+1} - U & = & \nabla G^*(-B^T H_k) -  \nabla G^*(-B^T H)
\end{subeqnarray}
We note that since $K^T H$ and $K^T H_k = 0 $ for any $k$, we have 
\begin{equation}
B^T (H_k - H) = \begin{pmatrix}
-(H_k - H) \\ 0
\end{pmatrix}
\end{equation}
First, we note that the $\lambda_G$ strong convexity of $G^*$ gives 
\begin{subeqnarray*}
\frac{\lambda_{G^*}}{2} \|X - Y\|^2 \leq D_{G^*}(X,Y) \leq \frac{L_{G^*}}{2} \|X - Y\|^2, \quad \forall X, Y. 
\end{subeqnarray*}
Further, we have that 
\begin{subeqnarray*}
2 \lambda_{G^*} D_{G^*}(X,Y) \leq \|\nabla G^*(X) - \nabla G^*(Y)\|^2 \leq 2 L_{G^*} D_{G^*}(X,Y), \quad \forall X, Y. 
\end{subeqnarray*}
This leads that 
\begin{subeqnarray*}
\|B(\nabla G^*(X) - \nabla G^*(Y))\|^2 \leq 2 L_{G^*} \|B\|^2 D_{G^*}(X,Y), \quad \forall X, Y. 
\end{subeqnarray*}
We now observe that the following holds true: 
\begin{eqnarray*}
\|H_{k+1} - H\|^2 &=& \| H_k - H \|^2   - 2\omega \langle -B^T (H_k - H), \nabla G^* (-B^T H_k)  - \nabla G^*(- B^T H) \rangle  \\ 
&& + \omega^2 \| B \nabla G^*(-B^T H_k) - B \nabla G^* (-B^T H) \|^2   \notag \\
&=&  \| H_k - H \|^2 -2 \omega \bigg( D_{G^*}(-B^T H_k, -B^TH) + D_{G^*}(-B^T H, -B^TH_k ) \bigg) \\
&& + \omega^2 \| B \nabla G^*(-B^T H_k) - B \nabla G^* (-B^T H) \|^2   \notag 
%&=& \| H_k - H \|^2 - \omega \lambda^* \| B^T (H_k - H) \|^2 \notag \\
%&&  \textcolor{red}{- 2\omega \left[ D_{G^*}(-B^TH, -B^T H_k) - \frac{\omega}{2} \|B (\nabla G^* (-B^T H_k) - \nabla G^* (-B^T H) ) \|^2 \right] }
%\\
%&& \leq  (1 - \omega \lambda^*) \|H_k -H \|^2, 
\end{eqnarray*}
The right hand side of the above identity can be split into two parts as follows: with $X = -B^T H_k$ and $Y = -B^TH$, 
\begin{eqnarray*}
{\rm RHS} &:=& \|X - Y\|^2 - 2 \omega (D_{G^*}(X,Y) + D_{G^*}(Y,X)) + \omega^2 \|B ( \nabla G^*(X) - \nabla G^*(Y))\|^2 \\
&=& \left [ \|X - Y\|^2 - \omega D_{G^*}(X,Y) \right ] - \omega \left [ D_{G^*}(Y,X) - \omega \|B ( \nabla G^*(X) - \nabla G^*(Y))\|^2 \right ].
\end{eqnarray*}
We first look 
\begin{eqnarray*}
\left ( 1 - \frac{\omega L_{G^*}}{2} \right )\|X - Y\|^2  \leq \left [ \|X - Y\|^2 - \omega D_{G^*}(X,Y) \right ] \leq \left ( 1 - \frac{\omega \lambda_{G^*}}{2} \right )\|X - Y\|^2 .
\end{eqnarray*}
On the other hand, we have that 
\begin{equation}
D_{G^*}(Y,X) - \omega \|B ( \nabla G^*(X) - \nabla G^*(Y))\|^2 \geq (1 - \omega 2L_{G^*} \|B\|^2 ) D_{G^*}(Y,X)  
\end{equation}
Thus, by choosing $\omega$ such that  
\begin{equation}
1 - \frac{\omega \lambda_{G^*}}{2} > 0, \quad \mbox{ and } \quad 1 - 2 \omega L_{G^*} \|B\|^2 > 0,  
\end{equation}
Now, for $U$ error, we use the Bregman distance:  
\begin{eqnarray*}
\lambda_G\| U - U_{k+1} \|^2 &\leq& \langle U - U_{k+1}, \nabla G(U) - \nabla G(U_{k+1}) \rangle = \langle U - U_{k+1}, -B^T H - (-B^T H_{k}) \rangle \\
&=& \langle \nabla G^*(-B^TH) - \nabla G^*(-B^T H_{k}), -B^T H - (-B^T H_{k}) \rangle \\
&\leq& \frac{1}{\lambda_G} \|B^T (H - H_{k})\|^2 = \frac{1}{\lambda_G} \|H - H_k\|^2. 
\end{eqnarray*}
This leads to linear convergence since 
\begin{equation}
    \|U - U_{k+1} \| \leq \frac{1}{\lambda_G} \| H - H_k \|.
\end{equation}
This completes the proof. 
\end{proof} 

%\begin{remark}
% $ \langle U - U_{k+1}, \nabla G(U) - \nabla G(U_{k+1}) \rangle$ is the A-inner product in the linear case, from which we can obtain 
% \begin{equation}
%     \| U - U_{k+1} \|_A \leq \frac{1}{\sqrt{\lambda_G } } \| H - H_k \|.
% \end{equation}
%\end{remark}
\end{comment} 

\subsubsection{Augmented Lagrangian and Inexact Uzawa method}
In this section, we shall now present the general inexact Uzawa iterative framework of the Algorithm \ref{alg:SCAFFOLD}. We recall that the Lagrangian is given as follows for $r > 0$,  
\begin{equation}
L_r(X,z,H) = F(X) + \langle H, K z - X \rangle + \frac{r}{2} \|Kz - X\|^2. 
\end{equation}
The method then reads as follows: 
\begin{subeqnarray}
X_{k+1} &=& \mathop{ \arg \min }_{X} L_r({X,z_{k}, H_k}) \\
z_{k+1} &=& \mathop{ \arg \min }_{z} L_r({X_{k+1},z, H_k}) \\
H_{k+1} &=& H_k + \omega (K z_{k+1} - X_{k+1}).      
\end{subeqnarray}
Generally, $X_{k+1}$ can also be defined approximately:
\begin{subeqnarray}
X_{k+1} \approx \mathop{ \arg \min }_{X} L_r(X,z_{k},H_k),
\end{subeqnarray}
while no changes are made for $z_{k+1}$ and $H_{k+1}$. This shall induce different solution technique. To describe the general case in one stroke. We first observe that $X_{k+1}$ obtained from \eqref{Xupdate} can be characterized as follows: 
\begin{equation}
(A + rI)(X_{k+1}) = \nabla F(X_{k+1}) + r X_{k+1} = H_k + rKz_k
\end{equation}
or equivalently, we have 
\begin{equation}
X_{k+1} = A_r^*(H_k + rKz_k),
\end{equation}
where $A_r = A + rI = \nabla G$ with $G = \frac{r}{2}\|\cdot\|^2 + F(\cdot)$ and $A_r^* = \nabla G^*.$ 
We shall denote $D_r^*$ by the approximate $A_r^*$ such that it satisfies the following relationship:
\begin{assump}\label{main:ass} 
The approximate inverse of $A_r$ satisfies the following identity: 
\begin{equation}
D_r^*(H_* + rKz_*) = X_*. 
\end{equation}
\end{assump} 
We remark that for the operator $A_r$, it holds true that 
\begin{equation}
A_r(X_*) = \nabla F(X_*) + r X_* = H_* + rKz_*. 
\end{equation}
Thus, $A_r^*$ satisfies the Assumption \ref{main:ass}. For the inexact solve, to achieve the Assumption \ref{main:ass}, it is notable that the initial iterate has to be set as $rKz_*$. Therefore, we shall assume that when $D_r^*$ requires the initial iterate, what we mean by $D_r^*(H_k + rKz_k)$ is that it uses $X_k = Kz_k$ as a start. Under this condition on the initial iterate, we note that the gradient descent method that is discussed in \S \ref{gdn} also satisfies the Assumption \ref{main:ass}. The general version of the Algorithm \ref{alg:SCAFFOLD} can be written as the following Algorithm \ref{algADMM1}.
\begin{algorithm}
\caption{Inexact Uzawa formulation of FL}\label{algADMMx} 
Given $H_0$ such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,K-1$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$),  
    \begin{equation} \label{Xupdatell}
    X_{k+1} = D_r^*(H_k + r Kz_k),
\end{equation} }
    \State{$z_{t+1}$ update: 
        \begin{equation} \label{zupdatezzz}
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} \label{Hupdatexx}
        H_{k+1} = H_k + \omega (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}

We remark that the Algorithm \ref{alg:SCAFFOLD} can, therefore, be interpreted as an inexact Uzawa method. In particular, if $D_r = A_r$, then we obtain the  Uzawa method with Gauss-Seidel block solver. Before we analyze the convergence of the algorithm, we shall list important facts about the algorithm. 
\begin{lemma}
If $H_0$ is given so that $K^TH_0$, then the Algorithm \ref{algADMM1} produces $H_k$ such that $K^TH_k = 0$ for all $k = 1,2,\cdots$. 
\end{lemma}
\begin{proof} 
Multiplying $K^T$ to \eqref{Hupdate} and adding it to \eqref{zupdate}, we have $K^T H_{k+1} = 0$ for all $k = 0,1,\cdots$. 
This completes the proof. 
\end{proof} 

We now define $P_Z = K(K^TK )^{-1} K^T$ and $Q_Z = I - P_Z$. First we note that $P_Z$ has the spectral radius less than or equal to one. 
\begin{lemma}
Let $P_Z = K(K^T K)^{-1}K^T$. Then it holds that 
\begin{equation}
\|P_Z X\| \leq \|X\|, \quad \forall X \quad \mbox{ or equivalently } \quad \lambda_{max} (P_Z) = 1. 
\end{equation}
\end{lemma}
\begin{proof}
We consider the Rayleigh Quotient. Using the Jenssen's inequality, we obtain that 
\begin{eqnarray*} 
\sup_{\|X\| \neq 0} \frac{\langle K(K^T K)^{-1}K^T X, X \rangle  }{\langle X,X \rangle} = \sup_{\|X\| \neq 0}  \frac{\frac{1}{N} \left \langle  \sum_{i = 1}^{N} X_i, \sum_{i = 1}^{N} X_i \right \rangle }{ \sum_{i = 1}^{N} \|X_i\|^2 }\leq 1. 
%&=& \sup_{\|X\| \neq 0}  \frac{ \frac{1}{N} \left \| \sum_{i=1}^{N} X_i \right \|^2 }{ \sum_{i = 1}^{N} \| X_i\|^2 } \\
%&\leq& \sup_{\|X\| \neq 0}  \frac{ \frac{1}{N} \left ( \sum_{i=1}^{N} \|X_i\| \right )^2 }{ \sum_{i = 1}^{N} \|X_i\|^2 } \leq 1.  
\end{eqnarray*}
This completes the proof. 
\end{proof}
Secondly, we note that both $P_Z$ and $Q_Z$ are symmetric. Furthermore, one can see for all $k = 0,1,\cdots$, 
\begin{subeqnarray}\label{invariantoperator}
P_Z H_k &=& 0    \\
P_Z H_* &=& 0,   \\
Q_Z H_k &=& H_k, \\
Q_Z H_* &=& H_*, \\
P_Z (Kz) &=& Kz, \\
Q_Z (Kz) &=& 0. 
\end{subeqnarray}
In particular, we have that   
\begin{lemma}
The following holds: 
\begin{equation} 
\langle Kz_* - Kz_i, H_* - H_j \rangle = 0, \quad \forall i, j \geq 0. 
\end{equation}
and 
\begin{equation}
\langle X_{k+1} - X_*, H_{k+1} - H_* \rangle  \\
= \frac{1}{\omega} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle 
\end{equation} 
\end{lemma}
\begin{proof}
Note that 
\begin{eqnarray*} 
\langle Kz_* - Kz_i, H_* - H_j \rangle = \langle Kz_* - Kz_i, Q_Z(H_* - H_j) \rangle = \langle Q_Z(Kz_* - Kz_i), H_* - H_j \rangle = 0.   
\end{eqnarray*} 
We now notice that since $H_{k+1} - H_k = -\omega Q_ZX_{k+1}$ and $Q_ZX_* = 0.$ Thus, we have $H_{k+1} - H_k = -\omega Q_Z(X_{k+1} - X_*).$ This gives 
\begin{eqnarray*} 
\langle X_{k+1} - X_*, H_{k+1} - H_* \rangle &=& \langle Q_Z(X_{k+1} - X_*), H_{k+1} - H_* \rangle \\
&=& \frac{1}{\omega} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle.    
\end{eqnarray*} 
This completes the proof. 
\end{proof}
Lastly, we show the nonexpansiveness as stated below. More precisely, we have the following result: 
\begin{lemma}
The following nonexpansiveness holds true: 
\begin{equation}
\|P_ZX\|^2 + \|Q_ZX\|^2 = \| X\|^2, \quad \forall X \in \Reals{N_x}. 
\end{equation}
\end{lemma}

\begin{proof}
We observe that 
\begin{eqnarray*}
\|P_Z X\|^2 + \|Q_Z X \|^2 &=& \|P_Z X \|^2 + \|P_Z X \|^2 - 2 \langle P_Z X, X \rangle + \|X\|^2 \\
&=& \|P_Z X\|^2 + \|P_Z X \|^2 - 2 \langle P_ZX, P_ZX \rangle + \|X\|^2 \\
&=& \| X\|^2
\end{eqnarray*}
This completes the proof. 
\end{proof}
We shall use the standard notation that for all $k \geq 0$ to discuss the convergence:  
\begin{eqnarray*}
E_k^X &=& X_* - X_k \\
E_k^Z &=& Kz_* - Kz_k \\ 
E_k^H &=& H_* - H_k. 
\end{eqnarray*}

In the following two sections, we shall present convergence analysis. The first section will deal with the Gauss-Seidel method in which $D_r = A_r$. The second section shall deal with $D_r^*$ being the $n-$step Gradient Descent method. Algorithmic details are presented in each subsection.  

In passing to each subsection, we shall present an important result from the non-expansiveness: 
\begin{lemma}\label{main:lem1} 
The Algorithm \ref{algADMM1} produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \right \|^2. 
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\end{lemma}
\begin{proof} 
The Algorithm \ref{algADMM1} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& D_r^* (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $D_r^*$ is an approximate of $A_r^*$, the Fenchel-dual conjugate of $A_r$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& D_r^{*} (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T D_r^* (H_k + r K z_k)) = P_Z [D_r^* (H_k + r K z_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{*} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{*}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH}, and using the orthogonality, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2. 
\end{eqnarray*}
This completes the proof. 

% It is important to observe that 
% \begin{equation}
% X_* = A_r^*(H_* + rKz_*) = D_r^*(H_* + rKz_*). 
% \end{equation}

% Therefore, we see that
% \begin{eqnarray*}
% && \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% && \qquad = \|H_* - H_k - \omega (D_r^* (H_* + rKz_*) - D_r^*(H_k + rKz_*))\|. 
% \end{eqnarray*}
% The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
% \begin{eqnarray*}
% -\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] \right ). 
% \end{eqnarray*}
% Lastly, for $H$, we have 
% \begin{eqnarray*}
% E_{k+1}^H &=& E_k^H + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
% &=& E_k^H - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
% &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
% && \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
% &=& E_k^H - \omega Q_Z (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \\ 
% &=& Q_Z [E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))] 
% \end{eqnarray*}
% First, we shall observe that   
% \begin{eqnarray*}
% \omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left( P_Z [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right )\| \\
% &=& \|P_Z [(H_* - H_k) - \omega \left( [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right ) ] \|
% \end{eqnarray*}
% Next, we observe that there is a close relationship between $E_k^Z$ and $E_k^H$: 
% \begin{eqnarray*}
% \|H_{*} - H_{k+1}\| = \|Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))]\|. 
% \end{eqnarray*}
% Therefore, we have that by the nonexpansiveness, 
% \begin{eqnarray*}
% \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 %\\   
% %&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% %&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
% \end{eqnarray*}
% This completes the proof. 
\end{proof}
%\begin{remark}
%There is an alternative method to obtain the same result. This is to use the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$. Namely, we have that
%\begin{eqnarray*}
%E_{k+1}^H &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
%&& \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
%- \omega E_{k+1}^Z &=& -\omega \left ( P_Z [ %A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK %z_k) ] \right ). 
%\end{eqnarray*}
%Thus, we have that 
%\begin{eqnarray*}
%E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - %\omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} %(H_k + r K z_k). 
%\end{eqnarray*}
%Now, we have that 
%\begin{eqnarray*}
%\left \|E_{k+1}^H - \omega E_{k+1}^Z \right %\|^2 &=& \left \|E_{k+1}^H \right \|^2 + %\omega^2 \left \|E_{k+1}^Z \right \|^2 \\
%&=& \left \|E_k^H - \omega \left [ A_r^{*} %(H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \right ] \right \|^2. 
%\end{eqnarray*}
%The first equality is due to the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$.  
%\end{remark}

In the following two sections, we shall present the convergence of the Algorithm \ref{algADMM1}. This section consists of two parts. One part is the convergence when $D_r^* = A_r^*$ and the other part is the convergence when $D_r^*$ is the $n-$step GD method. 

\subsubsection{Linear Convergence of Algorithm \ref{algADMM1} for $D_r = A_r$}

In this section, we shall establish the linear convergence of the exact Uzawa method with $D_r = A_r$.
\begin{comment} 
\begin{lemma}
The following holds true: for all $Y, Y_k$ in $\Reals{N_x}$, 
\begin{eqnarray}
\|A_r^*(Y) - A_r^*(Y_k) \| &\leq& \frac{1}{r + \lambda_F} \|Y- Y_k\|, \\ 
\|(I - \omega A_r^*) (Y) - (I - \omega A_r^*) (Y_k)   \| &\leq& \left(1 - \frac{\omega}{r + L_F} \right) \|Y -Y_k \|
\end{eqnarray}
\end{lemma}
\begin{proof}
    Note that $A_r^*(Y)$ corresponds to the gradient of the function $F_r^*(Y)$ that is the dual of $F_r(X) = F(X) + \frac{r}{2} \| X\|^2$. 
    $F_r^*$ is $\frac{1}{r +L_F}$-strongly convex and $\frac{1}{r + \lambda_F}$-smooth since $F_r$ is $(r + \lambda)$-strongly convex and $(r + L_F)$-smooth.
    Therefore, we have 
    \begin{equation}
        \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|. 
    \end{equation}

For $ (I -  \omega A_r^*)(Y)$, we have
 that the 
 \begin{equation}
 \begin{aligned}
    \|  I - \omega \nabla^2F^*_r(Y) \| 
 = \rho ( I - \omega \nabla^2F^*_r(Y)) & \leq \max \left\{|1 - \omega \lambda_{min}(\nabla^2 F_r^*(Y))|, | 1-\omega \lambda_{max} \nabla^2 F_r^*(Y)| \right\} \\
 & \leq \max \left\{  |1 - \frac{\omega}{r + L_F} |, | 1-\frac{\omega}{r + \lambda_F}| \right\}
 \end{aligned}
 \end{equation}
 where $\omega < \frac{2}{ \frac{1}{r + \lambda_F}}$ for any $Y$. 
 For optimal $\omega$ is $\frac{2}{ \frac{\omega}{r + \lambda_F}+ \frac{\omega}{r + L_F}}$, which leads to 
 \begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa + 1} \|Y - Y_k \|,
\end{equation}
where $\kappa = \frac{r+L_F}{r+\lambda_F}$. 

In other analysis where we restrict $\omega \leq \frac{1}{ \frac{1}{r + \lambda_F} }$, we have the following optimal bound
\begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa} \|Y - Y_k \|.
\end{equation}
\end{proof}
\begin{remark}
    This Hessian argument always requires $L$-smoothness of the objective thanks to the mollifier argument. See Lemma \ref{lemmaGD}.
\end{remark}
\end{comment} 

% \begin{proof}
% Let $X = A_r^*(Y)$, $X_k = A_r^*(Y_k)$, that is $Y  = A_r(X)$, $Y_k = A_r(X)$
% \begin{eqnarray*}
% (r+\lambda) \|X - X_{k}\|^2 &\leq& \langle X - X_{k+1}, A_r(X) - A_r(X_{k+1}) \rangle \\
% &=& \langle X - X_k, Y - Y_k \rangle \\ 
% &\leq& \|X - X_k\|  \|Y - Y_k\|. 
% \end{eqnarray*}
% Hence, 
% \begin{eqnarray*}
% \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|.   
% \end{eqnarray*}
% This completes the proof.
% \end{proof}

\begin{theorem}\label{thm:GS}
The Algorithm \ref{algADMM1} with GS as local solve, and $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ has the convergence rate given as follows: 
\begin{eqnarray}\label{gsrate}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \rho^2_{GS}(r,L_F,\lambda_F) \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ), 
\end{eqnarray}
where with $\kappa(G) = \frac{r+L_F}{r + \lambda_F}$, 
\begin{equation} 
\rho^2_{GS}(r,L_F,\lambda_F) =  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 = \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 
\end{equation} 
We also have that 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ). 
\end{eqnarray*}
Furthermore, there exists an interval around $0$ such that if $r \in [0,r_{\rm opt}]$, then the convergence can be achieved. For larger $r > L_F^2/(8\lambda_F)$, the convergence is also guaranteed for any $L_F$ and $\lambda_F$. 
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM1} produces iterates given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^* (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $A_r^*$ is the Fenchel-dual conjugate of $A_r$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. Now due to Lemma \ref{main:lem1}, we have that with $D_r = A_r$,  
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 %\\   
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
%\begin{comment} 
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{*} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{*}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ]. 
\end{eqnarray*}
The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] \right ). 
\end{eqnarray*}
Lastly, for $H$, we have 
\begin{eqnarray*}
H_{*} - H_{k+1} &=& H_* - H_k + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
&=& H_* - H_k - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
&=& H_* - H_k - \omega [ A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
&& - P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] ] \\
&=& H_* - H_k - \omega Q_Z (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)) \\ 
&=& Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))] 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
H_{*} - H_{k+1} - \omega (Kz_* - K z_{k+1}) = H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)). 
\end{eqnarray*}
%\end{comment} 
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^*(H_k + rKz_*) - A_r^*(H_k + rKz_k). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^*} = 1/(r + L_F)$ and $L_{G^*} = 1/(r + \lambda_F)$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{r + L_F} + \frac{1}{r + \lambda_F}} = \frac{2 (r + \lambda_F)(r + L_F)}{2r + L_F + \lambda_F}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F} \|E_k^H\|\|E_k^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{r + \lambda_F} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^*} + L_{G^*})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^*(H_k + r Kz_*) - A_r^{*} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r + \lambda_F} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}
\end{comment} 
This provides the convergence rate. Finally, we notice that for all $r \geq 0$, 
\begin{eqnarray*}
\frac{r^2}{\omega^2} = \frac{r^2 (2r + L_F + \lambda_F)^2}{4(r + \lambda_F)^2(r + L_F)^2} < 1. 
\end{eqnarray*}
Thus, we obtain that due to the orthogonality, 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 &=& \|A_r^*(H_* + rKz_*) - A_r^*(H_k + r K z_k) \|^2 \\
&\leq& \frac{1}{(r + \lambda_F)^2}\|E_k^H - r E_k^Z\|^2 = \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + r^2 \|E_k^Z\|^2 \right ) \\
&=& \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \frac{r^2}{\omega^2} \|E_k^Z\|_\omega ^2 \right ) \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) 
\end{eqnarray*}
We now discuss the convergence factor denoted by $\rho_{GS}^2$ and given by 
{\small{\begin{eqnarray*}
f(r) &=& \rho^2_{GS}(r, L_F, \lambda_F) = \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 \\
&=& \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 \\
&=& \frac{4r^4 + 4r^3 ( L_F + \lambda_F) + r^2 ((L_F + \lambda_F)^2 + (L_F-\lambda_F)^2) + 2r \lambda_F(L_F - \lambda_F)^2 + \lambda_F^2(L_F - \lambda_F)^2}{4r^4 + 4r^3 ( L_F + 3\lambda_F) + r^2 ((L_F + \lambda_F)^2 + 8\lambda_F (L_F+\lambda_F) + 4\lambda_F^2) + r (4\lambda_F^2(L_F + \lambda_F) + 2\lambda_F(L_F + \lambda_F)^2) + \lambda_F^2(L_F - \lambda_F)^2}. 
\end{eqnarray*}}}
Clearly, if $r = 0$, then $f(r) = \left ( \frac{L_F - \lambda_F}{L_F + \lambda_F} \right )^2.$ Thus, the convergence is guaranteed with the rate similar to the Gradient descent method. On the other hand, for all $L_F, \lambda_F \geq 0$, we have the convergence can be achieved under the sufficient condition that
\begin{equation}
r > \frac{L_F^2}{8 \lambda_F}.
\end{equation}
We also note that a simple calculation shows that the derivative of $f(r)$ is given as follows: 
\begin{equation}
f'(r) = \frac{-4(L_F - \lambda_F)^2}{(2r + L_F + \lambda_F)^3} + \frac{2r \lambda_F}{(r + \lambda_F)^3}. 
\end{equation}
Therefore, for small $r$, it takes the negative sign. Thus, but it changes its sign for larger $r$ and continues to be positive. Thus, there exists a single critical point, which gives the optimal $r_{\rm opt}$. On the other hand, the convergence rate is smaller than one at $r = 0$. Thus, the optimal $r_{\rm opt}$ is such that 
\begin{equation}
f'(r_{\rm opt}) = 0. 
\end{equation} 
%Readers refer to the Figure \ref{exam}). 
We can conclude that it converges linearly for all fixed $0 \leq r \leq r_{\rm opt}$. This completes the proof.
\end{proof} 
%\begin{figure}[h]
%\centering
%\includegraphics[width=12cm,height=6cm]{plot1.png}
%\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = %0.5$}\label{exam} 
%\end{figure}
\begin{comment} 

\begin{remark}[Convergence rate when $\omega = r$]
In our current convergence analysis, we require $\beta \in \left( 0,\frac{1}{2} \right )$. We shall make a more precise convergence analysis as follows.
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{ L_F r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{L_F }{r + L_F} \right )^2 \right ) &=& \left ( \frac{ L_F  r^{2\alpha} (r + L_F)}{(r + L_F)^2(r + \lambda_F)} + \frac{L_F^2 (r+ \lambda_F) }{(r + L_F)^2 (r + \lambda_F)} \right ). 
%&& \Longleftrightarrow ((r + L)(r + \lambda) + L r^2 ) L^2 < (r + L)^3 (r + \lambda). 
\end{eqnarray*} 
The convergence rate in $z$-error is given by 
\begin{eqnarray*}
    \left ( \frac{L_F r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) &=&    \left ( \frac{L_F }{(r^{1 - 2 \beta} + L_F/r^{2\beta})(r + \lambda_F)} + \left ( \frac{1}{1 + \lambda_F/r} \right )^2 \right )   
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, we have that 
\begin{equation}
\frac{L_F }{(r^{1 - 2 \beta} + L_F r^{-2\beta})(r + \lambda_F)} = \frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta} + \lambda_F L_F r^{-2\beta}}    
\end{equation}
and we have 
\begin{equation}
    \left ( 1 + \lambda_F/r \right )^{-2}  \leq 1 - \frac{2 \lambda_F}{r} + \frac{3 \lambda_F^2}{(1 -\varepsilon)^4 r^2},  
\end{equation}
since it holds that 
\begin{eqnarray*}
(1 + x)^{-2} &=& 1 - 2x + \frac{3}{ (1 + \xi )^4} x^2, \quad x \in (-\varepsilon,1), \quad \xi \in[-\varepsilon, x] \\ 
&\leq&   1 - 2x + \frac{3}{ (1 -\varepsilon)^4} x^2 \end{eqnarray*}
Thus, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}

\textbf{Case 2: $r \ll 1$}.
For error in $H$, the convergence rate is bounded below by $\left( \frac{L_F}{r + L_F} \right)^2$. The convergence in error $H$ deteriorates as $r \to 0$. 

For error in $z$, the convergence rate can be made arbitrarily small if $r$ is sufficiently small. 

\textbf{Therefore, we conclude that when $\omega = r$. For sufficiently large $r$, the convergence in $H$ error becomes better while the convergence in $z$ error deteriorates when $r$ becomes larger. The asymptotic rate in $z$ error is $1 - \frac{2 \lambda_F}{r}$. 
On the other hand, for small $r \ll 1$, the convergence in $H$ deteriorates while the convergence in $z$ error becomes better. }
\end{remark}

\begin{remark}[Convergence rate when $\omega = r + \lambda_F$]
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{(L_F - \textcolor{red}{\lambda_F})}{r + L_F} \right )^2 \right ) 
\end{eqnarray*} 

The convergence rate in $z$-error is given by 
\begin{eqnarray*} 
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) 
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
The analysis is similar to that when $\omega = r$.

For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F - \lambda_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}
The asymptotic convergence rate is $1 - \frac{2\lambda_F}{r}$. 

\textbf{Case 2: $r \ll 1$}.
For $H$ error, the convergence rate is bounded below by $\left( \frac{L_F - \lambda_F}{r + L_F} \right)^2 $. Asymptotically, it is $\left( \frac{L_F - \lambda_F}{L_F} \right)^2$.

For $z$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently small.
\end{remark}

\begin{remark}
    In fact, from standard asymptotic analysis, one should always choose $\omega > r$, say, $\omega = r + \lambda_F$, with sufficiently small $r$.
\end{remark}
\end{comment} 

% \begin{remark}
% We note that the larger the $r$, the convergence of $H_k$ to $H_*$ seems to be faster while the convergence of $Kz_k$ to $Kz_*$ deteriorates.% On the other hand, the larger $r$ guarantees the sufficiently near orthogonality for the cross terms.  
% \end{remark}
%\begin{remark}
%It is interesting to note that if $\omega = r + L_F$, then the cross term disappears. Thus, the error analysis reduces to the following form: 
%\begin{eqnarray*}
%\|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} &\leq& \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - K z_k\|^2_{\omega} \\ 
%&=&  \left ( \frac{r}{r + \lambda_F} \right )^2 \left ( \|H_{*} - H_{k}\|^2 + \|Kz_* - K z_k\|^2_{\omega} \right %).
%\end{eqnarray*}
%\end{remark} 

\begin{comment} 
\subsection{Analysis based on $E_k^X$ and the convexity of $F$ due to \cite{shi2014linear}} 
In this section, we shall adapt the proof originated from \cite{shi2014linear} to establish the result of linear convergence.
\begin{lemma} 
The algorithm \ref{algADMM1} produces iterates $(X_k,z_k,H_k)$ such that the following identity holds: for all $k=0,1,2\cdots$, 
\begin{eqnarray*}\label{equal form 2}
\nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &=& 0, \\
H_{k+1} - H_{k} + r \left( I - P_Z \right) X_{k+1} &=& 0,\\
K z_{k+1} - P_Z X_{k+1} &=& 0.
\end{eqnarray*}
\end{lemma} 
\begin{proof} 
First of all, we note that $K^TH = 0$ with $H = H_k$ or $H = H_*$ and also 
\begin{equation}
Kz = P_Z X. 
\end{equation}
Now, subtracting \eqref{H update} from \eqref{X update}, multiplying $K^T$ to \eqref{H update} and adding it to \eqref{z update}, we have
\begin{equation}\label{equal form 1}
    \begin{split}
        \nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &= 0, \\
        K^T H_{k+1} &= 0, \\
        H_{k+1} - H_{k}  - r (K z_{k+1} - X_{k+1}) &= 0. 
    \end{split}
\end{equation}
Multiplying the third equation in \eqref{equal form 1} by $K^T$ and using the second equation, we complete the proof. 
\end{proof}





We shall obtain a simple but important lemma: 
\begin{lemma} 
The following identity holds: 
\begin{eqnarray}
\nabla F(X_{k+1}) - \nabla F(X_*) &=& r K (z_{k} - z_{k+1}) + H_{k+1} - H_*  \label{difference1}\\
H_{k+1} - H_k &=& - r Q_Z (X_{k+1} - X_*)  \label{difference2} \\
K(z_{k+1} - z_*) &=& P_Z (X_{k+1} - X_*) \label{difference3}    
\end{eqnarray}
\end{lemma} 
\begin{proof} 
We recall the optimality condition, which can be given as follows:
\begin{subeqnarray*}
\nabla F(X_*) - H_* &=& 0 \\ 
K^T H_* &=& 0 \\
K z_* - X_* &=& 0 
\end{subeqnarray*}
However, using the property of $Q_Z$ and $P_Z$, the optimality condition implies that it holds true 
\begin{subeqnarray}\label{KKT}
0 &=& P_Z (K z_* - X_{*}) = K z_* - P_Z X_* \\ 
0 &=& Q_Z (Kz_* - X_*) = - Q_Z X_*. 
\end{subeqnarray}
Subtracting the optimality conditions \eqref{KKT} from \eqref{equal form 2}. This completes the proof. 
\end{proof} 
%\begin{proposition}
%If each $F_i$ is $\lambda_i$-strongly convex, and $L_i$-smooth, then $F(X) = \frac{1}{n}\sum_{i = 1}^n F_i(x_i)$ is $\lambda$-strongly convex and $\frac{M_f}{n}$-smooth, where $\lambda = \frac{\min_{i} m_i}{n}$, $M_f = \max_i M_i$.  
%\end{proposition}
The main theorem considers the convergence of a vector $Y$ that combines the primal variable $Kz$ and the dual variable $H$,
\begin{equation}
Y = \begin{pmatrix}
Kz \\
H
\end{pmatrix} \quad \mbox{ and } \quad C = \begin{pmatrix}
r I & 0 \\
0 & \frac{1}{r} I 
\end{pmatrix}
\end{equation}
We also define a $C-$norm on $Y = (y_1,y_2)^T$ by 
\begin{eqnarray*}
\|Y\|_C^2 &=& (CY, Y) = r \|y_1\|^2 + \frac{1}{r}\|y_2\|^2 \\ 
&=& r (Kz, Kz) + \frac{1}{r} (H, H) = (rK^TK z, z) + \frac{1}{r}(H,H).   
\end{eqnarray*}

We shall need the following lemma: 
\begin{lemma}
We have the following identity:
\begin{eqnarray*}
2 \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \|Y_{k} - Y_*\|^2 - \|Y_{k+1} - Y_* \|^2 - \|Y_{k+1} - Y_k\|^2. 
\end{eqnarray*} 
\end{lemma} 
\begin{proof} 

\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\ 
&=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\
&=&  \langle Y_{k} - Y_*, Y_{k+1} - Y_* \rangle - \| Y_{k+1} - Y_* \|^2
\end{eqnarray*} 
On the other hand, we have 
\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_k + Y_k - Y_* \rangle \\ 
&=& - \|Y_{k+1} - Y_{k}\|^2 + \langle Y_{k} - Y_{k+1}, Y_k - Y_* \rangle . 
\end{eqnarray*} 
By adding these two identities, we obtain the result. This completes the proof. 
\end{proof}
\begin{theorem}
Assume that $K^TH_0 = 0$. Then the ADMM iterations produces $Y_k = [Kz_k; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [Kz_*; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1+\delta} \| Y_{k} - Y_*\|^2_C,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C
\end{equation}
\end{theorem}
\begin{proof}
We begin with using the $\lambda-$strongly convexity condition for $F$ as follows: 
\begin{eqnarray*}
\lambda \| X_{k+1} - X_* \|^2 &\leq& \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*)\rangle \\
&\leq& \langle X_{k+1} - X_*, r K(z_k - z_{k+1}) \rangle + \langle X_{k+1} - X_*, H_{k+1} - H_* \rangle \\
&=& r \langle X_{k+1} - X_*, P_Z (K(z_k - z_{k+1}))  \rangle + \langle X_{k+1} - X_*, Q_Z(H_{k+1} - H_*) \rangle \\
&=& r \langle  P_Z (X_{k+1} - X_*) , Kz_k - Kz_{k+1}   \rangle + \langle Q_Z (X_{k+1} - X_*), H_{k+1} - H_* \rangle  \\
&=& r \langle Kz_k - Kz_{k+1}, Kz_{k+1} - Kz_*  \rangle + \frac{1}{r} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle \\
&=& (Y_{k} - Y_{k+1})^T C (Y_{k+1} - Y_*),  
\end{eqnarray*}
where 
\begin{equation}
Y_k = \begin{pmatrix} 
      Kz_k \\
      H_k 
      \end{pmatrix}, \quad Y_{k+1} = \begin{pmatrix} 
      Kz_{k+1} \\
      H_{k+1}  
      \end{pmatrix}
,\quad Y_{*} = \begin{pmatrix} 
      Kz_{*} \\
      H_{*}  
      \end{pmatrix}
\end{equation}

and the matrix 
\begin{equation}
    C = \begin{pmatrix}
r I & 0 \\
0 &  \frac{1}{r} I
\end{pmatrix}
\end{equation}
This implies
\begin{equation}
    \lambda  \| X_{k+1} - X_* \|^2 \leq  \frac{1}{2} \|Y_k -Y_* \|^2_C - \frac{1}{2}  \|Y_{k+1} - Y_* \|^2_C -  \frac{1}{2}  \| Y_k - Y_{k+1}\|^2_C. 
\end{equation}
Now, by rearranging terms, we have 
\begin{equation}\label{ineq from strong convexity}
2\lambda \| X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C + \|Y_{k+1} - Y_* \|^2_C \leq  \|Y_k -Y_* \|^2_C.
\end{equation}
This immediately, leads to 
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C.
\end{equation}
Having \eqref{ineq from strong convexity}, it suffices to show for some $\delta > 0$, we have 
\begin{equation}\label{claim}
\delta \|Y_{k+1} - Y_* \|^2_C \leq 2 \lambda \|X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C,
\end{equation}
or equivalently,
\begin{eqnarray*}\label{claimequivalence}
\delta \left( r  \|Kz_{k+1}- Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*} \|^2 \right) &\leq&  2 \lambda \|X_{k+1} - X_* \|^2 \\
&+& r \|Kz_{k}- Kz_{k+1} \|^2 + \frac{1}{r} \|H_k - H_{k+1} \|^2, 
\end{eqnarray*}
which will imply the desired inequality: 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1 +\delta} \|Y_{k} - Y_* \|^2_C.
\end{equation}
To prove the inequality \eqref{claim}, first, we observe that the following holds true:   
\begin{equation}\label{bound on z-z*}
\|Kz_{k+1} - Kz_* \|^2 \leq \|X_{k+1} -X_* \|^2.  
\end{equation}
Further, from \eqref{difference1} and using $L$-smoothness of $F$, we have  
\begin{equation}
\|H_{k+1} - H_* \| \leq r \|K z_k - K z_{k+1} \| + L \| X_{k+1} -  X_*\|
\end{equation}
This implies 
\begin{equation}\label{bound on H-H*}
\begin{split}
\|H_{k+1} - H_*\|^2 & \leq \left(r \|Kz_k - Kz_{k+1} \| + L\|X_{k+1} -  X_*\| \right)^2 \\  & \leq 2 \left( r^2 \| Kz_k - Kz_{k+1}\|^2 + L^2 \| X_{k+1} -  X_*\|^2 \right). 
\end{split}
\end{equation}
Thus, we have 
\begin{equation}
\frac{1}{r} \|H_{k+1} - H_*\|^2 \leq 2 \left( r \| Kz_k - Kz_{k+1}\|^2 + \frac{L^2}{r} \| X_{k+1} -  X_*\|^2 \right).
\end{equation}
Substituting \eqref{bound on z-z*} and \eqref{bound on H-H*} into left hand side of \eqref{claimequivalence} and rearranging, we have 
\begin{eqnarray*}
\delta \left(r\|Kz_{k+1} - Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*}\|^2 \right) &\leq& \delta \left( r + \frac{2L^2}{r} \right ) \| X_{k+1} -  X_*\|^2  \\
&& + 2\delta r \|Kz_k - Kz_{k+1} \|^2 \\
&\leq& 2\lambda \|X_{k+1} - X_* \|^2 + r \|K z_{k}- Kz_{k+1}\|^2 \\
&& + \frac{1}{r}\|H_k - H_{k+1} \|^2,
\end{eqnarray*}
by making $\delta$ sufficiently small such that 
\begin{equation}
\begin{aligned}
   \delta \leq \frac{2\lambda}{ r + \frac{2L^2}{r}} \quad and \quad \delta \leq \frac{1}{2}.
\end{aligned}
\end{equation}
%To maximize $\delta$, one can choose $r = \sqrt{2} L$. Then $\delta \leq \frac{1}{\sqrt{2}} \frac{\lambda}{L}$. 
%\textcolor{red}{I guess this is not of our main interest here !}
This completes the proof. 
\end{proof}
\end{comment} 
%\begin{remark} 
%Since we have that with $\omega = \frac{2}{r + L_F + r + \lambda_F}$, 
%\begin{eqnarray*}
%\lambda_{max}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+L_F} = \frac{r + L_F - \omega}{r + L_F} \\  
%\lambda_{min}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+\lambda_F} = \frac{r %+ \lambda_F - \omega}{r + \lambda_F}.
%\end{eqnarray*}
%Thus, we have that
%\begin{equation}
%\frac{\kappa(A_1) - 1}{\kappa(A_1) + 1} = \frac{ \frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} - 1}{\frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} + 1} =\frac{ \frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} - 1}{\frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} + 1} = \frac{\frac{r + \lambda_F}{r + L_F}  - \frac{\lambda_F}{L_F}}{\frac{r + \lambda_F}{r + L_F} + \frac{\lambda_F}{L_F}}  
%\end{equation} 
%and 
%\begin{equation}
%\frac{\kappa(A_2) - 1}{\kappa(A_2) + 1} = \frac{ \frac{r + \lambda_F}{r + L_F} - 1}{\frac{r + \lambda_F}{r + L_F}  + 1}  
%\end{equation}
%On the other hand, we have that 
%\begin{equation}
%\sin (\theta_1 + \theta_2) = 
%\end{equation}
%and $\lambda_{min} = 1 - \frac{\omega}{r + \lambda_F} = \frac{\lambda_F}{r + \lambda_F}$ while $H_{A_r} = 1/(r + L_F)$ and $1/(r + \lambda_F)$. Therefore, if $r$ is sufficiently large, then $\sin\theta$ is quite small. 
%\begin{theorem}
%For $A_1$ and $A_2$, symmetric positive definite matrices, we have 
%\begin{equation}
%(x,A_1A_2y) \leq \sin (\theta_1 + %\theta_2) \|A_1x\|\|A_2y\|,  
%\end{equation}
%where $\sin \theta_1 = \frac{\kappa(A_1) - 1}{\kappa(A_2) + 1}$ and $\sin \theta_2 = \frac{\kappa(A_2) - 1}{\kappa(A_2) + 1}$. 
%\end{theorem}
%\end{remark}
\begin{remark}
We remark that it is more natural to write the convergence rate in terms of $\kappa(G^*)$, the condition number of $G^*$ since the choice of $\omega$ is made for solving the system relevant to $G^*$. However, it is also fine to use $\kappa(G)$ since it is more relvant to the problem to be solved and we have that
\begin{equation}
\kappa(G^*) = \kappa(G) \quad \rightarrow \quad 
\frac{\kappa(G) - 1}{\kappa(G) + 1} = \frac{\kappa(G^*) - 1}{\kappa(G^*) + 1}. 
\end{equation}
\end{remark} 

\subsubsection{Convergence analysis of Algorithm \ref{algADMM1} for $D_r^* \neq A_r^*$}\label{gdn} 

In this section, our goal is to establish the convergence of inexact Uzawa with Gauss-Seidel method, $D_r = A_r$, is replaced by inexact local solve for $X$-variable. This includes the $n-$step of Gradient Descent iteration. We recall that the Gauss-Seidel method can be interpreted as to solve the following optimization exactly in $X$-update \eqref{Xupdate} of the Algorithm \ref{algADMM1}:  
\begin{equation}
\min_{X} L_r(X,z_k,H_k).  
\end{equation}
We now let $G(X) = L_r(X,z_k,H_k)$, i.e.,  
\begin{equation}
G(X) = F(X) + \langle H_k, Kz_k - X\rangle + \frac{r}{2}\|Kz_k - X\|^2 
\end{equation}
and let $Y_*  = {\rm arg}\min_X G(X)$. Note that we reserve $X_{k+1}$ as the result of $N-$step of GD method. Then the following statements hold true: 
\begin{enumerate} 
\item $Y_* = A_r^*(H_k + rKz_k)$ 
\item $\nabla F(Y_*) + r Y_* = H_k + rKz_k$ 
\item $\nabla G(Y_*) = 0$. 
\end{enumerate} 
Note that it is easy to show that $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex. Our discussion is for general inexact local solve and the outcome will be denoted by $X_{k+1}$, i.e., 
\begin{equation}
X_{k+1} = D_r^*(H_k + rKz_k). 
\end{equation}
For the convergence analysis, the exact Gauss-Seidel case will be used as an intermediate step. Thus, naturally, the convergence estimate could be made to be sharper. We begin our discussion by introducing two quantities:
\begin{eqnarray*}
\textsf{E}_1 &:=& E_k^H - \omega \left \{A_r^{*}(H_* + rKz_*) - A_r^*(H_k + rKz_k) \right \}\\
\textsf{E}_2 &:=& A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k).  
\end{eqnarray*}
It is evident that $\textsf{E}_1$ is for the error of the Algorithm \ref{algADMM1} with $D_r^* = A_r^*$ and $\textsf{E}_2$ represents the difference between two iterates, one from the inexact solve for $X-$variable and the other from the exact solve. We shall now see that Lemma \ref{main:lem1} can lead to the following estimate easily. 
\begin{eqnarray*}
&& \left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega \{ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \} \right \|^2 \\
&& \qquad \leq \|\textsf{E}_1\|^2 + \omega^2 \|\textsf{E}_2\|^2 + 2 \omega \|\textsf{E}_1\| \|\textsf{E}_2\|. 
\end{eqnarray*}
The first term is relevant to the Algorithm \ref{algADMM1} with $D_r = A_r$. We shall assume that there exists $\delta < 1$ such that 
\begin{equation}\label{main:cvdelta}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ).
\end{equation}
Namely, the inexact solve provides a reasonably close solution to $Y_*$. Under this assumption, we shall establish the convergence of Algorithm \ref{algADMM1}. How small $\delta$ can be, for still allowing the convergence is determined by the close investigation of the convergence of Algorithm \ref{algADMM1} with exact local solve after incorporating $\delta$. We note that $\delta$ is provided in Lemma \ref{gssmooth} below.    

We are now in a position to provide a main instrumental lemma in this section. 
\begin{theorem}\label{main:ins}
Let $\delta_{GS}$ be the convergence rate for the Algorithm \ref{algADMM1}, when $D_r = A_r$, as given in equation \eqref{gsrate}, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be such that the equation \eqref{main:cvdelta} hold. Then, we have the following estimate:   
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \delta_{GS} + \delta \right )^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{theorem} 
\begin{proof}
We shall let $E^2 = \|E_k^H\|^2 + \|E_k^Z\|_\omega^2$. Due to the equation \eqref{main:cvdelta}, we have that 
\begin{equation}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 E^2. 
\end{equation}
We then obtain that 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 &\leq& \|\textsf{E}_1\|^2 + \|\textsf{E}_2\|_\omega^2 + 2 \|\textsf{E}_1\| \|\textsf{E}_2\|_\omega  \\ 
&=& \delta_{GS}^2 E^2 + \delta^2 E^2 + 2 \delta_{GS} \delta E^2 = (\delta + \delta_{GS})^2 E^2. 
\end{eqnarray*}
%Here we have invoked a simple Cauchy-Schwarz inequality that 
%\begin{equation*}
%ab \leq \frac{1}{2\varepsilon} a^2 + \frac{\varepsilon}{2} b^2, \quad \forall a, b\in \Reals{}.  
%\end{equation*}
%Finally, we observe that $\delta_{GS}$
This completes the proof. 
\end{proof}

We also note that the $n-$step of GD method is basically, given as follows: for a given step size $\gamma > 0$ and $X_k = Kz_k$, 
\begin{subeqnarray}\label{ngd1}
X_{k+\frac{1}{n}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{n}} &=& X_{k+\frac{1}{n}} - \gamma \nabla G(X_{k+\frac{1}{n}}) \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{n-1}{n}} &=& X_{k+\frac{n-2}{n}} - \gamma \nabla G(X_{k + \frac{n-2}{n}}) \\
X_{k+\frac{n}{n}} &=& 
X_{k+\frac{n-1}{n}} - \gamma \nabla G(X_{k+\frac{n-1}{n}}), 
\end{subeqnarray}
where $\nabla G$ is given as follows: 
\begin{equation} 
\nabla G(X) = \nabla F(X) + rX - H_k - rKz_k. 
\end{equation}

We shall now present a simple but important lemma: 
\begin{lemma}\label{main:lm} 
Let $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ and $D_r^*(H_k + rKz_k)$ be the $n$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \leq  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2n} \|Y_* - X_k\|^2. 
\end{eqnarray*}
\end{lemma}
\begin{proof}
Let $Y_* = {\rm arg} \min_{X} G(X)$. On the other hand, $X_{k+1}$ is obtained by the following iteration: 
\begin{eqnarray*} 
X_{k+\frac{1}{n}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{n}} &=& X_{k+\frac{1}{n}} - \gamma \nabla G(X_{k+\frac{1}{n}}) \\ 
&\vdots& \\  
X_{k+\frac{n-1}{n}} &=& X_{k+\frac{n-2}{n}} - \gamma \nabla G(X_{k + \frac{n-2}{n}}) \\
X_{k+\frac{n}{n}} &=& 
X_{k+\frac{n-1}{n}} - \gamma \nabla G(X_{k+\frac{n-1}{n}}).  
\end{eqnarray*}
Since $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex, we see that by Lemma \ref{lemmaGD}, we have that 
\begin{eqnarray*}
\|Y_* - X_{k+1}\|^2 \leq \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2n} \|Y_* - X_k\|^2. 
\end{eqnarray*}
%
%&\leq& 2 \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \|Y_* - X_*\|^2 + \|X_* - X_k\|^2 \right ) \\
%&\leq& c \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \frac{1}{r + \lambda_F} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2 \right ) + \|X_* - X_k\|^2 \right ). 
%\end{eqnarray*} 
This completes the proof. 
\end{proof}
\begin{remark} 
The choice of $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ is optimal for GD and the convergence rate can be calculated. Since $\kappa(G) = \frac{r + L_F}{r + \lambda_F}$, we have that 
\begin{equation}
\frac{\kappa(G)-1}{\kappa(G)+1} = \frac{L_F - \lambda_F}{2r + L_F + \lambda_F}. 
\end{equation}
The convergence is even faster for large $r$ and large $n$.
\end{remark}
We shall now see that the norm of $\textsf{E}_2$ can be made to be quite small. 
\begin{lemma}\label{gssmooth} 
Let $\gamma = \frac{2}{\lambda_G + L_G}$, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be the $n$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 \leq 10 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2n} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{lemma}
\begin{proof}
By Lemma \ref{main:lm}, we have that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 &=& \omega^2 \|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \\
&\leq& \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2n} \omega^2 \|Y_* - X_k\|^2. 
\end{eqnarray*}
On the other hand, we have that since $X_k = Kz_k$ and due to Theorem \ref{thm:GS},  
\begin{eqnarray*}
\omega^2 \|Y_* - X_k\|^2 &\leq& 2 \omega^2 \|Y_* - X_*\|^2 + 2 \|X_* - X_k\|_\omega^2 \\ 
&\leq& \frac{2 \omega^2}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|X_* - X_k\|_\omega^2 \\
&\leq& \frac{8(r + L_F)^2}{(2r + L_F + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|E_k^Z\|_\omega^2 \\ 
&\leq& 10 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) \end{eqnarray*} 
The last inequality is due to the fact that $X_k = Kz_k$ and 
\begin{equation}
\omega = \frac{2(r + \lambda_F)(r + L_F)}{(2r + L_F + \lambda_F)},
\end{equation} 
thus, 
\begin{equation}
\frac{2 \omega^2}{(r + \lambda_F)^2} = \frac{8(r+L_F)^2}{(2r + L_F + \lambda_F)^2}  \leq 8.  
\end{equation}
This completes the proof. 
\end{proof}
This result gives that if $n$ is large enough, then we can obtain the convergence of $n-$step GD based FL. 

\begin{theorem}\label{main:ngd}
The convergence rate for the Algorithm \ref{algADMM1}, when $n$-step GD is used and $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ can be estimated as follows: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \rho^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ), 
\end{eqnarray*}
where 
\begin{equation}
\rho = \sqrt{ \left (\frac{r}{r+\lambda_F} \right )^2 + \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F } \right )^2  } + \sqrt{10} \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F } \right )^n.
\end{equation}
\end{theorem} 
\begin{proof}
From Lemma \ref{gssmooth}, we have that 
\begin{equation}
\delta = \sqrt{10} \left (\frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^n.
\end{equation}
Therefore, by using Theorem \ref{main:ins}, we have that the convergence rate is given as follows:
\begin{equation}
\rho = \rho_{GS} + \delta. 
\end{equation}
This completes the proof. 
\end{proof}
\begin{corollary}
Let $r = 0$, the FL algorithm with $n-$step GD, we have the following convergence rate:
\begin{equation}
\rho = \frac{L_F - \lambda_F}{L_F + \lambda_F} + \sqrt{10} \left ( \frac{L_F - \lambda_F}{L_F + \lambda_F} \right )^n. 
\end{equation}
\end{corollary}
\begin{remark}
By applying $n$ sufficiently large, i.e., the local solve is done accurate enough, then the overall convergence rate reduces to 
\begin{equation}
\rho^2 \approx \left ( \frac{L_F - \lambda_F}{L_F + \lambda_F} \right )^2. 
\end{equation}
We note that if $r$ is chosen to be large, then, $n$ can be made to be small enough so that the overall convergence rate can reduce to that of GS. However, the larger $r$ (unless it is large enough), can lead to the regime where the GS rate can be larger than one, depending on $L_F$ and $\lambda_F$, thus the convergence can not be made. Furthermore, we can show that if $n$ satisfies the following inequality: 
\begin{equation}
n > \frac{\log[\sqrt{10}(\kappa(G) + 1)/2]}{\log[(\kappa(G) +1)/(\kappa(G) - 1)]}. 
\end{equation}
then the convergence can be obtained. 
\end{remark}

\subsubsection{Direct convergence rate computation and its challenge}

We note that this perturbation argument does not explain the convergence of the one step GD method as discussed in \cite{mishchenko2022proxskip}. Also, this estimate can not be sharp since it is given relative to the convergence with respect to the Gauss-Seidel case. It is challenging to attempt to estimate directly to the GD $n-$step case. For a simple case in which the single GD step is used for $X$ variable, we can obtain the following results: 
\begin{lemma} 
Let $\gamma > 0$ and $\omega = 1/\gamma$. Then, for the GD one step for $X$ variable, i.e., $n = 1$, the error bound can be shown to result in the following form: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|_\omega^2 \leq (1 - \gamma \lambda_F)^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ).  
\end{eqnarray*}
\end{lemma} 
\begin{proof} 
Here, with $X_k = Kz_k$, 
\begin{eqnarray*}
D_r^*( H_k + rKz_k) &=& X_k - \gamma \nabla G(X_k) = X_k - \gamma (\nabla F(X_k) + rX_k - H_k - rKz_k) \\&=& Kz_k - \gamma (\nabla F(Kz_k) - H_k) %  - \langle (I - \omega \gamma)(H_* - H_k), (I - \gamma A) (X_* - X_k) \rangle = 0. 
\end{eqnarray*}
On the other hand, we have 
\begin{eqnarray*}
A_r^*(H_* + rKz_*) &=& X_* - \gamma \nabla G(X_*) = X_* - \gamma (\nabla F(X_*) + rX_* - H_* - rKz_*) \\
&=& X_* - \gamma (\nabla F(X_*) - H_*). 
\end{eqnarray*}
We note that from lemma \ref{main:lem1}, the following error bound holds true: with $\omega = 1/\gamma$,
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 &=& \left \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \right \|^2 \\
&=& \left \|E_k^H - \omega (E_k^Z -\gamma (\nabla F(X_*) - \nabla F(Kz_k)) - \omega \gamma (H_* - H_k)  \right \|^2 \\
&=& \left \|E_k^H - \omega (E_k^Z -\gamma (\nabla F(X_*) - \nabla F(Kz_k)) - \omega \gamma (H_* - H_k)  \right \|^2 \\
&=& \omega^2 \left \|E_k^Z - \gamma (\nabla F(X_*) - \nabla F(Kz_k)) \right \|^2 \\ 
&\leq& (1 - \gamma \lambda_F)^2 \|E_k^Z\|_\omega^2 \leq (1 - \gamma \lambda_F)^2 \left ( \left \|E_{k}^H \right \|^2 + \omega^2 \left \|E_{k}^Z \right \|^2 \right ).
\end{eqnarray*}
Therefore, we complete the proof that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} \leq (1 - \gamma \lambda_F)^2 \left( \|H_* - H_k\|^2 + \|Kz_* - Kz_k\|^2_\omega \right ).
\end{eqnarray*}
\end{proof}
\begin{remark}
This argument is difficult to extend to many steps of GD since the orthogonality can not be established in general. Therefore, the perturbation argument is used and the optimal choice of $n$ seems to be taken into account only for the dominant terms when $n$ is large enough. 
\end{remark}

\section{Unresolved Open problem and Outlook} 

The open problem is to achieve the convergence rate similar to the conjugate gradient method, i.e., 
\begin{equation}
\rho = 1 - \frac{1}{\sqrt{\kappa}}. 
\end{equation}
Unlike the stochastic case, it is extremely difficult. We shall illustrate the difficulty using the linear case. We consider the convergence of the iterative method based on inexact Block Gauss-Seidel for $U$ block and Richardson for $H$ block. The Algorithm can be written as given in the Algorithm \ref{algADMM3}.
\begin{algorithm}
\caption{Federated Learning formulation of FL}\label{algADMM3} 
Given $H_0$ such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,K-1$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$),  
    \begin{equation} \label{Xupdate1}
    X_{k+1} = G_{n,r}(X_k;H_k + r Kz_k),
\end{equation} }
    \State{$z_{t+1}$ update: 
        \begin{equation} \label{zupdate}
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} \label{Hupdate5}
        H_{k+1} = H_k + \omega (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}
We note that the action of the operator $G_{n,r}$ depends on $X_k$. The case when $G_{n,r}$ is the standard $n-$step GD, the algorithm can be written in a very standard way as given in Algorithm \ref{GD1}. 
\begin{algorithm}
\caption{The case that $G_{n,r}$ is the Standard GD}\label{GD1} 
Given $H_k$ and $z_k$ with $K^TH_k = 0$, update for $X_{k+1}$ is obtained as follows:  
\begin{algorithmic}
\For{$\ell=1,2,\cdots,n$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$ and $b_{H,rKz} = H_k + rKz_k$),  
    \begin{equation} \label{Xupdate}
    X_{k+\ell/n} = X_{k+(\ell-1)/n} + \gamma (b_{H,rKz} - A_r X_{k+(\ell-1)/n})
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{subeqnarray} 
X_{k+\frac{1}{n}} &=& X_{k} + \gamma (H_k 
+ rKz_k - A_r X_k) \nonumber \\ 
X_{k+\frac{2}{n}} &=& X_{k+\frac{1}{n}} + \gamma (H_k + rKz_k - A_r(X_{k+\frac{1}{n}})) \nonumber \\
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \nonumber \\  
X_{k+\frac{n-1}{n}} &=& X_{k+\frac{n-2}{n}} + \gamma (H_k + rKz_k - A_r(X_{k + \frac{n-2}{n}})) \nonumber \\
X_{k+\frac{n}{n}} &=& X_{k+\frac{n-1}{n}} + \gamma (H_k + rKz_k - A_r(X_{k+\frac{n-1}{n}})). 
\end{subeqnarray}
The Algorithm \ref{GD1} satisfies the following identity for $n \rightarrow \infty$, i.e., $Y_*$ such that
\begin{equation} 
A(Y_*) + r Y_* = H_k + rKz_k. 
\end{equation}
In passing to the next section, we shall make a simple remark. In case $H_k = H_*$ and $z_k = z_*$, we see that both schemes lead to $X_{k+1} = X_*$ in a single iteration. Thus, we observe that for all $r \geq 0$, 
\begin{equation}
X_* = G_{n,r}(X_*; H_* +r Kz_*) = A_r^{-1}(H_* + rKz_*).  
\end{equation} 

\subsection{General framework of convergence analysis for Algorithm \ref{algADMM3}}

In this section, we shall discuss the basic framework to analyze the convergence of the Algorithm \ref{algADMM3}. We shall use the standard notation that for all $k \geq 0$ to discuss the convergence:  
\begin{eqnarray*}
E_k^X &=& X_* - X_k \\
E_k^Z &=& Kz_* - Kz_k \\ 
E_k^H &=& H_* - H_k. 
\end{eqnarray*}
The following is the main result in this section. 
\begin{theorem}\label{main:theorem0} 
The Algorithm \ref{algADMM3} with the inexact or exact solve, the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega (A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k)) \right \|^2. 
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM3} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& G_{n,r} (Kz_k;H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $G_{n,r}$ is an approximate of $A_r^{-1}$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z = K(K^TK)^{-1}K$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& G_{n,r} (Kz_k; H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T G_{n,r} (Kz_k;H_k + r K z_k)) = P_Z [G_{n,r} (Kz_k; H_k + r K z_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{-1} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{-1}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{-1} (H_* + rK z_*) - G_{n,r}(Kz_k; H_k + rK z_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorHx}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{-1} (H_* + r K z_*) - G_{n,r}(Kz_k; H_k + r K z_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH}, and using the orthogonality between $E_i^H$ and $E_j^Z$ for all $i,j$, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k))\|^2. 
\end{eqnarray*}
This completes the proof. 

% It is important to observe that 
% \begin{equation}
% X_* = A_r^*(H_* + rKz_*) = D_r^*(H_* + rKz_*). 
% \end{equation}

% Therefore, we see that
% \begin{eqnarray*}
% && \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% && \qquad = \|H_* - H_k - \omega (D_r^* (H_* + rKz_*) - D_r^*(H_k + rKz_*))\|. 
% \end{eqnarray*}
% The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
% \begin{eqnarray*}
% -\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] \right ). 
% \end{eqnarray*}
% Lastly, for $H$, we have 
% \begin{eqnarray*}
% E_{k+1}^H &=& E_k^H + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
% &=& E_k^H - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
% &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
% && \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
% &=& E_k^H - \omega Q_Z (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \\ 
% &=& Q_Z [E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))] 
% \end{eqnarray*}
% First, we shall observe that   
% \begin{eqnarray*}
% \omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left( P_Z [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right )\| \\
% &=& \|P_Z [(H_* - H_k) - \omega \left( [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right ) ] \|
% \end{eqnarray*}
% Next, we observe that there is a close relationship between $E_k^Z$ and $E_k^H$: 
% \begin{eqnarray*}
% \|H_{*} - H_{k+1}\| = \|Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))]\|. 
% \end{eqnarray*}
% Therefore, we have that by the nonexpansiveness, 
% \begin{eqnarray*}
% \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 %\\   
% %&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% %&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
% \end{eqnarray*}
% This completes the proof. 
\end{proof}

\subsection{Convergence analysis of FL Algorithm \ref{algADMM3} with GS} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem10} 
Given 
\begin{equation}
\omega \leq r + \lambda_F, 
\end{equation}
the Algorithm \ref{algADMM3} with GS, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( 1 - \frac{1}{\kappa(A_r)} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ). 
\end{eqnarray*}
\end{theorem}
\begin{proof} 
We observe that  
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2  &\leq& \left \|E_k^H - \omega (A_r^{-1}(H_*+rKz_*) - A_{r}^{-1} (H_k + rKz_k)) \right \|^2 \\
&=&  \left \|E_k^H - \omega (A_{r}^{-1}(H_*) + A_r^{-1}(rKz_*) - A_r^{-1}(H_k) - A_r^{-1}(rKz_k) \right \|^2 \\
&=& \left \|(I - \omega A_{r}^{-1})(E_k^H) - \omega r A_r^{-1} (E_k^Z) \right. \|^2 \\
&\leq& \|(I - \omega A_r^{-1}) E_k^H \|^2 + \|\omega r A_r^{-1} E_k^Z\|^2 - 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle. 
\end{eqnarray*}
%We shall divide both sides by $\omega$, to obtain that 
%\begin{eqnarray*}
%&& \left ((1/\omega)E_{k+1}^H, E_{k+1}^H \right ) + \left ( \omega E_{k+1}^Z, E_{k+1}^Z \right ) \leq ((1/\omega)(I - \omega A_r^{-1}) E_k^H, (I - \omega A_r^{-1}) E_k^H ) \\
%&& \quad + r^2 (\omega A_r^{-1} E_k^Z, A_r^{-1} E_k^Z) - 2r  \langle \sqrt{(1/\omega)} (I - \omega A_r^{-1}) E_k^H, \sqrt{\omega} A_r^{-1} E_k^Z \rangle. 
%\end{eqnarray*}
%We observe that since $\omega = r + \lambda_F$, $\omega A_r^{-1} \leq 1$, 
%\begin{equation}
%\left ( \omega A_r^{-1} A_r E_{k+1}^Z, E_{k+1}^Z \right )
%\end{equation} 
We therefore, note that 
\begin{eqnarray*}
\|(I - \omega A_r^{-1}) E_k^H \|^2 &\leq& \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^H\|^2 \\ 
\|\omega r A_r^{-1} E_k^Z\|^2 &\leq& \left ( \frac{r}{r+\lambda_F} \right )^2 
\|E_k^Z\|_\omega^2 \\ 
- 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle &\leq& \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^H\|^2 + \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^Z\|_\omega^2.  
\end{eqnarray*}
This completes the proof for GS case. 
\end{proof} 

\begin{remark}
This case can not produce the convergence rate which is similar to that of CG. 
\end{remark}

\subsection{Convergence analysis of FL Algorithm \ref{algADMM3} with GD} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem11} 
Given 
\begin{equation}
\omega \leq r + \lambda_F, 
\end{equation}
the Algorithm \ref{algADMM3} with GD, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
%\begin{eqnarray*}
%\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( 1 - %\frac{1}{\kappa(A_r)} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 \right ) \left (\left %\|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ). 
%\end{eqnarray*}
\end{theorem}
\begin{proof} 
We observe that  
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2  &\leq& \left \|E_k^H - \omega (A_r^{-1}(H_*+rKz_*) - G_{n,r}(H_k + rKz_k)) \right \|^2 \\
&=& \left \|E_k^H - \omega (A_{r}^{-1}(H_* + rKz_*) - A_r^{-1}(H_k + rKz_*) + A_r^{-1}(H_k + rKz_* ) - G_{n,r}(H_k +rKz_k)) \right \|^2 \\
&=& \left \|(I - \omega A_{r}^{-1})(E_k^H) - \omega (A_r^{-1}(H_k + rKz_* ) - G_{n,r}(H_k +rKz_k)) \right. \|^2 \\
&=& \| E_1 - E_2 \|^2 
%&\leq& \|(I - \omega A_r^{-1}) E_k^H \|^2 + \|\omega r A_r^{-1} %E_k^Z\|^2 - 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} %E_k^Z \rangle. 
\end{eqnarray*}
%We shall divide both sides by $\omega$, to obtain that 
%\begin{eqnarray*}
%&& \left ((1/\omega)E_{k+1}^H, E_{k+1}^H \right ) + \left ( \omega E_{k+1}^Z, E_{k+1}^Z \right ) \leq ((1/\omega)(I - \omega A_r^{-1}) E_k^H, (I - \omega A_r^{-1}) E_k^H ) \\
%&& \quad + r^2 (\omega A_r^{-1} E_k^Z, A_r^{-1} E_k^Z) - 2r  \langle \sqrt{(1/\omega)} (I - \omega A_r^{-1}) E_k^H, \sqrt{\omega} A_r^{-1} E_k^Z \rangle. 
%\end{eqnarray*}
%We observe that since $\omega = r + \lambda_F$, $\omega A_r^{-1} \leq 1$, 
%\begin{equation}
%\left ( \omega A_r^{-1} A_r E_{k+1}^Z, E_{k+1}^Z \right )
%\end{equation} 
We therefore, note that 
\begin{eqnarray*}
&& \|(I - \omega A_r^{-1}) E_k^H \|^2 \leq  \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^H\|^2 \\
&& \|\omega (A_r^{-1}(H_k + rKz_* ) - G_{n,r}(H_k +rKz_k)) \|^2 \\
&& \qquad \leq \|\omega (A_r^{-1}(H_k + rKz_*) - A_r^{-1}(H_k + rKz_k) + A_r^{-1}(H_k + rKz_*) - G_{n,r}(H_k +rKz_k)) \|^2 \\
&& \qquad \leq 2 \|\omega (A_r^{-1}(H_k + rKz_* ) - A_r^{-1}(H_k + rKz_k)\|^2 + 2\|\omega(A_r^{-1}(H_k + rKz_k) - G_{n,r}(H_k +rKz_k)) \|^2 \\
&& \qquad \leq 2 \omega^2 \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - Kz_k\|^2 + 2 \omega^2 \delta^{2n} \|Y_* - Kz_k\|^2 \\ 
&& \qquad \leq 2 \omega^2 \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - Kz_k\|^2 + 2 \frac{\omega^2}{(r + \lambda_F)^2} \delta^{2n} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ) \\ 
%- 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle &\leq& \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^H\|^2 + \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^Z\|_\omega^2.  
\end{eqnarray*}
This completes the proof for GS case. 
\end{proof} 


%\begin{remark}
%There is an alternative method to obtain the same result. This is to use the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$. Namely, we have that
%\begin{eqnarray*}
%E_{k+1}^H &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
%&& \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
%- \omega E_{k+1}^Z &=& -\omega \left ( P_Z [ %A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK %z_k) ] \right ). 
%\end{eqnarray*}
%Thus, we have that 
%\begin{eqnarray*}
%E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - %\omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} %(H_k + r K z_k). 
%\end{eqnarray*}
%Now, we have that 
%\begin{eqnarray*}
%\left \|E_{k+1}^H - \omega E_{k+1}^Z \right %\|^2 &=& \left \|E_{k+1}^H \right \|^2 + %\omega^2 \left \|E_{k+1}^Z \right \|^2 \\
%&=& \left \|E_k^H - \omega \left [ A_r^{*} %(H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \right ] \right \|^2. 
%\end{eqnarray*}
%The first equality is due to the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$.  
%\end{remark}

We note that these two main theorems produce identity for the convergence estimate. The key is now to obtain the estimate of the right hand side with an appropriate choice of $\omega$ and $\gamma$. 

\subsubsection{Convergence analysis of Algorithm \ref{algADMM3} with GD given in Algorithm \ref{GD1}}
Throughout this section, we shall set 
\begin{equation}
\delta = \frac{\kappa(A_r)-1}{\kappa(A_r)}. 
\end{equation} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem04} 
Given $\gamma = \frac{1}{r + L_F}$ and \begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{r + \lambda_F} + \frac{1}{r+L_F}}, 
\end{equation}
the Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( \delta^n + (1 - \delta^n) \frac{r}{r+\lambda_F} \right )^2  + \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
\end{eqnarray*}
where $\kappa(S_r) = \frac{(r+L_F)}{(r+\lambda_F)}(1 - \delta^n).$ Furthermore, we know that $E_k^Z$ is orthogonal to $E_k^H$. \textcolor{red}{If we can show that these can be decomposed into eigenvectors of $A$ or $A$-orthogonal to each other}, then we have that under the assumption that $n$ is chosen so that 
\begin{equation}
1 - \frac{1}{\sqrt{\kappa}} = \delta^n. 
\end{equation}
We have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq 
\left ( 1 - \frac{1}{\sqrt{\kappa}} \right )^2  \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
\end{eqnarray*}
\end{theorem}
We choose $1 - \delta^n = \frac{1}{\sqrt{\kappa}}$, then we have 
\begin{equation}
1 - \frac{1}{\sqrt{\kappa}} = \delta^n. 
\end{equation}
Furthermore, we have 
\begin{equation}
\kappa(S_r) = \frac{L_F}{\lambda_F}(1 - \delta^n) = \sqrt{\kappa}. 
\end{equation}
\begin{remark}
For $r = 0$, we have that 
\begin{equation}
\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa}} = \delta^{n}. % + [\kappa(A) (1 - \delta^n)]^2
\end{equation}
Taking log, we have 
\begin{equation}
n \log \delta = \log \left (\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa}}\right ). 
\end{equation}
Therefore, we see that 
\begin{equation}
n = \frac{\log \left (\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa}}\right )}{\log \delta}.  
\end{equation}
\end{remark}
\begin{proof} 
We first recall that the following identity holds: with $\mathcal{H}_\gamma = I - \gamma A_r$, 
\begin{eqnarray*}
G_{n,r}(X;H_k + rKz_k) &=& (I - \gamma A_r)^n X + (I - (I - \gamma A_r)^n) A_r^{-1} (H_k + rKz_k) \\
&=& \mathcal{H}_\gamma^n X + (I - \mathcal{H}_\gamma^n) A_r^{-1} (H_k + rKz_k). 
\end{eqnarray*} 
With an observation that 
\begin{equation}
A_r^{-1}(H_* + rKz_*) = G_{n,r}(Kz_*;H_* + rKz_*).  
\end{equation}
According to Theorem, we shall need to estimate the following: 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad =  \left \|E_k^H - \omega (G_{n,r}(Kz_*;H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad = \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_*+rKz_*) - G_{n,r}(Kz_*; H_k+rKz_*)) \right. \\
&& \qquad\qquad \left. - \omega ( G_{n,r}(Kz_*;H_k+rKz_*) - G_{n,r}(Kz_k;H_k+rKz_k)) \right \|^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*}
We shall now investigate the following: 
\begin{subeqnarray*}
E_1 &=& E_k^H - \omega (G_{n,r}(Kz_*;H_* + rKz_*) - G_{n,r}(Kz_*;H_k + rKz_*)) \\ 
&=& E_k^H - \omega (I - \mathcal{H}_\gamma^n) A_r^{-1}(H_* - H_k) \\
&=& (I - \omega (I- \mathcal{H}_{\gamma}^n) A_r^{-1}) E_k^H \\ 
E_2 &=& G_{n,r}(Kz_*;H_k+rKz_*) - G_{n,r} (Kz_k;H_k+rKz_k) \\ 
&=& \mathcal{H}_\gamma^n K(z_* - z_k) + (I - \mathcal{H}_\gamma^n)A_r^{-1}r(Kz_* - Kz_k) \\ 
&=& \left ( \mathcal{H}_\gamma^n + r (I - \mathcal{H}_\gamma^n)A_r^{-1} \right) K(z_* - z_k). 
\end{subeqnarray*}
This leads that 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_* + rKz_*) - G_{n,r} (Kz_k; H_k + rKz_k)) \right \|^2 \\
&& \qquad = \|E_1 - \omega E_2\|^2 \\
&& \qquad =  \|E_1\|^2 - 2  \langle E_1, \omega E_2\rangle + \|E_2\|_\omega ^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*} 

A simple choice of $\gamma$ would be $\gamma = \frac{1}{r + L_F}$. Then, we see that, in an increasing order: 
\begin{subeqnarray*}
\sigma(\mathcal{H}_\gamma^n) &=& \{(1 - \gamma (r + L_F)^n, \cdots, (1 - \gamma (r + \lambda_F)^n  \} \\ 
\sigma(I - \mathcal{H}_\gamma^n) &=& \{1 - (1 - \gamma (r + \lambda_F))^n, \cdots, 1 - (1 - \gamma (r + L_F))^n  \} \\
\rho((I - \mathcal{H}_\gamma^n)A_r^{-1}) &=& \max \left \{ \frac{(1 - (1 - \gamma (r + \lambda_F))^n)}{r+\lambda_F}, \frac{1 - (1 - \gamma (r + L_F))^n}{r + L_F}  \right \} \\ 
\rho(\mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r) &=& \max \left \{ (1 - \gamma (r + \lambda_F))^n + (1 - (1 - \gamma (r+\lambda_F))^n))\frac{r}{r+\lambda_F}, \right. \\
&& \qquad \left. (1 - \gamma (r+ L_F))^n + (1 - (1 - \gamma (r+L_F))^n)\frac{r}{r + L_F}  \right \}. 
\end{subeqnarray*}
Thus, we have that 
\begin{subeqnarray*}
\sigma(\mathcal{H}_\gamma^n) &=& \{(1 - \gamma (r+L_F))^n, \cdots, (1 - \gamma (r+\lambda_F))^n  \} \\ 
\sigma(I - \mathcal{H}_\gamma^n) &=& \{1 - (1 - \gamma (r+\lambda_F))^n, \cdots, 1 - (1 - \gamma (r+L_F))^n  \} \\
\rho((I - \mathcal{H}_\gamma^n)A_r^{-1}) &=& \max \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}  \right \} \\ 
\rho(\mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r) &=& \max \left \{ \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F}, \frac{r}{r + L_F}  \right \}. 
\end{subeqnarray*}
The easy bound would be for $E_2$. We note that 
\begin{equation}
\|E_2\| \leq \max \left \{ \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F}, \frac{r}{r + L_F}  \right \} \|Kz_* - Kz_k\|. 
\end{equation}
On the other hand, we have that with $S_r = (I - \mathcal{H}_\gamma^n)A_r^{-1}$, 
\begin{equation*} 
\lambda_{min}(S_r) = \min \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}    \right \} \quad \mbox{ and } \quad \lambda_{max}(S_r) = \max \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}    \right \}.  
\end{equation*}  
Thus, the choice of $\omega$ given as follows:  
\begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{r + \lambda_F} + \frac{1}{r+L_F}}. 
\end{equation} 
With this choice, we obtain the convergence rate given as follows: 
\begin{equation}
\|E_1\| \leq \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right ) \|H_* - H_k\|. 
\end{equation} 
We shall now make it clear by choosing $n$ small and $n$ large. First, for $n \gg 1$, we have that
\begin{equation}
\|E_2\| \leq \left ( \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F} \right ) \|Kz_* - Kz_k\| 
\end{equation}
and 
\begin{equation}
\kappa(S_{r}) = \frac{(r+L_F)}{(r+\lambda_F)} (1 - \delta^n).  
\end{equation} 
On the other hand, if $n = O(1)$, then it is unclear since there are many factors. This completes the proof. 
\end{proof} 
\begin{remark}
Note that $\mathcal{H}_\gamma$ takes the spectral radius for the vector 
\begin{equation}
(I - \gamma A)^n \phi = \left ( 1 - \frac{\lambda}{L} \right )^n \phi = \left (1 - \frac{1}{\kappa} \right )^n \phi, 
\end{equation}
namely, for eigenvector that corresponds to the smallest eigenvalue. On the other hand, the spectral radius for $\phi$, which is also corresponding to the smallest eigenvalue of $A$ as well, i.e., 
\begin{equation}
I - \omega (I - (I - \gamma A)^n) A^{-1} 
\end{equation}
happens for $\phi$ as well. In any case, two operators $(I - \gamma A)^n$ and $(I - \omega (I - (I - \gamma A)^n)A^{-1}$ share eigenvectors with $A$ or $A^{-1}$. 
\end{remark}

%\begin{corollary}\label{main:corollary} 
%Given $\gamma = \frac{1}{r + L_F}$, and $\omega = 1/\gamma$, we set $n = 1$, then we have 
%the Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
%\begin{eqnarray*}
%\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( \delta^n + (1 - \delta^n) \frac{r}{r+\lambda_F} \right )^2  + \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
%\end{eqnarray*}
%where $\kappa(S_r) = \frac{(r+L_F)}{(r+\lambda_F)}(1 - \delta^n).$ 
%\end{corollary}
%\begin{proof} 
%We have that independent of $E_2$, we have that with $\omega = 1/\gamma$, 
%\begin{eqnarray*}
%E_1 = E_k^H - \omega (I - (I - \gamma A_r)A_r^{-1} (H_* - H_k) = E_k^H - \omega \gamma (H_* - %H_k) = 0 
%\end{eqnarray*}
%This completes the proof. 
%\end{proof} 

\section{Conclusion}\label{con} 
We have provided a linear convergence of FL under the condition that the local solve is done accurate enough. Or if we provide a single GD step, then the convergence is also achieved as well. But, it is unclear yet how many GD step is desired to achieve the optimal rate. 


\bibliographystyle{plain}
\bibliography{mybib}
\end{document} 




For the convergence measure, we introduce the so-called Lyapunov function:  
\begin{equation} 
\Psi_k := \|Kz_k - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2.
\end{equation} 
We further define
\begin{subeqnarray}
w_k &=& Kz_k - A (Kz_k) \\ 
w_* &=& Kz_* - A (X_*). 
\end{subeqnarray}
We note that since $\omega = 1/(\gamma n)$ and $p = 1/n$. $\omega^2 = p^2/\gamma^2$. Thus, the weights in the Lyapunov function is nothing else than, 
\begin{equation} 
\omega^2 \Psi_k := \omega^2 \|Kz_k - X_*\|^2 + \|H_k - H_*\|^2.
\end{equation}

Under these settings, we shall then show that ProxSkip generates iterates 
$\{Kz_t\}_{t = 1,\cdots}$ converges linearly in the sense that 
\begin{equation}
\mathbb{E}(\Psi_T) \leq (1 - \zeta)^T \Psi_0, 
\end{equation}
where $\zeta = \gamma \lambda_F$. Here $\gamma = 1/L_F$. 
%Given $\psi : \Reals{d} \mapsto \Reals{}$, we define $\psi^*(y) := \sup_{x \in \Reals{d}} \{ \langle x, y \rangle - \psi(x)\}$ to be its Fenchel conjugate.  
%
%The proximity operator of $\psi^*$ satisfies for any $\tau > 0$. 
%\begin{equation} 
%u = {\rm prox}_{\tau \psi^*} (y) \mbox{ implies } u \in y - \tau %\partial \psi^*(u). 
%\end{equation} 
\begin{theorem} 
For $\gamma > 0$ and $0 < p \leq 1$, we have 
\begin{equation} 
\mathbb{E}(\Psi_{k+1}) \leq (1 - \min \{ \gamma \lambda_F, p^2 \}) \Psi_k. 
\end{equation} 
where the expectation is taken over the $\theta_t$ in the Algorithm 1. 
\end{theorem}  
\begin{proof} 
We let $Kz = P_Z(X)$.   
\begin{equation}
X := X_{k+1} - \frac{\gamma}{p} H_k \quad \mbox{ and } \quad Y = X_* - \frac{\gamma}{p} H_*. 
\end{equation}
Then since $P_Z H_k = 0$ and $P_Z H_* = 0$, we have 
\begin{equation} 
X_* = P_Z(Y). 
\end{equation} 
The method reads as follows: 
\begin{equation}
Kz_{k+1} = \left \{ \begin{array}{ll} P_Z(X) & \mbox{ with probability } p \\ X_{k+1} & \mbox{ with probability } 1 - p \end{array}  \right. 
\end{equation}
Furthermore, we have that 
\begin{equation}
H_{k+1} = H_k + \frac{p}{\gamma} (Kz_{k+1} - X_{k+1}) = 
\left \{ \begin{array}{ll} 
H_k + \frac{p}{\gamma} \left ( P_Z(X_{k+1}) - X_{k+1} \right ) & \mbox{ with  } p \\ H_k & \mbox{ with } 1 - p \end{array}  \right. 
\end{equation}
Now, we compute the expected value of the Lyapunov function 
\begin{equation}
\Psi_k := \|Kz_k - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 
\end{equation}
at the time step $k+1$, with respect to the coin toss at iteration $k$, which is 
\begin{eqnarray}
\mathbb{E}(\Psi_{k+1}) &=& p \left ( \|P_Z(X_{k+1}) - X_*\|^2 + \frac{\gamma^2}{p^2} \left \|H_k + \frac{p}{\gamma} (P_Z(X_{k+1}) - X_{k+1})  - H_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&=& p \left ( \|P_Z(X) - P_Z(Y) \|^2 + \left \| \frac{\gamma}{p} H_k + (P_Z(X) - X_{k+1})  - \frac{\gamma}{p} H_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{t+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&=& p \left ( \|P_Z(X) - P_Z(Y) \|^2 + \left \|P_Z(X) - (X_{k+1} - \frac{\gamma}{p} H_k )  + (Y - X_*) \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&=& p \left ( \|P_Z(X) - P_Z(Y) \|^2 + \left \|Q_Z(X) - Q_Z(Y) \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&\leq& p \left \|\left ( X_{k+1} - \frac{\gamma}{p} H_k \right ) - \left ( X_* - \frac{\gamma}{p} H_* \right ) \right \|^2 \\
&& \quad + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\ 
&=& \|X_{k+1} - X_* \|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 - 
2 \gamma \langle X_{k+1} - X_*, H_k - H_* \rangle \\ 
&=& \|(X_k - \gamma A X_k) - (X_* - \gamma AX_*)\|^2 -  \gamma^2 \|H_k - H_*\|^2 \\
&& \quad + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 \\
&\leq& (1 - \gamma \lambda_F) \|X_K - X_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 \\
&\leq& \max \{ (1 - \gamma \lambda_F), (1 - p^2) \} \Psi_k \\
&=& (1 - \min \{ \gamma \lambda_F, p^2 \} ) \Psi_k. 
\end{eqnarray}
This completes the proof. 
\end{proof} 
\begin{remark}
This results in the choice of $p$, should be not too small. This is misleading since the Gauss-Seidel iteration means that it uses a lot of iterations. But, it can lead to the convergence of the algorithm. We now ask if we can replace GD 1step by GS method. 
\end{remark} 

\end{comment} 

%\begin{remark} 
%Since we have that with $\omega = \frac{2}{r + L_F + r + \lambda_F}$, 
%\begin{eqnarray*}
%\lambda_{max}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+L_F} = \frac{r + L_F - \omega}{r + L_F} \\  
%\lambda_{min}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+\lambda_F} = \frac{r %+ \lambda_F - \omega}{r + \lambda_F}.
%\end{eqnarray*}
%Thus, we have that
%\begin{equation}
%\frac{\kappa(A_1) - 1}{\kappa(A_1) + 1} = \frac{ \frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} - 1}{\frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} + 1} =\frac{ \frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} - 1}{\frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} + 1} = \frac{\frac{r + \lambda_F}{r + L_F}  - \frac{\lambda_F}{L_F}}{\frac{r + \lambda_F}{r + L_F} + \frac{\lambda_F}{L_F}}  
%\end{equation} 
%and 
%\begin{equation}
%\frac{\kappa(A_2) - 1}{\kappa(A_2) + 1} = \frac{ \frac{r + \lambda_F}{r + L_F} - 1}{\frac{r + \lambda_F}{r + L_F}  + 1}  
%\end{equation}
%On the other hand, we have that 
%\begin{equation}
%\sin (\theta_1 + \theta_2) = 
%\end{equation}
%and $\lambda_{min} = 1 - \frac{\omega}{r + \lambda_F} = \frac{\lambda_F}{r + \lambda_F}$ while $H_{A_r} = 1/(r + L_F)$ and $1/(r + \lambda_F)$. Therefore, if $r$ is sufficiently large, then $\sin\theta$ is quite small. 
%\begin{theorem}
%For $A_1$ and $A_2$, symmetric positive definite matrices, we have 
%\begin{equation}
%(x,A_1A_2y) \leq \sin (\theta_1 + %\theta_2) \|A_1x\|\|A_2y\|,  
%\end{equation}
%where $\sin \theta_1 = \frac{\kappa(A_1) - 1}{\kappa(A_2) + 1}$ and $\sin \theta_2 = \frac{\kappa(A_2) - 1}{\kappa(A_2) + 1}$. 
%\end{theorem}
%\end{remark}

\begin{comment} 
\subsubsection{Convergence analysis of Algorithm \ref{algADMM3} with $G_{n,r} = A_r^{-m}$}
In this section, we shall discuss the case when the exact solver for $A_r$ is applied a number of times, say $m$. The main motivation behind this investigation is to use the action of the parameter $r$. 

\begin{theorem}\label{main:theorem04} 
The Algorithm \ref{algADMM3} with $G_{n,r} = A_r^{-m}$ produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 + \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM3} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^{-m}(H_k + rKz_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ). 
\end{eqnarray*}
We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z = K(K^TK)^{-1}K$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& A_r^{-m}(H_k + rKz_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T A_r^{-m}(H_k + rKz_k)) = P_Z [A_r^{-m} (H_k + rKz_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{-m} (H_*+rKz_*) \\
Kz_{*} &=& P_Z [A_r^{-m}(H_*+rKz_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{-m} (H_* + rKz_*) - A_r^{-m}(H_k + rKz_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{-m} (H_*+rKz_*) - A_r^{-m}(H_k + rKz_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH2}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{-m} (H_*+rKz_*) - A_r^{-m}(H_k +rKz_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH2}, and using the orthogonality between $E_i^H$ and $E_j^Z$ for all $i,j$, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{-m}(H_*+rKz_*) - A_r^{-m}(H_k+rKz_k))\|^2. 
\end{eqnarray*}
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^{-m}(H_k + rKz_*) - A_r^{-m}(H_k + rKz_k). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{-m} (H_* + r K z_*) - A_r^{-m} (H_k + r K z_k))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^{-m}} = 1/(r + L_F)^m$ and $L_{G^{-m}} = 1/(r + \lambda_F)^m$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{(r + L_F)^m} + \frac{1}{(r + \lambda_F)^m}} = \frac{2 (r + \lambda_F)^m(r + L_F)^m}{(r + L_F)^m + (r + \lambda_F)^m}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right ) \frac{r}{(r+\lambda_F)^m} \|E_k^H\|\|E_k^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right ) \frac{r}{(r+\lambda_F)^m}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^{-m}} + L_{G^{-m}})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^{-m}(H_k + r Kz_*) - A_r^{-m} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{(r+\lambda_F)^m} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 + \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\end{comment} 

\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}

We remark that 
\begin{equation}
\frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} = \frac{\frac{(r+L_F)^m}{(r+\lambda_F)^m} - 1}{\frac{(r+L_F)^m}{(r+\lambda_F)^m}+1} = \frac{(r+L_F)^m - (r+\lambda_F)^m}{(r+L_F)^m + (r+\lambda_F)^m}. 
\end{equation} 
This provides the convergence rate that speeds up with the larger $r$ now. 
\end{proof} 


\end{comment} 

\bibliographystyle{plain}
\bibliography{mybib}
\end{document} 

\section{appendix} 

With $\mathcal{M}_r$, 
\begin{eqnarray*}
\mathcal{M}_r = \begin{pmatrix} 
(\mathcal{A}_r^{-1} - \mathcal{R})\mathcal{A}_r & (\mathcal{A}_r^{-1} - \mathcal{R})\mathcal{B}^T - \mathcal{A}_r^{-1} \mathcal{B}^T \\
\omega \mathcal{B} (\mathcal{A}_r^{-1} - \mathcal{R}) \mathcal{A}_r & (I - \omega \mathcal{B} \mathcal{A}_r^{-1} \mathcal{B}^T) + \omega \mathcal{B}(\mathcal{A}_r^{-1} - \mathcal{R}) \mathcal{B}^T 
\end{pmatrix}, 
\end{eqnarray*}
we have that 
{\small{\begin{eqnarray*}
\mathcal{M}_r^T \mathcal{M}_r &=& \begin{pmatrix} 
I - A R^T & \omega (I - AR^T) B^T \\
- B R^T & I - \omega BR^TB^T  
\end{pmatrix} \begin{pmatrix} 
I - R A & - RB^T \\
\omega B(I - R A) & I - \omega BRB^T  
\end{pmatrix} \\ 
&=& \begin{pmatrix} 
(I - A R^T)(I - RA) + \omega^2 (I - AR^T) B^TB(I - RA) & - (I - AR^T)RB^T + \omega (I - AR^T) B^T ( I - \omega BRB^T) \\
- BR^T (I - RA) + \omega (I - \omega BR^TB^T)B(I - RA) & BR^T RB^T + (I - \omega BR^TB^T)(I - \omega BR B^T)
\end{pmatrix} \\
&=& 
\begin{pmatrix}
M_{11} & M_{12} \\
M_{21} & M_{22}
\end{pmatrix} \\
&\approx& \begin{pmatrix} 
(I - A R^T)(I - RA) + \omega^2 (I - AR^T) B^TB(I - RA) & 0 \\
0 & M_{22} - M_{21} M_{11}^{-1} M_{12} 
\end{pmatrix}. 
\end{eqnarray*}}}
Thus, we shall investigate the following:
\begin{equation}
\lambda_{min}(M_{11}) \leq M_{11} \leq \lambda_{max}(M_{11}). 
\end{equation} 

Spectrum can be obtained by the Schur complement.



\subsection{Note to reminder} 

\textcolor{red}{Jinchao's Suggestion : 
We define 
\begin{equation}
S_r^n = B R_n B^T 
\end{equation}
Then, we have with $g = BA_r^{-1} f$,  
\begin{subeqnarray*}
H_{k+1} &=& H_k + \omega ( g - S_r^n H_k) \\ 
H_* &=& H_* + \omega (g - S_r H_*). 
\end{subeqnarray*}
If $R = A_r^{-1}$, then, it is true that 
$$xxx 
U_{k+1} = U_k + R (f - B^T H_k - A_r U_k) = A_r^{-1}f - A_r^{-1} B^T H_k. 
$$
Thus, we have that 
$$
H_{k+1} = H_k + \omega (BA_r^{-1} f - S_r H_k). 
$$
On the other hand, if $R \neq A_r^{-1}$, then we have that
$$
H_{k+1} = H_k + \omega B U_{k+1} = H_k + \omega (BRf + B(I - RA_r)U_k - BR B^T H_k).  
$$
Therefore, we will have a different $g$ here. 
}

\section{Exact Uzawa Derivation}
The Exact Uzawa is given as follows:  
\begin{subeqnarray*} 
\mathcal{A}_r U_{k+1} &=& f - \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
This is equivalent to say that 
\begin{subeqnarray*} 
U_{k+1} &=& U_k + \mathcal{A}_r^{-1} ( f - \mathcal{B}^T H_k - \mathcal{A}_r U_k) = (I - \mathcal{A}_r^{-1} \mathcal{A}_r)U_k + \mathcal{A}_r^{-1} (f - \mathcal{B}^TH_k) \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
The last equation is due to the fact that
\begin{equation}
\mathcal{B} \mathcal{A}_r^{-1} f- \mathcal{S}_r H_k = \mathcal{B} \mathcal{A}_r^{-1} (f - \mathcal{B}^T H_k - (I - \mathcal{A}_r^{-1} \mathcal{A}_r) U_k) = \mathcal{B}U_{k+1}.  
\end{equation} 
\begin{remark}
The operator $\mathcal{A}_r$ will be directly inverted and thus, independent of $k$, the iterate count, the action of $\mathcal{A}_r$ does not require any initial condition such as $U_k$.  
\end{remark}

\section{Inexact Uzawa Derivation} 
The inexact Uzawa is to introduce $\mathcal{R} \approx \mathcal{A}_r^{-1}$, but, it is dependent on $U_k$,   
\begin{subeqnarray*} 
\mathcal{R}_k^{-1} U_{k+1} &=& f - \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
This is equivalent to say that 
\begin{subeqnarray*} 
U_{k+1} &=& U_k + \mathcal{R}( f - \mathcal{B}^T H_k - \mathcal{A}_r U_k) = (I - \mathcal{R} \mathcal{A}_r)U_k + \mathcal{R} (f - \mathcal{B}^TH_k) \\ 
H_{k+1} &=&  \cancel{H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k)} \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
The last equation leads to 
\begin{equation}
H_{k+1} = H_k + Q_B \mathcal{B} U_{k+1} = H_k + Q_B \left [  \mathcal{B} ( (I - \mathcal{R}\mathcal{A}_r)U_k + \mathcal{R}(f - \mathcal{B}^T H_k) \right ]. 
\end{equation} 

Another derivation of the inexact uzawa is to introduce a approximate solver $\mathcal{R} \approx \mathcal{A}_r^{-1}$, which is assumed to be linear:   
\begin{subeqnarray*} 
\mathcal{R}^{-1} \overline{U} + \mathcal{B}^T \overline{H} &=& f \\ 
\mathcal{B} \overline{U} &=& 0. %H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
%&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
Namely, we consider an approximate equation: 
\begin{equation} 
\overline{U} = \mathcal{R} ( f - B^T \overline{H}) \quad \mbox{ and } \quad B \mathcal{R} B^T \overline{H} = \mathcal{R} f.  
\end{equation} 
We then consider the Uzawa for the new system:
\begin{subeqnarray*} 
\mathcal{R}^{-1} \overline{U}_{k+1} &=& f - \mathcal{B}^T \overline{H}_k \\ 
\overline{H}_{k+1} &=& \overline{H}_k + Q_B ( \mathcal{B} \mathcal{R}f - \mathcal{S}^n_r \overline{H}_k) \\
&=& \overline{H}_k + Q_B \mathcal{B} \overline{U}_{k+1}.  
\end{subeqnarray*}
Thus, the error equation is given as follows: 
\begin{eqnarray*}
U_{k+1} - \overline{U}_{k+1} = \mathcal{A}_r^{-1} (f - \mathcal{B}^T H_k) - \mathcal{R} (f - \mathcal{B}^T \overline{H}_k) = (\mathcal{A}_r^{-1} - \mathcal{R})f 
\end{eqnarray*}
Therefore, we have
\begin{equation}
B \mathcal{R} B^T (H- \overline{H}) = (\mathcal{A}_r^{-1} -  R)f - B (\mathcal{A}_r^{-1} - R) B^TH. 
\end{equation} 
On the other hand, we have that
\begin{eqnarray*}
U - \overline{U} &=& \mathcal{A}_r^{-1} ( f - B^T H) - R (f - B^T \overline{H} ) = (\mathcal{A}_r^{-1} - R)f - \mathcal{A}_r^{-1} B^T H + R B^T \overline{H} \\
&=& (\mathcal{A}_r^{-1} - R)f - \mathcal{A}_r^{-1} B^T (H - \overline{H}) + (R - \mathcal{A}_r^{-1} )B^T \overline{H}. 
\end{eqnarray*}








\begin{subeqnarray*} 
U_{k+1} &=& \overline{\mathcal{R}}( f - \mathcal{B}^T H_k ) = \overline{\mathcal{R}} f - \overline{\mathcal{R}} \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}

\end{document} 


This is equivalent to say that 
\begin{subeqnarray*} 
U_{k+1} &=& U_k + \mathcal{R}( f - \mathcal{B}^T H_k - \mathcal{A}_r U_k) = (I - \mathcal{R} \mathcal{A}_r)U_k + \mathcal{R} (f - \mathcal{B}^TH_k) \\ 
H_{k+1} &=& \textcolor{red}{Skip this !} H_k + Q_B ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + Q_B \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
The last equation leads to

\end{document} 

\section{Convergence analysis of  Algorithm \ref{algADMM1}}

In this section, we shall present the convergence of the Algorithm \ref{algADMM1}. This section consists of two parts. One part is the convergence when $D_r^* = A_r^*$ and the other part is the convergence when $D_r^*$ is the $N-$step GD method. 

\subsection{Linear Convergence of Algorithm \ref{algADMM1} for $D_r = A_r$}

In this section, we shall establish the linear convergence of the exact ADMM method with $D_r = A_r$.
\begin{comment} 
\begin{lemma}
The following holds true: for all $Y, Y_k$ in $\Reals{N_x}$, 
\begin{eqnarray}
\|A_r^*(Y) - A_r^*(Y_k) \| &\leq& \frac{1}{r + \lambda_F} \|Y- Y_k\|, \\ 
\|(I - \omega A_r^*) (Y) - (I - \omega A_r^*) (Y_k)   \| &\leq& \left(1 - \frac{\omega}{r + L_F} \right) \|Y -Y_k \|
\end{eqnarray}
\end{lemma}
\begin{proof}
    Note that $A_r^*(Y)$ corresponds to the gradient of the function $F_r^*(Y)$ that is the dual of $F_r(X) = F(X) + \frac{r}{2} \| X\|^2$. 
    $F_r^*$ is $\frac{1}{r +L_F}$-strongly convex and $\frac{1}{r + \lambda_F}$-smooth since $F_r$ is $(r + \lambda)$-strongly convex and $(r + L_F)$-smooth.
    Therefore, we have 
    \begin{equation}
        \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|. 
    \end{equation}

For $ (I -  \omega A_r^*)(Y)$, we have
 that the 
 \begin{equation}
 \begin{aligned}
    \|  I - \omega \nabla^2F^*_r(Y) \| 
 = \rho ( I - \omega \nabla^2F^*_r(Y)) & \leq \max \left\{|1 - \omega \lambda_{min}(\nabla^2 F_r^*(Y))|, | 1-\omega \lambda_{max} \nabla^2 F_r^*(Y)| \right\} \\
 & \leq \max \left\{  |1 - \frac{\omega}{r + L_F} |, | 1-\frac{\omega}{r + \lambda_F}| \right\}
 \end{aligned}
 \end{equation}
 where $\omega < \frac{2}{ \frac{1}{r + \lambda_F}}$ for any $Y$. 
 For optimal $\omega$ is $\frac{2}{ \frac{\omega}{r + \lambda_F}+ \frac{\omega}{r + L_F}}$, which leads to 
 \begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa + 1} \|Y - Y_k \|,
\end{equation}
where $\kappa = \frac{r+L_F}{r+\lambda_F}$. 

In other analysis where we restrict $\omega \leq \frac{1}{ \frac{1}{r + \lambda_F} }$, we have the following optimal bound
\begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa} \|Y - Y_k \|.
\end{equation}
\end{proof}
\begin{remark}
    This Hessian argument always requires $L$-smoothness of the objective thanks to the mollifier argument. See Lemma \ref{lemmaGD}.
\end{remark}
\end{comment} 

% \begin{proof}
% Let $X = A_r^*(Y)$, $X_k = A_r^*(Y_k)$, that is $Y  = A_r(X)$, $Y_k = A_r(X)$
% \begin{eqnarray*}
% (r+\lambda) \|X - X_{k}\|^2 &\leq& \langle X - X_{k+1}, A_r(X) - A_r(X_{k+1}) \rangle \\
% &=& \langle X - X_k, Y - Y_k \rangle \\ 
% &\leq& \|X - X_k\|  \|Y - Y_k\|. 
% \end{eqnarray*}
% Hence, 
% \begin{eqnarray*}
% \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|.   
% \end{eqnarray*}
% This completes the proof.
% \end{proof}

\begin{theorem}\label{thm:GS}
The Algorithm \ref{algADMM1} with $D_r = A_r$ and $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ has the convergence rate given as follows: 
\begin{eqnarray}\label{gsrate}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \rho^2_{GS}(r,L_F,\lambda_F) \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ), 
\end{eqnarray}
where with $\kappa(G) = \frac{r+L_F}{r + \lambda_F}$, 
\begin{equation} 
\rho^2_{GS}(r,L_F,\lambda_F) =  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2. 
\end{equation} 
\textcolor{red}{The first one is from error in Schur Complement and the second one is from error between exact $X$ and the $X_k$. The expression is from the case when GS is used. I am thinking how it will be like for linear analysis.}


We also have that 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ). 
\end{eqnarray*}
Furthermore, there exists a single optimal $r > 0$ which gives the optimal convergence rate and the convergence factor is always smaller than one for all $r \geq 0$. 
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM1} produces iterates given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^* (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $A_r^*$ is the Fenchel-dual conjugate of $A_r$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. Now due to Lemma \ref{main:lem1}, we have that with $D_r = A_r$,  
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 %\\   
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\begin{comment} 
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{*} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{*}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ]. 
\end{eqnarray*}
The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] \right ). 
\end{eqnarray*}
Lastly, for $H$, we have 
\begin{eqnarray*}
H_{*} - H_{k+1} &=& H_* - H_k + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
&=& H_* - H_k - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
&=& H_* - H_k - \omega [ A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
&& - P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] ] \\
&=& H_* - H_k - \omega Q_Z (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)) \\ 
&=& Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))] 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
H_{*} - H_{k+1} - \omega (Kz_* - K z_{k+1}) = H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)). 
\end{eqnarray*}
\end{comment} 
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^*(H_k + rKz_*) - A_r^*(H_k + rKz_k). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^*} = 1/(r + L_F)$ and $L_{G^*} = 1/(r + \lambda_F)$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{r + L_F} + \frac{1}{r + \lambda_F}} = \frac{2 (r + \lambda_F)(r + L_F)}{2r + L_F + \lambda_F}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F} \|E_k^H\|\|E_k^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{r + \lambda_F} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^*} + L_{G^*})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^*(H_k + r Kz_*) - A_r^{*} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r + \lambda_F} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}
\end{comment} 
This provides the convergence rate. Finally, we notice that for all $r \geq 0$, 
\begin{eqnarray*}
\frac{r^2}{\omega^2} = \frac{r^2 (2r + L_F + \lambda_F)^2}{4(r + \lambda_F)^2(r + L_F)^2} < 1. 
\end{eqnarray*}
Thus, we obtain that due to the orthogonality, 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 &=& \|A_r^*(H_* + rKz_*) - A_r^*(H_k + r K z_k) \|^2 \\
&\leq& \frac{1}{(r + \lambda_F)^2}\|E_k^H - r E_k^Z\|^2 = \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + r^2 \|E_k^Z\|^2 \right ) \\
&=& \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \frac{r^2}{\omega^2} \|E_k^Z\|_\omega ^2 \right ) \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) 
\end{eqnarray*}
We now discuss the convergence factor denoted by $\rho_{GS}^2$ and given by 
\begin{equation}
f(r) = \rho^2_{GS}(r, L_F, \lambda_F) = \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 = \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 
\end{equation}
We note that the simple calculation shows that the derivative of $f(r)$ is given as follows: 
\begin{equation}
f'(r) = \frac{-4(L-\lambda)^2}{(2r + L_F + \lambda_F)^3} + \frac{2r \lambda_F}{(r + \lambda_F)^3}. 
\end{equation}
Therefore, for small $r$, it takes the negative sign, but it changes its sign for larger $r$ and continues to be positive. Thus, there exists a single critical point, which gives the optimal $r_{\rm opt}$. On the other hand, it is continuously increasing (see Figure \ref{exam}). Since $\lim_{r \rightarrow \infty} f(r) = 1$ and thus, for any $r \geq 0$, the convergence factor is smaller than one. Thus, it converges linearly for all fixed $r \geq 0$. This completes the proof.
\end{proof} 
\begin{figure}[h]
\centering
\includegraphics[width=12cm,height=6cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
\begin{comment} 

\begin{remark}[Convergence rate when $\omega = r$]
In our current convergence analysis, we require $\beta \in \left( 0,\frac{1}{2} \right )$. We shall make a more precise convergence analysis as follows.
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{ L_F r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{L_F }{r + L_F} \right )^2 \right ) &=& \left ( \frac{ L_F  r^{2\alpha} (r + L_F)}{(r + L_F)^2(r + \lambda_F)} + \frac{L_F^2 (r+ \lambda_F) }{(r + L_F)^2 (r + \lambda_F)} \right ). 
%&& \Longleftrightarrow ((r + L)(r + \lambda) + L r^2 ) L^2 < (r + L)^3 (r + \lambda). 
\end{eqnarray*} 
The convergence rate in $z$-error is given by 
\begin{eqnarray*}
    \left ( \frac{L_F r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) &=&    \left ( \frac{L_F }{(r^{1 - 2 \beta} + L_F/r^{2\beta})(r + \lambda_F)} + \left ( \frac{1}{1 + \lambda_F/r} \right )^2 \right )   
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, we have that 
\begin{equation}
\frac{L_F }{(r^{1 - 2 \beta} + L_F r^{-2\beta})(r + \lambda_F)} = \frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta} + \lambda_F L_F r^{-2\beta}}    
\end{equation}
and we have 
\begin{equation}
    \left ( 1 + \lambda_F/r \right )^{-2}  \leq 1 - \frac{2 \lambda_F}{r} + \frac{3 \lambda_F^2}{(1 -\varepsilon)^4 r^2},  
\end{equation}
since it holds that 
\begin{eqnarray*}
(1 + x)^{-2} &=& 1 - 2x + \frac{3}{ (1 + \xi )^4} x^2, \quad x \in (-\varepsilon,1), \quad \xi \in[-\varepsilon, x] \\ 
&\leq&   1 - 2x + \frac{3}{ (1 -\varepsilon)^4} x^2 \end{eqnarray*}
Thus, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}

\textbf{Case 2: $r \ll 1$}.
For error in $H$, the convergence rate is bounded below by $\left( \frac{L_F}{r + L_F} \right)^2$. The convergence in error $H$ deteriorates as $r \to 0$. 

For error in $z$, the convergence rate can be made arbitrarily small if $r$ is sufficiently small. 

\textbf{Therefore, we conclude that when $\omega = r$. For sufficiently large $r$, the convergence in $H$ error becomes better while the convergence in $z$ error deteriorates when $r$ becomes larger. The asymptotic rate in $z$ error is $1 - \frac{2 \lambda_F}{r}$. 
On the other hand, for small $r \ll 1$, the convergence in $H$ deteriorates while the convergence in $z$ error becomes better. }
\end{remark}

\begin{remark}[Convergence rate when $\omega = r + \lambda_F$]
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{(L_F - \textcolor{red}{\lambda_F})}{r + L_F} \right )^2 \right ) 
\end{eqnarray*} 

The convergence rate in $z$-error is given by 
\begin{eqnarray*} 
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) 
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
The analysis is similar to that when $\omega = r$.

For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F - \lambda_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}
The asymptotic convergence rate is $1 - \frac{2\lambda_F}{r}$. 

\textbf{Case 2: $r \ll 1$}.
For $H$ error, the convergence rate is bounded below by $\left( \frac{L_F - \lambda_F}{r + L_F} \right)^2 $. Asymptotically, it is $\left( \frac{L_F - \lambda_F}{L_F} \right)^2$.

For $z$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently small.
\end{remark}

\begin{remark}
    In fact, from standard asymptotic analysis, one should always choose $\omega > r$, say, $\omega = r + \lambda_F$, with sufficiently small $r$.
\end{remark}
\end{comment} 

% \begin{remark}
% We note that the larger the $r$, the convergence of $H_k$ to $H_*$ seems to be faster while the convergence of $Kz_k$ to $Kz_*$ deteriorates.% On the other hand, the larger $r$ guarantees the sufficiently near orthogonality for the cross terms.  
% \end{remark}
%\begin{remark}
%It is interesting to note that if $\omega = r + L_F$, then the cross term disappears. Thus, the error analysis reduces to the following form: 
%\begin{eqnarray*}
%\|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} &\leq& \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - K z_k\|^2_{\omega} \\ 
%&=&  \left ( \frac{r}{r + \lambda_F} \right )^2 \left ( \|H_{*} - H_{k}\|^2 + \|Kz_* - K z_k\|^2_{\omega} \right %).
%\end{eqnarray*}
%\end{remark} 

\subsection{Analysis based on $E_k^X$ and the convexity of $F$ due to \cite{shi2014linear}} 
In this section, we shall adapt the proof originated from \cite{shi2014linear} to establish the result of linear convergence.
\begin{lemma} 
The algorithm \ref{algADMM1} produces iterates $(X_k,z_k,H_k)$ such that the following identity holds: for all $k=0,1,2\cdots$, 
\begin{eqnarray*}\label{equal form 2}
\nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &=& 0, \\
H_{k+1} - H_{k} + r \left( I - P_Z \right) X_{k+1} &=& 0,\\
K z_{k+1} - P_Z X_{k+1} &=& 0.
\end{eqnarray*}
\end{lemma} 
\begin{proof} 
First of all, we note that $K^TH = 0$ with $H = H_k$ or $H = H_*$ and also 
\begin{equation}
Kz = P_Z X. 
\end{equation}
Now, subtracting \eqref{H update} from \eqref{X update}, multiplying $K^T$ to \eqref{H update} and adding it to \eqref{z update}, we have
\begin{equation}\label{equal form 1}
    \begin{split}
        \nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &= 0, \\
        K^T H_{k+1} &= 0, \\
        H_{k+1} - H_{k}  - r (K z_{k+1} - X_{k+1}) &= 0. 
    \end{split}
\end{equation}
Multiplying the third equation in \eqref{equal form 1} by $K^T$ and using the second equation, we complete the proof. 
\end{proof}





We shall obtain a simple but important lemma: 
\begin{lemma} 
The following identity holds: 
\begin{eqnarray}
\nabla F(X_{k+1}) - \nabla F(X_*) &=& r K (z_{k} - z_{k+1}) + H_{k+1} - H_*  \label{difference1}\\
H_{k+1} - H_k &=& - r Q_Z (X_{k+1} - X_*)  \label{difference2} \\
K(z_{k+1} - z_*) &=& P_Z (X_{k+1} - X_*) \label{difference3}    
\end{eqnarray}
\end{lemma} 
\begin{proof} 
We recall the optimality condition, which can be given as follows:
\begin{subeqnarray*}
\nabla F(X_*) - H_* &=& 0 \\ 
K^T H_* &=& 0 \\
K z_* - X_* &=& 0 
\end{subeqnarray*}
However, using the property of $Q_Z$ and $P_Z$, the optimality condition implies that it holds true 
\begin{subeqnarray}\label{KKT}
0 &=& P_Z (K z_* - X_{*}) = K z_* - P_Z X_* \\ 
0 &=& Q_Z (Kz_* - X_*) = - Q_Z X_*. 
\end{subeqnarray}
Subtracting the optimality conditions \eqref{KKT} from \eqref{equal form 2}. This completes the proof. 
\end{proof} 
%\begin{proposition}
%If each $F_i$ is $\lambda_i$-strongly convex, and $L_i$-smooth, then $F(X) = \frac{1}{n}\sum_{i = 1}^n F_i(x_i)$ is $\lambda$-strongly convex and $\frac{M_f}{n}$-smooth, where $\lambda = \frac{\min_{i} m_i}{n}$, $M_f = \max_i M_i$.  
%\end{proposition}
The main theorem considers the convergence of a vector $Y$ that combines the primal variable $Kz$ and the dual variable $H$,
\begin{equation}
Y = \begin{pmatrix}
Kz \\
H
\end{pmatrix} \quad \mbox{ and } \quad C = \begin{pmatrix}
r I & 0 \\
0 & \frac{1}{r} I 
\end{pmatrix}
\end{equation}
We also define a $C-$norm on $Y = (y_1,y_2)^T$ by 
\begin{eqnarray*}
\|Y\|_C^2 &=& (CY, Y) = r \|y_1\|^2 + \frac{1}{r}\|y_2\|^2 \\ 
&=& r (Kz, Kz) + \frac{1}{r} (H, H) = (rK^TK z, z) + \frac{1}{r}(H,H).   
\end{eqnarray*}

We shall need the following lemma: 
\begin{lemma}
We have the following identity:
\begin{eqnarray*}
2 \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \|Y_{k} - Y_*\|^2 - \|Y_{k+1} - Y_* \|^2 - \|Y_{k+1} - Y_k\|^2. 
\end{eqnarray*} 
\end{lemma} 
\begin{proof} 

\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\ 
&=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\
&=&  \langle Y_{k} - Y_*, Y_{k+1} - Y_* \rangle - \| Y_{k+1} - Y_* \|^2
\end{eqnarray*} 
On the other hand, we have 
\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_k + Y_k - Y_* \rangle \\ 
&=& - \|Y_{k+1} - Y_{k}\|^2 + \langle Y_{k} - Y_{k+1}, Y_k - Y_* \rangle . 
\end{eqnarray*} 
By adding these two identities, we obtain the result. This completes the proof. 
\end{proof}
\begin{theorem}
Assume that $K^TH_0 = 0$. Then the ADMM iterations produces $Y_k = [Kz_k; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [Kz_*; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1+\delta} \| Y_{k} - Y_*\|^2_C,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C
\end{equation}
\end{theorem}
\begin{proof}
We begin with using the $\lambda-$strongly convexity condition for $F$ as follows: 
\begin{eqnarray*}
\lambda \| X_{k+1} - X_* \|^2 &\leq& \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*)\rangle \\
&\leq& \langle X_{k+1} - X_*, r K(z_k - z_{k+1}) \rangle + \langle X_{k+1} - X_*, H_{k+1} - H_* \rangle \\
&=& r \langle X_{k+1} - X_*, P_Z (K(z_k - z_{k+1}))  \rangle + \langle X_{k+1} - X_*, Q_Z(H_{k+1} - H_*) \rangle \\
&=& r \langle  P_Z (X_{k+1} - X_*) , Kz_k - Kz_{k+1}   \rangle + \langle Q_Z (X_{k+1} - X_*), H_{k+1} - H_* \rangle  \\
&=& r \langle Kz_k - Kz_{k+1}, Kz_{k+1} - Kz_*  \rangle + \frac{1}{r} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle \\
&=& (Y_{k} - Y_{k+1})^T C (Y_{k+1} - Y_*),  
\end{eqnarray*}
where 
\begin{equation}
Y_k = \begin{pmatrix} 
      Kz_k \\
      H_k 
      \end{pmatrix}, \quad Y_{k+1} = \begin{pmatrix} 
      Kz_{k+1} \\
      H_{k+1}  
      \end{pmatrix}
,\quad Y_{*} = \begin{pmatrix} 
      Kz_{*} \\
      H_{*}  
      \end{pmatrix}
\end{equation}

and the matrix 
\begin{equation}
    C = \begin{pmatrix}
r I & 0 \\
0 &  \frac{1}{r} I
\end{pmatrix}
\end{equation}
This implies
\begin{equation}
    \lambda  \| X_{k+1} - X_* \|^2 \leq  \frac{1}{2} \|Y_k -Y_* \|^2_C - \frac{1}{2}  \|Y_{k+1} - Y_* \|^2_C -  \frac{1}{2}  \| Y_k - Y_{k+1}\|^2_C. 
\end{equation}
Now, by rearranging terms, we have 
\begin{equation}\label{ineq from strong convexity}
2\lambda \| X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C + \|Y_{k+1} - Y_* \|^2_C \leq  \|Y_k -Y_* \|^2_C.
\end{equation}
This immediately, leads to 
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C.
\end{equation}
Having \eqref{ineq from strong convexity}, it suffices to show for some $\delta > 0$, we have 
\begin{equation}\label{claim}
\delta \|Y_{k+1} - Y_* \|^2_C \leq 2 \lambda \|X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C,
\end{equation}
or equivalently,
\begin{eqnarray*}\label{claimequivalence}
\delta \left( r  \|Kz_{k+1}- Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*} \|^2 \right) &\leq&  2 \lambda \|X_{k+1} - X_* \|^2 \\
&+& r \|Kz_{k}- Kz_{k+1} \|^2 + \frac{1}{r} \|H_k - H_{k+1} \|^2, 
\end{eqnarray*}
which will imply the desired inequality: 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1 +\delta} \|Y_{k} - Y_* \|^2_C.
\end{equation}
To prove the inequality \eqref{claim}, first, we observe that the following holds true:   
\begin{equation}\label{bound on z-z*}
\|Kz_{k+1} - Kz_* \|^2 \leq \|X_{k+1} -X_* \|^2.  
\end{equation}
Further, from \eqref{difference1} and using $L$-smoothness of $F$, we have  
\begin{equation}
\|H_{k+1} - H_* \| \leq r \|K z_k - K z_{k+1} \| + L \| X_{k+1} -  X_*\|
\end{equation}
This implies 
\begin{equation}\label{bound on H-H*}
\begin{split}
\|H_{k+1} - H_*\|^2 & \leq \left(r \|Kz_k - Kz_{k+1} \| + L\|X_{k+1} -  X_*\| \right)^2 \\  & \leq 2 \left( r^2 \| Kz_k - Kz_{k+1}\|^2 + L^2 \| X_{k+1} -  X_*\|^2 \right). 
\end{split}
\end{equation}
Thus, we have 
\begin{equation}
\frac{1}{r} \|H_{k+1} - H_*\|^2 \leq 2 \left( r \| Kz_k - Kz_{k+1}\|^2 + \frac{L^2}{r} \| X_{k+1} -  X_*\|^2 \right).
\end{equation}
Substituting \eqref{bound on z-z*} and \eqref{bound on H-H*} into left hand side of \eqref{claimequivalence} and rearranging, we have 
\begin{eqnarray*}
\delta \left(r\|Kz_{k+1} - Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*}\|^2 \right) &\leq& \delta \left( r + \frac{2L^2}{r} \right ) \| X_{k+1} -  X_*\|^2  \\
&& + 2\delta r \|Kz_k - Kz_{k+1} \|^2 \\
&\leq& 2\lambda \|X_{k+1} - X_* \|^2 + r \|K z_{k}- Kz_{k+1}\|^2 \\
&& + \frac{1}{r}\|H_k - H_{k+1} \|^2,
\end{eqnarray*}
by making $\delta$ sufficiently small such that 
\begin{equation}
\begin{aligned}
   \delta \leq \frac{2\lambda}{ r + \frac{2L^2}{r}} \quad and \quad \delta \leq \frac{1}{2}.
\end{aligned}
\end{equation}
%To maximize $\delta$, one can choose $r = \sqrt{2} L$. Then $\delta \leq \frac{1}{\sqrt{2}} \frac{\lambda}{L}$. 
%\textcolor{red}{I guess this is not of our main interest here !}
This completes the proof. 
\end{proof}
\end{comment} 
%\begin{remark} 
%Since we have that with $\omega = \frac{2}{r + L_F + r + \lambda_F}$, 
%\begin{eqnarray*}
%\lambda_{max}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+L_F} = \frac{r + L_F - \omega}{r + L_F} \\  
%\lambda_{min}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+\lambda_F} = \frac{r %+ \lambda_F - \omega}{r + \lambda_F}.
%\end{eqnarray*}
%Thus, we have that
%\begin{equation}
%\frac{\kappa(A_1) - 1}{\kappa(A_1) + 1} = \frac{ \frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} - 1}{\frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} + 1} =\frac{ \frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} - 1}{\frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} + 1} = \frac{\frac{r + \lambda_F}{r + L_F}  - \frac{\lambda_F}{L_F}}{\frac{r + \lambda_F}{r + L_F} + \frac{\lambda_F}{L_F}}  
%\end{equation} 
%and 
%\begin{equation}
%\frac{\kappa(A_2) - 1}{\kappa(A_2) + 1} = \frac{ \frac{r + \lambda_F}{r + L_F} - 1}{\frac{r + \lambda_F}{r + L_F}  + 1}  
%\end{equation}
%On the other hand, we have that 
%\begin{equation}
%\sin (\theta_1 + \theta_2) = 
%\end{equation}
%and $\lambda_{min} = 1 - \frac{\omega}{r + \lambda_F} = \frac{\lambda_F}{r + \lambda_F}$ while $H_{A_r} = 1/(r + L_F)$ and $1/(r + \lambda_F)$. Therefore, if $r$ is sufficiently large, then $\sin\theta$ is quite small. 
%\begin{theorem}
%For $A_1$ and $A_2$, symmetric positive definite matrices, we have 
%\begin{equation}
%(x,A_1A_2y) \leq \sin (\theta_1 + %\theta_2) \|A_1x\|\|A_2y\|,  
%\end{equation}
%where $\sin \theta_1 = \frac{\kappa(A_1) - 1}{\kappa(A_2) + 1}$ and $\sin \theta_2 = \frac{\kappa(A_2) - 1}{\kappa(A_2) + 1}$. 
%\end{theorem}
%\end{remark}
\begin{remark}
We remark that it is more natural to write the convergence rate in terms of $\kappa(G^*)$, the condition number of $G^*$ since the choice of $\omega$ is made for solving the system relevant to $G^*$. However, it is also fine to use $\kappa(G)$ since it is more relvant to the problem to be solved and we have that
\begin{equation}
\kappa(G^*) = \kappa(G) \quad \rightarrow \quad 
\frac{\kappa(G) - 1}{\kappa(G) + 1} = \frac{\kappa(G^*) - 1}{\kappa(G^*) + 1}. 
\end{equation}
\end{remark} 

\subsection{Convergence analysis of Algorithm \ref{algADMM1} for $D_r^* \neq A_r^*$}\label{gdn} 

In this section, our goal is to establish the convergence of inexact Uzawa with Gauss-Seidel method, $D_r = A_r$, is replaced by inexact local solve for $X$-variable. This includes the $N-$step of Gradient Descent iteration. We recall that the Gauss-Seidel method can be interpreted as to solve the following optimization exactly in $X$-update \eqref{Xupdate} of the Algorithm \ref{algADMM1}:  
\begin{equation}
\min_{X} L_r(X,z_k,H_k).  
\end{equation}
We now let $G(X) = L_r(X,z_k,H_k)$, i.e.,  
\begin{equation}
G(X) = F(X) + \langle H_k, Kz_k - X\rangle + \frac{r}{2}\|Kz_k - X\|^2 
\end{equation}
and let $Y_*  = {\rm arg}\min_X G(X)$. Note that we reserve $X_{k+1}$ as the result of $N-$step of GD method. Then the following statements hold true: 
\begin{enumerate} 
\item $Y_* = A_r^*(H_k + rKz_k)$ 
\item $\nabla F(Y_*) + r Y_* = H_k + rKz_k$ 
\item $\nabla G(Y_*) = 0$. 
\end{enumerate} 
Note that it is easy to show that $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex. Our discussion is for general inexact local solve and the outcome will be denoted by $X_{k+1}$, i.e., 
\begin{equation}
X_{k+1} = D_r^*(H_k + rKz_k). 
\end{equation}
For the convergence analysis, the exact Gauss-Seidel case will be used as an intermediate step. Thus, naturally, the convergence estimate could be made to be sharper. We begin our discussion by introducing two quantities:
\begin{eqnarray*}
\textsf{E}_1 &:=& E_k^H - \omega \left \{A_r^{*}(H_* + rKz_*) - A_r^*(H_k + rKz_k) \right \}\\
\textsf{E}_2 &:=& A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k).  
\end{eqnarray*}
It is evident that $\textsf{E}_1$ is for the error of the Algorithm \ref{algADMM1} with $D_r^* = A_r^*$ and $\textsf{E}_2$ represents the difference between two iterates, one from the inexact solve for $X-$variable and the other from the exact solve. We shall now see that Lemma \ref{main:lem1} can lead to the following estimate easily. 
\begin{eqnarray*}
&& \left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega \{ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \} \right \|^2 \\
&& \qquad \leq \|\textsf{E}_1\|^2 + \omega^2 \|\textsf{E}_2\|^2 + 2 \omega \|\textsf{E}_1\| \|\textsf{E}_2\|. 
\end{eqnarray*}
The first term is relevant to the Algorithm \ref{algADMM1} with $D_r = A_r$. We shall assume that there exists $\delta < 1$ such that 
\begin{equation}\label{main:cvdelta}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ).
\end{equation}
Namely, the inexact solve provides a reasonably close solution to $Y_*$. Under this assumption, we shall establish the convergence of Algorithm \ref{algADMM1}. How small $\delta$ can be, for still allowing the convergence is determined by the close investigation of the convergence of Algorithm \ref{algADMM1} with exact local solve after incorporating $\delta$.   

We are now in a position to provide a main instrumental lemma in this section. 
\begin{theorem}\label{main:ins}
Let $\delta_{GS}$ be the convergence rate for the Algorithm \ref{algADMM1}, when $D_r = A_r$, as given in equation \eqref{gsrate}, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be such that the equation \eqref{main:cvdelta} hold. Then, we have the following estimate:   
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \delta_{GS} + \delta \right )^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{theorem} 
\begin{proof}
We shall let $E^2 = \|E_k^H\|^2 + \|E_k^Z\|_\omega^2$. Due to the equation \eqref{main:cvdelta}, we have that 
\begin{equation}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 E^2. 
\end{equation}
We then obtain that 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 &\leq& \|\textsf{E}_1\|^2 + \|\textsf{E}_2\|_\omega^2 + 2 \|\textsf{E}_1\| \|\textsf{E}_2\|_\omega  \\ 
&=& \delta_{GS}^2 E^2 + \delta^2 E^2 + 2 \delta_{GS} \delta E^2 = (\delta + \delta_{GS})^2 E^2. 
\end{eqnarray*}
%Here we have invoked a simple Cauchy-Schwarz inequality that 
%\begin{equation*}
%ab \leq \frac{1}{2\varepsilon} a^2 + \frac{\varepsilon}{2} b^2, \quad \forall a, b\in \Reals{}.  
%\end{equation*}
%Finally, we observe that $\delta_{GS}$
This completes the proof. 
\end{proof}

We also note that the $N-$step of GD method is basically, given as follows: for a given step size $\gamma > 0$ and $X_k = Kz_k$, 
\begin{subeqnarray}\label{ngd1}
X_{k+\frac{1}{N}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} - \gamma \nabla G(X_{k+\frac{1}{N}}) \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} - \gamma \nabla G(X_{k + \frac{N-2}{N}}) \\
X_{k+\frac{N}{N}} &=& 
X_{k+\frac{N-1}{N}} - \gamma \nabla G(X_{k+\frac{N-1}{N}}), 
\end{subeqnarray}
where $\nabla G$ is given as follows: 
\begin{equation} 
\nabla G(X) = \nabla F(X) + rX - H_k - rKz_k. 
\end{equation}

We shall now present a simple but important lemma: 
\begin{lemma}\label{main:lm} 
Let $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ and $D_r^*(H_k + rKz_k)$ be the $N$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \leq  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \|Y_* - X_k\|^2. 
\end{eqnarray*}
\end{lemma}
\begin{proof}
Let $Y_* = {\rm arg} \min_{X} G(X)$. On the other hand, $X_{k+1}$ is obtained by the following iteration: 
\begin{eqnarray*} 
X_{k+\frac{1}{N}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} - \gamma \nabla G(X_{k+\frac{1}{N}}) \\ 
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} - \gamma \nabla G(X_{k + \frac{N-2}{N}}) \\
X_{k+\frac{N}{N}} &=& 
X_{k+\frac{N-1}{N}} - \gamma \nabla G(X_{k+\frac{N-1}{N}}).  
\end{eqnarray*}
Since $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex, we see that by Lemma \ref{lemmaGD}, we have that 
\begin{eqnarray*}
\|Y_* - X_{k+1}\|^2 &\leq& \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \|Y_* - X_k\|^2. 
\end{eqnarray*}
%
%&\leq& 2 \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \|Y_* - X_*\|^2 + \|X_* - X_k\|^2 \right ) \\
%&\leq& c \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \frac{1}{r + \lambda_F} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2 \right ) + \|X_* - X_k\|^2 \right ). 
%\end{eqnarray*} 
This completes the proof. 
\end{proof}
\begin{remark} 
The choice of $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ is optimal for GD and the convergence rate can be calculated. Since $\kappa(G) = \frac{r + L_F}{r + \lambda_F}$, we have that 
\begin{equation}
\frac{\kappa(G)-1}{\kappa(G)+1} = \frac{L_F - \lambda_F}{2r + L_F + \lambda_F}. 
\end{equation}
The convergence is even faster for large $r$ and large $N$.
\end{remark}
We shall now see that the norm of $\textsf{E}_2$ can be made to be quite small. 
\begin{lemma}
Let $\gamma = \frac{2}{\lambda_G + L_G}$, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be the $N$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 \leq 4 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{lemma}
\begin{proof}
By Lemma \ref{main:lm}, we have that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 &=& \omega^2 \|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \\
&\leq& \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \omega^2 \|Y_* - X_k\|^2. 
\end{eqnarray*}
On the other hand, we have that since $X_k = Kz_k$ and due to Theorem \ref{thm:GS},  
\begin{eqnarray*}
\omega^2 \|Y_* - X_k\|^2 &\leq& 2 \omega^2 \|Y_* - X_*\|^2 + 2 \|X_* - X_k\|_\omega^2 \\ 
&\leq& \frac{2 \omega^2}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|X_* - X_k\|_\omega^2 \\
&\leq& \frac{8(r + L_F)^2}{(2r + L_F + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|E_k^Z\|_\omega^2 \\ 
&\leq& 4 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) \end{eqnarray*} 
The last inequality is due to the fact that $X_k = Kz_k$ and 
\begin{equation}
\omega = \frac{2(r + \lambda_F)(r + L_F)}{(2r + L_F + \lambda_F)},
\end{equation} 
thus, 
\begin{equation}
\frac{2 \omega^2}{(r + \lambda_F)^2} = \frac{8(r+L_F)^2}{(2r + L_F + \lambda_F)^2} \leq 2.  
\end{equation}
This completes the proof. 
\end{proof}
This result gives that if $N$ is large enough, then we can obtain the convergence of $N-$step GD based FL. 


%We shall compute %\begin{equation}
%I - \gamma A_r =^{???} \frac{\kappa(A_r) - 1}{\kappa(A_r)+1} I. 
%\end{equation}



%Therefore, we have that 
%\begin{eqnarray*}
%c_1(r) &=& \sup_{v = (X,z)} \frac{ \left \langle \overline{R}^{-1} \begin{pmatrix} X - r \gamma Kz \\ z \end{pmatrix},\begin{pmatrix} X - r \gamma Kz \\ z \end{pmatrix} \right \rangle }{(A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)} \\
%&=& \sup_{v = (X,z)} \frac{ \langle C (X - r \gamma Kz), (X - r \gamma Kz) \rangle + \langle rK^T K z, z \rangle }{(A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)}, 
%\end{eqnarray*}
%where $C = [\gamma I + \gamma (I - \gamma A_r^{-1})]^{-1}$. We note that if $\gamma 
%\ll 1$, then 


\end{document} 

\section{Gauss-Seidel Solve for the Linear Case for two blocks $U = (X,z)$ and $H$} 

We recall that 
\begin{equation}
\mathcal{A}_r = \begin{pmatrix} 
A_0 + rI & - rK \\ -rK^T & rK^TK 
\end{pmatrix} 
\quad \mbox{ and } \quad 
\mathcal{B} = \begin{pmatrix} 
-I & K \end{pmatrix}. 
\end{equation} 
In this section, we shall consider the Gauss-Seidel solve for the matrix $ \nabla G = \mathcal{A}_r$. We write an equivalent form of the above equation using the Schur complement system. We shall consider applying the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
\mathcal{L} = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad \psi = \mathcal{L}^{-1} = \begin{pmatrix}
A_r^{-1} & 0\\
(rK^T K)^{-1} rK^T A_r^{-1} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + \mathcal{L}^{-1} (f - B^T H_k - \mathcal{A}_r U_k) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that the exact solutions satiafy \begin{eqnarray}
U_{*} &=& U_{*} + \mathcal{L}^{-1} (f - B^T H_{*} - \mathcal{A}_r U_{*}) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray}
Thus, with the convention that $E_{k}^U = U_* - U_k$ and $E_{k}^H = H_* - H_k$, we obtain the error equations: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{L}^{-1} (- \mathcal{A}_r E^U_k - B^T E^H_k) \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray}
In the other direction, we have that 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{L}^{-1} (- \mathcal{A}_r  E^U_k - B^T E^H_k) \\
          &=& E^U_k - \mathcal{L}^{-1} \mathcal{A}_r E^U_k - \mathcal{L}^{-1} B^T E^H_k \\
          &=& (I - \mathcal{L}^{-1} \mathcal{A}_r) E^U_k - \mathcal{L}^{-1} B^T E_k^H \\ 
E^H_{k+1} &=& E^H_k + \omega B ((I - \mathcal{L}^{-1} \mathcal{A}_r) E^U_k - \mathcal{L}^{-1} B^T E_k^H ) \\
&=& (I - \omega B \mathcal{L}^{-1} B^T) E_k^H + \omega B (I - \mathcal{L}^{-1} \mathcal{A}_r)(E_k^U). 
\end{eqnarray} 
In the matrix form, we have 
\begin{equation}
\begin{pmatrix} 
I  & 0 \\
-\omega B & I
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k+1} \\
E^H_{k+1}
\end{pmatrix}  = 
\begin{pmatrix} 
I - \mathcal{L}^{-1} \mathcal{A}_r & - \mathcal{L}^{-1} B^T \\
0 & I
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k} \\
E^H_{k}
\end{pmatrix} 
\end{equation}
Or, we have 
\begin{equation}
\begin{pmatrix} 
E^U_{k+1} \\
E^H_{k+1}
\end{pmatrix}  = 
\begin{pmatrix} 
I - \mathcal{L}^{-1} \mathcal{A}_r & - \mathcal{L}^{-1} B^T \\
\omega B (I - \mathcal{L}^{-1} \mathcal{A}_r) & I - \omega B \mathcal{L}^{-1} B^T
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k} \\
E^H_{k}
\end{pmatrix}.  
\end{equation}

We shall denote $Q_B = 1/\omega$ and noticed that this expression is very similar to what Bramble has in his paper. Following Bramble, we shall assume there are two contraction paratmers, $\delta$ and $\gamma$ which are defined by the following inequality:
\begin{subeqnarray}
\|(\mathcal{A}_r^{-1} - \psi)(\phi)\|_{\mathcal{A}_r} \leq \delta \|\phi\|_{\mathcal{A}_r^{-1}} \\ 
\|(I - \omega \mathcal{S}_r)v\|_{Q_B} \leq \gamma \|v\|_{Q_B}. 
\end{subeqnarray}
Namely, $\delta$ is the convergence rate for Gauss-Seidel method for example and $\gamma$ is the convergence rate for the Schur complement solve. We assume that $\delta$ satisfies the following:
\begin{equation}
\delta < \frac{1-\gamma}{3-\gamma}. 
\end{equation} 
Bramble showed the following. We see that the following holds: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k - \psi (\mathcal{A}_r  E^U_k + B^T E^H_k) \\
          &=& (\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E^U_k + B^T E_k^H) - \mathcal{A}_r^{-1} B^T E^H_k \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray} 
By taking $\mathcal{A}_r$ norm, we arrive at 
\begin{eqnarray}
\|E^U_{k+1}\|_{\mathcal{A}_r} &=& \|(\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E^U_k + B^T E_k^H) - \mathcal{A}_r^{-1} B^T E^H_k \|_{\mathcal{A}_r}. 
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray} 
This gives that 
\begin{eqnarray}
\|E^U_{k+1}\|_{\mathcal{A}_r} &\leq& \delta \|E_k^U\|_{\mathcal{A}_r} + (1 + \delta) \|E_k^H\|_{Q_B}. 
\end{eqnarray}
We note that 
\begin{eqnarray}
E^H_{k+1} &=& (I - \omega B \psi B^T) E_k^H + \omega B (I - \psi \mathcal{A}_r)(E_k^U) \\ 
&=& (I - \omega B \mathcal{A}_r^{-1} B^T) E_k^H + \omega B (\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E_k^U + B^T E_k^H)  
\end{eqnarray} 
Thus we have that 
\begin{eqnarray}
\|E^H_{k+1}\|_{Q_B} &\leq& (\gamma + \delta) \|E_k^H\|_{Q_B} + \delta \|E_k^U\|_{\mathcal{A}_r}. 
\end{eqnarray} 
Therefore, we have that in a matrix form: 
\begin{equation}
\begin{pmatrix} 
\|E^U_{k+1}\|_{\mathcal{A}_r} \\
\|E^H_{k+1}\|_{Q_B}
\end{pmatrix} \leq M^{k+1}  
\begin{pmatrix} 
\|E^U_{0}\|_{\mathcal{A}_r} \\
\|E^H_{0}\|_{Q_B}
\end{pmatrix}  
\end{equation}
Here 
\begin{equation}
M = \begin{pmatrix} 
\delta & 1 + \delta \\ \delta & \gamma + \delta 
\end{pmatrix}. 
\end{equation}
This matrix is symmetric with respect to the inner product defined as follows: 
\begin{equation}
\left [ \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, \begin{pmatrix}  x_2 \\ y_2 \end{pmatrix} \right ] = \frac{\delta}{1+\delta} x_1 x_2 + y_1 y_2. 
\end{equation} 
Since $M$ is symmetric, its norm is bounded by its spectral radius and it is roots of 
\begin{equation}
\lambda^2 - (2\delta + \gamma)\lambda - \delta (1 - \gamma) = 0. 
\end{equation} 
It is observed that the spectral radius $\rho$ is an increasing function of $\delta$ for any fixed $\gamma \in [0,1]$. Moreover $\rho = 1$ for $\delta = (1-\gamma)/(3-\gamma)$. The convergence factor is then given by 
\begin{equation}
\rho = \frac{2\delta + \gamma + \sqrt{ (2\delta + \gamma)^2 + 4\delta(1-\gamma)}}{2}. 
\end{equation} 
Note that here $\gamma$ is the convergence rate for Richardson while $\delta$ is the convergence rate for Gauss-Seidel method. The convergence estimate is given as follows: 
\begin{eqnarray*}
\left ( \frac{\delta}{1 + \delta} (\mathcal{A}_r E_k^U, E_k^U) + (Q_B E_k^H, E_k^H) \right ) \leq \rho^{2k} \left ( \frac{\delta}{1 + \delta} (\mathcal{A}_r E_k^U, E_k^U) + (Q_B E_k^H, E_k^H) \right ). 
\end{eqnarray*}

\end{document} 

% Directly taking $A-$norm: 
% \begin{eqnarray}
% \| E^U_{k+1} \|_A &\leq& \| I - L^{-1} \nabla G \| \|E^U_k\|_A  + \| L^{-1} B^T \|_A \|E^H_k\|_A \\
% \| E^H_{k+1}\| &\leq& \|I - \omega B L^{-1} B^T \| \|E_k^H\| + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
% \end{eqnarray}

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

\begin{equation}
    \|M \|_A \leq \sqrt{ \kappa(A)} \| M \|_2
\end{equation}
We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \lambda_{max}(A) \| L^{-1} B^T H\| \\
   & =\lambda_{max}(A) (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}
We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}
\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}
\textcolor{red}{ Here !} 

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}

%\end{document} 

\section{Appendix} 

\begin{lemma} 
The GD three step can be shown to be convergent linearly if one chooses $\gamma = \frac{1}{r}$ with $\omega = r$, sufficiently small enough, then it converges linearly.
\end{lemma} 
\begin{proof} 
For GD three step case, we have that with $N = 3$: 
\begin{eqnarray*}
D_r^*(H_k + rKz_*) = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}
We also note that  
\begin{equation*}
D_r^*(H_k + rKz_k) = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{equation*}
Let us define 
\begin{eqnarray*}
D_{H_*,H_k}^{Z_*} &:=& D_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*) \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*) + \gamma H_* \\
&& \qquad - ((I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma H_k)  \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*) \\
&& \qquad - (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma (H_* - H_k) \\ 
&=& (I - \gamma A)(\xi_1) ((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - ((I - \gamma A)((I - \gamma A)(X_*)) + \gamma H_k)) \\
&& \qquad + \gamma (I + (I - \gamma A)(\xi_1)) (H_* - H_k) \\
&=& \gamma [(I - \gamma A)(\xi_1)(I - \gamma A)(\xi_2) + I + (I - \gamma A)(\xi_1))] (H_* - H_k) \\
&=& \gamma [I + A_\gamma (\xi_1) + A_\gamma(\xi_1) A_\gamma(\xi_2)] (H_* - H_k). 
\end{eqnarray*}
\begin{eqnarray*}
D_{Z_*,Z_k}^{H_k} &=& D_r^*(H_k + rKz_*) - D_r^*(H_k + rKz_k) \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) + \gamma H_k)) \\
&& \qquad - (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k)) \\ 
&=& A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k). 
\end{eqnarray*}
We now observe the following estimate holds: 
\begin{eqnarray*}
&& \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (D_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 \\ 
&& \quad = \|H_* - H_k - \omega (D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k})\|^2,  \\
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k} \rangle + \omega^2 \langle  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k},  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k}\rangle \\ 
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} \rangle + \omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{H_*,H_k}^{Z_*}\rangle \\ 
&& \qquad - 2 \omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2 \omega^2 \langle  
D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k} \rangle \\
&& \qquad + \omega^2 \langle D_{Z_*,Z_k}^{H_k},  D_{Z_*,Z_k}^{H_k} \rangle. 
\end{eqnarray*}
On the other hand, we see that 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad = -2\omega \langle H_* - H_k, A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle \\ 
&& \qquad + 2 \omega^2 \langle \gamma (I + A_\gamma (\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2)) (H_* - H_k), A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k)  \rangle \\
&& \qquad = - 2\omega \langle (I - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))(H_* - H_k), A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k) \rangle. 
%&& \qquad \leq 2\omega|\omega\gamma - 1| \sin^2 \theta (1 - \gamma \lambda)^2 \|H_* - H_k\| \|X_* - X_k\| \\
%&& \qquad + 2\omega^2 \gamma (1 - \gamma \lambda)^3 \sin^3 \theta \|H_* - H_k\|\|X_* - X_k\|.   
\end{eqnarray*}
Therefore, we have that with $\alpha + \beta = 1$, 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad \leq (1 - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))^{2\alpha} \rho_1^3 \|H_* - H_k\|^2 \\
&& + (1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))^{2\beta} \rho_1^3 \|X_* - X_k\|_\omega^2. 
\end{eqnarray*}
On the other hand, we have 
\begin{eqnarray*}
\|H_* - H_k - \omega D_{H_*,H_k}^{Z_*} \|^2 &\leq& 
\left (1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2} \|H_* - H_k\|^2 \\ 
\|D_{Z_*,Z_k}^{H_k}\|^2 &=& (1 - \gamma \lambda)^6 \|X_* - X_k\|^2.   
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} \leq \rho_{E^H} \|H_* - H_k\|^2  \\
&& \qquad + \rho_{E^X} \|X_* - X_k\|_{\omega}^2, 
\end{eqnarray*}
where 
\begin{eqnarray*}
\rho_{E^H} &=& \left ( 1 + \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2\alpha} \rho^3 \right ) \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2} \\
\rho_{E^X} &=& \left ( 1 + \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2\beta} \rho^3 \right ) (1 - \gamma \lambda)^6. 
\end{eqnarray*}
We now choose $\alpha = 0$ and $\beta = 1$. Further, we choose $\omega = 1/(3\gamma)$. Then, we see that 
\begin{eqnarray*}
I - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1) A_\gamma(\xi_2)) &=& I - (I + (I - \gamma A(\xi_1) + (I - \gamma A(\xi_1))(I - \gamma A(\xi_2))))/3  \\
&=& [2\gamma A(\xi_1) + \gamma A(\xi_2) - \gamma^2 A(\xi_1) A(\xi_2)]/3 = \gamma + O(\gamma^2). 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
\rho_{E^H} &=& \left ( 1 + (\gamma + O(\gamma^2))^{2\alpha} \rho^3 \right )  O(\gamma^2) \\
\rho_{E^X} &=& \left ( 1 + (\gamma + O(\gamma^2))^{2\beta} \rho^3 \right ) (1 - \gamma \lambda)^6. 
\end{eqnarray*}
We note that with $r_1 \rightarrow r - \lambda$, we have 
\begin{equation} 
1 - \gamma \lambda = 1 - \frac{\lambda}{r} = \frac{r - \lambda}{r} = \frac{r_1}{r_1 + \lambda}. 
\end{equation} 
We note that if $r$ is sufficiently large, then we can make sure these two parameters smaller than one. Thus, we have the linear convergence.  This completes the proof. 
\end{proof}


We now present the main theorem in this subsection. 
\begin{lemma} 
The GD two step can be shown to be convergent linearly if one chooses $\gamma = \frac{1}{r}$ with $\omega = r$, sufficiently small enough, then it converges linearly.
\end{lemma} 
\begin{proof} 
For GD two step case, we have that with $N = 2$
\begin{eqnarray*}
D_r^*(H_k + rKz_*) = (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}
We also note that  
\begin{equation*}
D_r^*(H_k + rKz_k) = (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k.  
\end{equation*}
Let us define 
\begin{eqnarray*}
D_{H_*,H_k}^{Z_*} &:=& D_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*) \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_* - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k)  \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma (H_* - H_k) \\ 
&=& (I - \gamma A)(\xi_1) ( (I - \gamma A)(X_*) + \gamma H_* - (I - \gamma A)(X_*) - \gamma H_k ) + \gamma (H_* - H_k) \\ 
&=& \gamma (I - \gamma A)(\xi_1)(H_* - H_k) + \gamma (H_* - H_k) = \gamma A_{\gamma_1} (H_* - H_k) + \gamma (H_* - H_k) \\
&=& \gamma (I +  A_\gamma(\xi_1)) (H_* - H_k). 
\end{eqnarray*}
and 
\begin{eqnarray*}
D_{Z_*,Z_k}^{H_k} &=& D_r^*(H_k + rKz_*) - D_r^*(H_k + rKz_k) \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) + \gamma H_k) - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) \\ 
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) \\ 
&=& A_{\eta_1} ((I - \gamma A)(X_*) - (I - \gamma A)(X_k)) = A_{\eta_1} A_{\eta_2} (X_*  - X_k). 
\end{eqnarray*}
We now observe the following estimate holds: 
\begin{eqnarray*}
&& \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (D_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 \\ 
&& \quad = \|H_* - H_k - \omega (D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k})\|^2,  \\
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k} \rangle + \omega^2 \langle  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k},  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k}\rangle \\ 
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} \rangle + \omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{H_*,H_k}^{Z_*}\rangle \\ 
&& \qquad - 2 \omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2 \omega^2 \langle  
D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k} \rangle \\
&& \qquad + \omega^2 \langle D_{Z_*,Z_k}^{H_k},  D_{Z_*,Z_k}^{H_k} \rangle. 
\end{eqnarray*}
On the other hand, we see that 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad = -2\omega \langle H_* - H_k, A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle \\ 
&& \qquad + 2 \omega^2 \langle \gamma (I + A_\gamma (\xi_1)) (H_* - H_k), A_{\eta_1} A_{\eta_2} (X_*  - X_k)  \rangle \\
&& \qquad = - 2\omega \langle (I - \omega \gamma (I + A_\gamma(\xi_1))(H_* - H_k), A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle. 
%&& \qquad \leq 2\omega|\omega\gamma - 1| \sin^2 \theta (1 - \gamma \lambda)^2 \|H_* - H_k\| \|X_* - X_k\| \\
%&& \qquad + 2\omega^2 \gamma (1 - \gamma \lambda)^3 \sin^3 \theta \|H_* - H_k\|\|X_* - X_k\|.   
\end{eqnarray*}
Therefore, we have that with $\alpha + \beta = 1$, 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad \leq (1 - \omega \gamma (I + A_\gamma(\xi_1))^{2\alpha} \rho_1^2 \sin 3 \theta \|H_* - H_k\|^2 + (1 - \omega \gamma (I +  A_\gamma(\xi_1))^{2\beta} \rho_1^2 \sin 3 \theta \|X_* - X_k\|_\omega^2. 
\end{eqnarray*}
On the other hand, we have 
\begin{eqnarray*}
\|H_* - H_k - \omega D_{H_*,H_k}^{Z_*} \|^2 &\leq& 
(1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \|H_* - H_k\|^2 \\ 
\|D_{Z_*,Z_k}^{H_k}\|^2 &=& (1 - \gamma \lambda)^4 \|X_* - X_k\|^2.  \\ 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} \\
&& \qquad \leq \left ( 1 + (1 - \omega \gamma (I + A_\gamma(\xi_1)))^{2\alpha}\rho^2 \sin 3\theta \right ) (1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \|H_* - H_k\|^2  \\
&& \qquad + \left (1 + (1 - \omega\gamma (I + A_\gamma(\xi_1))^{2\beta} \rho^2 \sin 3\theta \right ) (1 - \gamma \lambda)^4  \|X_* - X_k\|_{\omega}^2. 
%&& + \left ( (1 - \gamma \lambda)^4 + \rho^{2\beta_2} \sin^{2\mu_2} \theta \right ) \|Kz_* - Kz_k\|_\omega^2 \\ 
%&=& \left ( 1 + \sin^{2\mu_1} \right ) (1 - \gamma \lambda)^2  \|H_* - H_k\|^2 \\
%&& + \left ( 1 + \sin^{2\mu_2} \right ) (1 - \gamma \lambda)^4 \|Kz_* - Kz_k\|_\omega^2
\end{eqnarray*}
We note that with $r_1 \rightarrow r - \lambda$, we have that $\rho$ behaves like 
\begin{equation} 
1 - \gamma \lambda = 1 - \frac{\lambda}{r} = \frac{r - \lambda}{r} = \frac{r_1}{r_1 + \lambda}.  
\end{equation} 
On the other hand, with the choice of $\omega = 1/(2\gamma)$, we have that for sufficiently larger $r$, 
\begin{eqnarray*}
\sigma(I - \omega \gamma (I + A_\gamma(\xi_1))) &=& \sigma (I - (I + A_\gamma(\xi_1))/2) \\
&=& \sigma (I - (I + I - \gamma A(\xi_1))/2) = \sigma(\gamma A(\xi_2)/2) = \frac{\gamma}{2} L.    
\end{eqnarray*}
Thus, if we choose $\alpha = 0$ and $\beta = 1$, then we see that 
\begin{eqnarray*}
&& \left ( 1 + (1 - \omega \gamma (I + A_\gamma(\xi_1)))^{2\alpha}\rho^2 \right ) (1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \\
&& = \left ( 1 + \rho^2 \right ) \frac{\gamma^2 L^2}{4} \\
&& \left (1 + (1 - \omega\gamma (I + A_\gamma(\xi_1))^{2\beta} \rho^2 \right ) (1 - \gamma \lambda)^4 = \left ( 1 + \left ( \frac{\gamma L}{2} \right )^{2\beta}\rho^2 \right ) = \left ( 1 + \frac{c}{r^2} \right ) \left (\frac{r}{r + \lambda} \right )^4 \\ 
&& < 1 \Longleftrightarrow r^4 + c r^2 < (r + \lambda)^4. 
\end{eqnarray*}
This completes the proof. 
\end{proof}

We notice that the following monotonicity holds for $F_N$. 
\begin{lemma}
The function $F_N(X,H) : \Reals{d} \mapsto \Reals{d}$, defined by 
\begin{equation}
F_N(X,H) := \underbrace{W(\cdots (W(W}_{\textsf{N} \, \mbox{times}}(X, \underbrace{H), \cdots H)),H)}_{\textsf{N}\, \mbox{times}}
\end{equation}
is strongly monotone in the second variable. Namely, we have that for some $\lambda_N > 0$, which does not degenerate in $N$,  
\begin{equation}
\langle H_1 - H_2, F_N(X,H_1) - F_N(X,H_2) \rangle \geq \lambda_N \|H_1 - H_2\|^2, \quad \forall H_1, H_2 \in \Reals{d}.
\end{equation}
More precisely, we have that 
\begin{equation} 
\lambda_N = \gamma + (1 - \gamma L) \lambda_{N-1} = \gamma \sum_{\ell=1}^N (1 - \gamma L)^{\ell-1}.  
\end{equation} 
with $\lambda_1 = \gamma$. Furthermore, $F_N$ is uniformly continuous in the second variable in the following sense that
\begin{equation}
\|F_N(X,H_1) - F_N(X,H_2)\| \leq L_{N} \|H_1 - H_2\|,  
\end{equation}
where 
\begin{equation} 
L_{N} = \gamma \sum_{\ell = 1}^{N} (1 - \gamma \lambda)^{\ell-1}. 
\end{equation} 
\end{lemma}
\begin{proof}
We prove it by induction arguement. For $N = 1$, we have that 
\begin{eqnarray*}
\langle H_1 - H_2, F_1(X,H_1) - F_1(X,H_2) \rangle &=& \langle H_1 - H_2, W(X,H_1) - W(X,H_2) \rangle \\ 
&=& \langle H_1 - H_2, [(I - \gamma A)(X)  + \gamma H_1] - [(I - \gamma A)(X) + \gamma H_2] \rangle \\
&=& \langle H_1 - H_2, \gamma (H_1 - H_2) \rangle = \gamma \|H_1 - H_2\|^2. 
\end{eqnarray*}
Here $\lambda_1 = \gamma$. Now, we assume that there exists $\lambda_{N-1}$ such that 
\begin{eqnarray*}
\langle H_1 - H_2, F_{N-1}(X,H_1) - F_{N-1}(X,H_2) \rangle \geq r_{N-1} \|H_1 - H_2\|^2. 
\end{eqnarray*}
We then consider the following: 
\begin{eqnarray*}
&& \langle H_1 - H_2, F_N(X,H_1) - F_N(X,H_2) \rangle \\
&& \qquad = \langle H_1 - H_2, [W(F_{N-1}(X,H_1)) + \gamma H_1] - [W(F_{N-1}(X,H_2)) + \gamma H_2] \rangle = \gamma \|H_1 - H_2\|^2 \\
&& \qquad \geq \gamma \|H_1 - H_2\|^2 + (1 - \gamma L ) \langle H_1 - H_2, F_{N-1}(X,H_1) - F_{N-1}(X,H_2) \rangle \\
&& \qquad = (\gamma + (1 - \gamma L) \lambda_{N-1}) \|H_1 - H_2\|^2 = \lambda_N \|H_1 - H_2\|^2. 
\end{eqnarray*}
This completes the proof. 
\end{proof}


\subsection{Inexact Gauss-Seidel with a fixed number of GD steps for $X$ block} 
For given $(z_k, H_k)$, we consider the solution to the following equation: 
\begin{equation}
\nabla F(X) + r X - H_k - r Kz_k = 0. 
\end{equation}
This has been characterized as $X_{k+1}$ in the block Gauss-Seidel method in the previous section. However, we are interested in solving it using the Gradient Descent method. We notice that the corresponding functional is given as follows: 
\begin{equation} 
G(X) = F(X) - \langle H_k, X \rangle + \frac{r}{2} \|Kz_k - X\|^2. 
\end{equation} 
The optimality condition is that 
\begin{equation} 
0 = \nabla G(X_{k+1}) = \nabla F(X) - H_k -r Kz_k + rX. 
\end{equation}
Namely, we apply the gradient descent by the following procedure: 
\begin{equation} 
X_{k+1} = X_k - \gamma \nabla G(X_k) = X_k - \gamma (\nabla F(X_k) - H_k - rKz_k + rX_k). 
\end{equation} 
If we set $X_k = Kz_k$, then we have that
\begin{eqnarray*} 
X_{k+1} &=& X_k - \gamma \nabla G(X_k) = X_k - \gamma (\nabla F(X_k) - H_k - rKz_k + rX_k) \\
&=& Kz_k - \gamma(\nabla F(X_k) - H_k).   
\end{eqnarray*}
This is the choice of algorithm introduced in \cite{mishchenko2022proxskip}.  
We shall consider to apply the inexact block Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
N = \begin{pmatrix}
D_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad N^{*} = \begin{pmatrix}
D_r^{*} & 0\\
(r K^T K)^{-1} r K^T D_r^{*} & r^{-1} (K^T K)^{-1} 
\end{pmatrix}.  
\end{equation}
The inexact Uzawa iteration can then be defined as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + N^{*} (-B^T H_k - \nabla G(U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that 
\begin{equation}
-B^TH_k - \nabla G(U_k) = \begin{pmatrix} H_k - A_r(X_k) + rK z_k \\ rK^T X_k - r K^T K z_k \end{pmatrix} 
\end{equation}
and thus 
\begin{eqnarray*}
&& N^{*}(-B^TH_k - \nabla G(U_k)) = N^{*} \begin{pmatrix} H_k - A_r(X_k) + rK z_k \\ rK^T X_k - r K^T K z_k \end{pmatrix} \\
&& \quad = 
\begin{pmatrix} D_r^{*} (H_k - A_r(X_k) + rK z_k) \\ (K^TK)^{-1} K^T D_r^{*} (H_k - A_r(X_k) + rK z_k) + (K^T K)^{-1}K^T X_k - z_k \end{pmatrix}
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
X_{k+1} &=& X_k + D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
z_{k+1} &=& (K^TK)^{-1} K^T (X_{k} + D_r^{*} (H_k - A_r(X_k) + r K z_k)) \\ 
H_{k+1} &=& H_k + \omega B U_{k+1} = H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
We shall apply $K$ to $z$ equation to obtain: 
\begin{eqnarray*}
X_{k+1} &=& X_k + D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
Kz_{k+1} &=& P_Z (X_{k} + D_r^{*} (H_k - A_r(X_k) + r K z_k)) \\ 
H_{k+1} &=& H_k + \omega B U_{k+1} = H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& X_* + D_r^{*} (H_* - A_r(X_*) + r K z_*) \\
Kz_{*} &=& P_Z (X_* + D_r^{*} (H_* - A_r(X_*) + rK z_*)) \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_k + D_r^{*} (H_* - A_r(X_*) + r K z_*) - D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z \left \{ X_* - X_k + D_r^{*} (H_* - A_r(X_*) + rK z_*) - D_r^{*} (H_k - A_r(X_k) + rK z_k) \right \} \\
H_{*} - H_{k+1} &=& H_* - H_k + \omega (-(X_{*} - X_{k+1}) + K(z_{*} - z_{k+1})) \\ 
&=& H_* - H_k - \omega (X_{*} - X_{k+1}) + \omega K( z_{*} - z_{k+1}). 
\end{eqnarray*}

\subsubsection{The convergence analysis of a single step GD} In this section, we study the convergence analysis of the single step GD for the first block, i.e., $D_r^{-1} = \gamma$. We note that the main convergence analytical tool introduced in \cite{mishchenko2022proxskip} was the nonexpansiveness of the proximal operator. In this case, the error equation is given as follows: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_k + \gamma \left [(H_* - A_r(X_*) + r K z_*) - (H_k - A_r(X_k) + r K z_k) \right ] \\
&=& X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \\ 
Kz_{*} - Kz_{k+1} &=& P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \\
H_{*} - H_{k+1} &=& H_* - H_k + \omega (-(X_{*} - X_{k+1}) + K ( z_{*} - z_{k+1})). 
\end{eqnarray*}
The main convergence of interest lies in $z_k$ to $z_*$ and $H_k$ to $H_*$. Therefore, we shall not be much interested in the convergence of $X_k$ to $X_*$. This means that we are more or less interested in the convergence of $X_k$ in $P_Z$ norm, i.e., 
\begin{equation*}
\|E_k^X\|_{P_Z}^2 := \langle P_Z E_k^X, E_k^X \rangle \rightarrow 0 \quad \mbox{ as } \quad k \rightarrow \infty.
\end{equation*}
But, it is nothing else than the convergence of $z_k$ to $z_*$. Our plan is to establish the convergence of $H_k$ to $H_*$ and $z_k$ to $z_*$ and then use it to establish the convergence of $X_k$ to $X_*$. We first note that the following error bound holds. Under the condition on $\gamma$ given as follows: 
\begin{equation}
0 \leq \gamma \leq \frac{1}{L}, 
\end{equation}
Again, the trick is to rewrite $E_{k+1}^Z$ in a somewhat different form as follows: 
\begin{eqnarray*}
-\omega(Kz_{*} - Kz_{k+1}) = -\omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ).
\end{eqnarray*}
we have that 
\begin{eqnarray*}
\omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ) \| \\ 
&=& \|P_Z (H_* - H_k) - \omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ) \| \\
&=& \|P_Z (H_* - H_k) - \omega \left ( P_Z \left \{(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k) \right \} \right ) \| \\ 
&=& \|P_Z [(H_* - H_k) - \omega \left \{(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k) \right \}] \|. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|H_* - H_k - \omega Q_Z [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
&=& \|Q_Z \left \{ H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \right \} \| \\
&=& \|Q_Z \left \{ H_* - H_k - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma [H_* - H_k]] \right \} \| \\
\end{eqnarray*}
The main idea is not to use $A$, but to use the convexity of the functional for sufficiently small $\gamma > 0$:  
\begin{equation} 
V(x) = \frac{1}{2} \|x\|^2 - \gamma F(x).
\end{equation}
Therefore, we have for $\gamma \leq 1/L$, 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \omega \|Kz_* - Kz_{k+1}\| &\leq& \|H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
&\leq& |1 - \omega\gamma|\|H_* - H_k\| + \omega \|(I - \gamma A)(X_*) - (I - \gamma A)(X_k)\| \\
&\leq& |1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda) \omega \|X_* - X_k\| \\
&=& |1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda)  \|Kz_* - Kz_k\|_\omega. 
\end{eqnarray*}
\begin{remark}
The standard analysis presented in \cite{mishchenko2022proxskip} has used the fact that 
\begin{eqnarray*}
\|X_* - X_k - \gamma (A(X_*) - A(X_k)) \| \leq (1 - \gamma \lambda) \|X_* - X_k\|.
\end{eqnarray*}
This is indeed equivalent to the analysis presented in this paper. However, such a view can not be extended to multiple step GD case as will be discussed below. 
\end{remark}
This concludes that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq 
|1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda) \|Kz_* - Kz_k\|_\omega. 
%&\leq& \|H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
%&\leq& \| (I - \omega\gamma) (H_* - H_k) - \omega \{ X_* - X_k - \gamma (A(X_*) - A(X_k)) \} \| \\
%&\leq& {\rm abs}(1 - \omega \gamma) \|H_* - H_k\| + \omega \|X_* - X_k - \gamma (A(X_*) - A(X_k)) \| \\
%&\leq& {\rm abs}(1 - \omega \gamma) \|H_* - H_k\| + \omega (1 - \gamma \lambda)\|Kz_* - Kz_k\|. 
\end{eqnarray*}
Let us simply choose $r = \omega$ as has been done in \cite{mishchenko2022proxskip}. Then, we choose $r$ and $\gamma$ so that 
\begin{eqnarray*}
|1 - \omega \gamma| < 1 \quad \mbox{ or } \quad -1 < 1 - \omega \gamma < 1.  
\end{eqnarray*}
Furthermore, we can choose $\gamma$ so that 
\begin{equation}
1 - c_0 = \max \{ |1 - \omega \gamma|, 1 - \gamma \lambda \}. 
\end{equation}
Then, we have the linear convergence. Namely, there exists $c_0$ such that 
\begin{equation}
[\|Kz_k - Kz_*\|_\omega + \|H_k - H_*\|] \leq (1 - c_0) [ \|Kz_{k-1} - Kz_*\|_\omega + \|H_{k-1} - H_*\| ], \quad \forall k = 1,\cdots. 
\end{equation}
Lastly, we can obtain the convergence of $X_k$ to $X_*$ from this result: 
\begin{eqnarray*}
\|X_{*} - X_{k+1}\| &=& \|X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))] \| \\
&\leq& \|X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \| \\ 
&\leq& \|X_* - X_k - \gamma (A(X_*) - A(X_k))\| + \gamma \|H_* - H_k\| \\
&\leq& (1 - \gamma \lambda) \|X_* - X_k\| + \gamma \|H_* - H_k\|.
\end{eqnarray*}
Therefore, we can show that $\|X_* - X_{k}\|$ also converges.  This completes the proof. 

\subsubsection{The convergence analysis for nonlinear solver}
We now look at the general case for $D_r$. As presented in \cite{mishchenko2022proxskip}, we shall set $X_k = Kz_k$ when we begin the GD step for the variable $X$. This includes a total of $N$-step GD for finding zero of the following nonlinear equation for a given $z_k$ and $H_k$:
\begin{equation} 
\nabla F(X) + r X = rKz_k + H_k 
\end{equation} 
reads as follows: with $X_k = Kz_k$ and $\gamma > 0$,   
\begin{eqnarray*} 
X_{k+\frac{1}{N}} &=& X_{k} + \gamma (H_k - A(X_k)) = [(I - \gamma A)(X_k) + \gamma H_k] \\ 
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} + \gamma (H_k - A(X_{k+\frac{1}{N}})) = [(I - \gamma A)(X_{k+\frac{1}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} + \gamma (H_k - A(X_{k+\frac{N-2}{N}})) =  [(I - \gamma A)(X_{k+\frac{N-2}{N}}) + \gamma H_k] \\  \\
X_{k+\frac{N}{N}} &=& X_{k+\frac{N-1}{N}} + \gamma (H_k - A(X_{k+\frac{N-1}{N}})) = [(I - \gamma A)(X_{k+\frac{N-1}{N}}) + \gamma H_k] \\ 
\end{eqnarray*}
Therefore, we see that its action is independent of $Kz_k$ in this setting, but it depends on $H_k$. The action of $D_r^*$ is important for the analysis. We shall interpret it as follows: 
\begin{equation} 
X_{k+1} = X_k + D_r^*(H_k - A(X_k)).  
\end{equation} 
%where 
%\begin{eqnarray*} 
%D_r^* (H_k - A(X_k)) &:=& \gamma (H_k - A(X_k))  \\ 
%&+& \gamma (H_k - A((I - \gamma A)(X_k) + \gamma H_k)) \\ 
%&+& \gamma (H_k - A((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k))) \\
%&+& \gamma (H_k - A((I - \gamma A)^2((I - \gamma A)(X_k) + \gamma H_k))) \\ 
%&+& \cdots \\
%&+& \gamma (H_k - A((I - \gamma A)^{N-2}((I - \gamma A)(X_k) + \gamma H_k))) \\
%&=& \gamma N H_k - \gamma \sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_k )
%\end{eqnarray*} 
For example, if $N = 2$, then 
\begin{equation} 
X_{k+1} = (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k. 
\end{equation}
and for example, if $N = 3$, then 
\begin{eqnarray*} 
X_{k+1} = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}


We note that $A(X_*) - H_* = \nabla F(X_*) - H_* = 0$ and the 
error equation is given as follows:  
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_{k} + D_r^*(H_* - A(X_*)) - D_r^* (H_k - A(X_k)) \\ 
Kz_* - Kz_{k+1} &=& P_Z (X_{*} - X_{k} + D_r^*(H_* - A(X_*)) - D_r^* (H_k - A(X_k)) \\ 
H_* - H_{k+1} &=& H_* - H_k - \omega (X_* - X_{k+1}) + \omega K(z_* - z_{k+1}). 
\end{eqnarray*}
On the other hand, we have as for the GS or a single step GD case,
\begin{eqnarray*}
Kz_{*} - Kz_{k+1} = P_Z \left ( X_* - X_{k+1} \right ). 
\end{eqnarray*}
and thus
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z \left ( X_* - X_{k+1} \right ) \right ). 
\end{eqnarray*}
This leads to the following bound:  
\begin{eqnarray*}
\omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left ( P_Z [X_* - X_k + D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_k) + rKz_k)] \right ) \| \\
%&=& \|-\omega \left ( P_Z [X_* - X_k + D_r^* (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_*) + rKz_*) \right.  \\
%&& + D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*)   \\
%&& \left. + D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) ] \right ) \| \\
&=& \|P_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))].
%&& - \omega (X_* - X_k + \left [ D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*) \right ] ) \\ 
%&& - \omega \left ( D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) \right ) ) \|. 
%
%\left ( (H_* - H_k) -\omega [X_* - X_k + D_r^* (H_k - A_r(X_*) + rKz_k) - D_r^* (H_k - A_r(X_k) + rKz_k) \right.  \\
%&& + \left. D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_*) + rKz_k) \right )  \| \\
%&\leq& \|P_Z [X_* - X_k + D_r^* (H_k - A_r(X_*) + rKz_k) - D_r^* (H_k - A_r(X_k) + rKz_k)] \| \\
%&& + \|P_Z[ D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_*) + rKz_k)]\| \\
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|Q_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_k) + rKz_k)] \| \\  
&=& \|Q_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k)] \|. 
%&& - \omega (X_* - X_k + \left [ D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*) \right ] ) \\ 
%&& - \omega \left ( D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) \right ) ) \| \\ 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq \\ && \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))] \| \\
&=& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k)) + D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\ 
&=& \|H_* - H_k - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-1}{N}}) + \gamma (H_* - H_k)] \| \\ 
&=& \|(1 - \omega \gamma) (H_* - H_k) - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-1}{N}})] \|.  
\\
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|X_* - X_{k+\frac{N-1}{N}}\|  \\ 
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-2}{N}}) + \gamma (H_* - H_k)\|  \\
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda)^2 \|X_* - X_{k+\frac{N-2}{N}} \| + \omega \gamma (1 - \gamma \lambda) \|H_* - H_k\| \\
&\leq& (1 - \gamma\omega) \|H_* - H_k\| + \omega ( 1- \gamma \lambda)^N \|X_* - X_k\| + \omega\gamma ((1 - \gamma \lambda) + \cdots + (1 - \gamma \lambda)^{N-1}) \|H_* - H_k\|. \end{eqnarray*}
We shall choose $\omega = 1/\gamma$. Then, we have that \begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} &\leq& \omega ( 1- \gamma \lambda)^N \|X_* - X_k\| + \omega\gamma \sum_{\mu = 1}^{N-1} (1 - \gamma \lambda)^\mu \|H_* - H_k\|.  
\end{eqnarray*}


Therefore, the convergence boils down to establish that there exists $c_0 > 0$ such that 
\begin{eqnarray*}
&& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))] \| \\
&&\qquad \leq (1 - c_0) \left ( \|H_{*} - H_{k}\| + \|Kz_* - Kz_{k}\|_{\omega} \right ). 
\end{eqnarray*}
We observe that for $N = 2$, we have that 
\begin{eqnarray*}
&&  \|H_* - H_k - \omega [X_* - X_k + \{ \gamma ( H_* - A(X_*) ) + \gamma (H_* - A((I - \gamma A)(X_*) + \gamma H_*)) \} \\
&& 
- \{ \gamma ( H_k - A(X_k)) + \gamma (H_k - A((I - \gamma A)(X_k))) \}
\end{eqnarray*}
\textcolor{red}{The trick is to target the second expression of $A$ with input given as $(I - \gamma A)(X)$:} 
\begin{eqnarray*}
&&  \|H_* - H_k - \omega [(I - \gamma A)(X_*) + \gamma H_* +  \gamma (H_* - A((I - \gamma A)(X_*) + \gamma H_*)) \} \\
&& 
- \{(I - \gamma A)(X_k) + \gamma H_k + \gamma (H_k - A((I - \gamma A)(X_k))) \} \| \\ 
&=& \|(I - \omega \gamma) (H_* - H_k) - \omega [[(I - \gamma A)(X_*) + \gamma H_*] - [(I - \gamma A)(X_k) + \gamma H_k] \\
&& - \gamma ( A((I - \gamma A)(X_*) + \gamma H_*) - A((I - \gamma A)(X_k)) + \gamma H_k) \} \\ 
&\leq& |I - \omega \gamma |\|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k)\| \\ 
&\leq& (1 - \omega \gamma) \|H_* - H_k\| + (1 - \gamma \lambda)^2 \|X_* - X_k\|_\omega + \gamma \omega (1 - \gamma \lambda) \|H_* - H_k\| \\ 
&=& (1 - \gamma^2 \lambda) \|H_* - H_k\| + (1 - \gamma \lambda)^2 \|X_* - X_k\|_\omega. 
\end{eqnarray*}

%&& \qquad - \{ \gamma N H_k - \gamma \sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_k ) \}] \| \\
%&&  \|(I - \omega \gamma N ) (H_* - H_k) - \omega [X_* - X_k - \gamma \{\sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_*) + \gamma (I - \gamma A)^{\ell-2} H_* )\} \\ 
%&& \qquad - \{\sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_* ) \}] \| \\
%&&\qquad \leq (1 - c_0) \left ( \|H_{*} - H_{k}\| + \|Kz_* - Kz_{k}\|_{\omega} \right ). 
%\end{eqnarray*}



This leads that that there exists $c_0 > 0$ such that
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\|_\omega + \|H_* - H_{k+1}\| \leq (1 - c_0) \left [ \|Kz_* - Kz_k\|_\omega + \|H_* - H_k\| \right ]. 
\end{eqnarray*}
This completes the proof. 

We now consider the case $N = 3$. 
\textcolor{red}{The trick is to target the second expression of $A$ with input given as $(I - \gamma A)(X)$:} 
\begin{eqnarray*}
&&  \|(1 - 2\gamma \omega) (H_* - H_k) - \omega [(I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - \gamma A((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*)] \\
&& - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) - \gamma A((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) \| \\ 
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) \| \\
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda)^2 \|(I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)(X_k) - \gamma H_k) \| \\
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda)^3 \|X_* - X_k\| + \gamma \omega (1 - \gamma \lambda)^2 \|H_* - H_k\|. 
\end{eqnarray*}
This gives 
\begin{eqnarray*} 
\|H_* - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} &\leq& (1 - 2\gamma \omega + \gamma \omega (1 - \gamma\lambda)^2) \|H_* - H_k\| \\
&& \quad + (1 - \gamma \lambda)^3 \|Kz_* - Kz_k\|_{\omega}. 
\end{eqnarray*} 
and thus 
\begin{equation} 
\|H_* - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq (1 - \gamma \omega) \|H_* - H_k\| + (1 - \gamma \lambda)^3 \|Kz_* - Kz_k\|_{\omega}. 
\end{equation} 
This completes the proof. 

\begin{eqnarray*}
&& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k)) + D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\ 
&& \|H_* - H_k - \omega [D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\
&& + 
\omega \|X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k))] \|. 
\end{eqnarray*}
We shall analyze one by one as follows: 
\begin{eqnarray*}
\|H_* - H_k - \omega [D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| &=& \\  
\end{eqnarray*}
For $N = 2$, we get for $\gamma \leq 1/L$, 
\begin{equation} 
(1 - 2\gamma \omega) \|H_* - H_k\| + \omega \gamma^2 L \|H^* - H_k\| \leq (1 - 2\gamma \omega) \|H_* - H_k\| + \omega \gamma \|H^* - H_k\| = (1 - \gamma \omega) \|H_* - H_k\|. 
\end{equation} 

\bibliographystyle{plain}
\bibliography{mybib}

%\end{document} 


\subsubsection{The convergence analysis for multiple GD steps}
We now look at the general case for $D_r$. As presented in \cite{mishchenko2022proxskip}, we shall set $X_k = Kz_k$ when we begin the GD step for the variable $X$. A total of $N$-step GD for finding zero of the following nonlinear function: 
\begin{equation} 
\nabla F(X) + r X = rKz_k + H_k 
\end{equation} 
reads as follows: with $\gamma > 0$, 
\begin{eqnarray*} 
X_{k+1} = X_{k} + N \gamma H_k - \gamma \left ( \sum_{\mu = 0}^{N-1} A\left ( X_{k + \mu/N}\right ) \right ).
\end{eqnarray*}
We note that $A(X_*) - H_* = \nabla F(X_*) - H_* = 0$ and the 
error equation is given as follows:  
\begin{eqnarray*}
X_{*} - X_{k+(\ell+1)/N} &=& X_* - X_{k + \ell/N} + \gamma (H_* - A(X_*)) - \gamma(H_{k + \ell/N} - A(X_{k + \ell/N})) \\
&=& X_* - X_{k+\ell/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + \ell/N})),  
\end{eqnarray*}
where $\ell = 0,1,\cdots,N-1$. More precisely, we have that 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) \\ 
X_{*} - X_{k+(N-1)/N} &=& X_* - X_{k + (N-2)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-2)/N})) \\  
X_{*} - X_{k+(N-2)/N} &=& X_* - X_{k + (N-3)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-3)/N})) \\  
&\vdots& \\ 
X_{*} - X_{k+1/N} &=& X_* - X_{k} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k})).  
\end{eqnarray*}
On the other hand, we have as for the GS or a single step GD case,
\begin{eqnarray*}
Kz_{*} - Kz_{k+1} = P_Z \left ( X_* - X_{k+1} \right ). 
\end{eqnarray*}
This means that in order to find out the relationship between $E_{k+1}^Z$ and $E_k^Z$, we must analyze the relationship between $E_{k+1}^X$ and $E_{k}^X$, which boils down to the following estimate under the product of $P_Z$. For all $\ell = 0,1,\cdots,N-1$, we have for $0 < \gamma \leq 1/L$, 
\begin{eqnarray*}
\|X_{*} - X_{k+(\ell+1)/N}\|_{P_Z} &=& \|X_* - X_{k+\ell/N} + \gamma (H_* - A(X_*)) - \gamma(H_{k} - A(X_{k+\ell/N}))\|_{P_Z} \\
&\leq& \|X_* - X_{k+\ell/N} - \gamma (A(X_*) - A(X_{k+\ell/N}))\|_{P_Z} \\ 
&\leq& (1 - \gamma \lambda) \|X_* - X_{k+\ell/N}\|_{P_Z}.  
\end{eqnarray*}
\textcolor{red}{This has to be proven !} 
This gives that 
\begin{eqnarray*}
\|X_{*} - X_{k+1/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k}\|_{P_Z}   \\ 
\|X_{*} - X_{k+2/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k+1/N}\|_{P_Z}   \\ 
\|X_{*} - X_{k+3/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k+2/N}\|_{P_Z}  \\ 
&\vdots& \\
\|X_{*} - X_{k+1}\|_{P_Z} &\leq& (1 - \gamma \lambda)^N \|X_* - X_{k+(N-1)/N}\|_{P_Z}.  
\end{eqnarray*}
This gives that 
\begin{eqnarray*}
\|X_* - X_{k+1}\|_{P_Z} &\leq& (1 - \gamma \lambda)^N \|X_* - X_k\|_{P_Z} \\ 
&=& (1 - \gamma \lambda)^N \|X_* - X_k\|_{P_Z}.  
\end{eqnarray*}
Therefore, we have 
\begin{eqnarray*}
\|Kz_{*} - Kz_{k+1}\| = \|X_* - X_{k+1}\|_{P_Z} \leq (1 - \gamma \lambda)^N \|Kz_* - Kz_k\| 
\end{eqnarray*}
and we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|H_* - H_k - \omega Q_Z (X_* - X_{k+1})\| = \|Q_Z [(H_* - H_k) - \omega (X_* - X_{k+1})]\| \\ 
&=& \|Q_Z [(H_* - H_k) - \omega [ X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) ]] \| \\ 
&=& \|Q_Z [(H_* - H_k) - \omega [ X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) ]\}\| \\ 
&\leq& \|Q_Z [(I - \omega \gamma)(H_* - H_k) - \omega (X_* - X_{k + (N-1)/N} - \gamma (A (X_*) - A(X_{k + (N-1)/N}))) ]\| \\ 
&\leq& |1 - \omega \gamma| \|H_* - H_k\| + \omega \|Q_Z[X_* - X_{k + (N-1)/N} - \gamma (A (X_*) - A(X_{k + (N-1)/N}))]\|.
\end{eqnarray*}
Therefore, we have 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\|_{1/r} &\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + (1 - \gamma\lambda) \|X_* - X_{k + (N-1)/N}\| \\
&\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + (1 - \gamma \lambda) \|X_* - X_{k + (N-2)/N} - \gamma (A(X_*) - A(X_{k + (N-2)/N})) + \gamma(H_* - H_k)\| \\
&\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda) \left \{ (1 - \gamma \lambda) \|X_* - X_{k + (N-2)/N}\| + \gamma\|H_* - H_k\| \right \} \\
&=& [|1 - \omega \gamma| + \gamma (1 - \gamma \lambda) \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^2 \|X_* - X_{k + (N-2)/N}\| \\ 
&=& [|1 - \omega \gamma| + \gamma (1 - \gamma \lambda) + \gamma (1 - \gamma \lambda)^2) \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^3 \|X_* - X_{k + (N-3)/N}\| \\ 
&=& \cdots \\ 
&\leq& [|1 - \omega \gamma| + \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^N \|X_* - X_{k}\| \\ 
&=&  [|1 - \omega \gamma| + \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\|.  
\end{eqnarray*}
This gives that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &\leq& [|1 - \omega \gamma| + \omega \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\| + \omega ( 1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\| \\ 
&=& \left ( |1 - \omega \gamma| + \frac{\omega}{\lambda} ((1 - \gamma \lambda) - (1 - \gamma \lambda)^N) \right ) \|H_* - H_k\| + \omega (1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\|. 
\end{eqnarray*}
Therefore, we arrive at the conclusion that 
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\| + \|H_* - H_{k+1}\| \leq 
\left ( (1 + \omega) (1 - \gamma \lambda)^N \right ) \|Kz_* - Kz_k\| + \left (|1 - \omega \gamma| + \frac{\omega}{\lambda} (1 - (\gamma\lambda)^N) \right ) \|H_* - H_k\|_r. 
\end{eqnarray*}
We note that the coefficient $(1 + \omega) ( 1- \gamma \lambda)^N$ can be made to be smaller than one easily. However, the second term needs a care and our goal is to make 
\begin{equation} 
|1 - \omega\gamma| + \frac{\omega}{\lambda} < 1 
\end{equation} 
Note that if $|1 - \omega\gamma| = 1 - \omega\gamma$, then we do not have a chance. But $|1 - \omega\gamma| = \omega\gamma - 1$ leads that 
\begin{equation}
\omega \gamma - 1 + \frac{\omega}{\lambda} < 1 
\end{equation}
to satisfy that there exists $c_0 > 0$ such that
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\| + \|H_* - H_{k+1}\|_r \leq (1 - c_0) \left [ \|Kz_* - Kz_k\| + \|H_* - H_k\|_r \right ]. 
\end{eqnarray*}
This completes the proof. 
\begin{remark}
We should use $M_2$ to eliminate $H$ part to make tighter bounds. In fact, we can use the $M_2$ inner product here to eliminate $H$ part.  
\end{remark}

\begin{itemize}
\item Does the convergence rate reduce to exact Gauss-Seidel case when $N\rightarrow \infty$ ? 
\end{itemize}

%\end{comment} 

\bibliographystyle{plain}
\bibliography{mybib}

%\end{document}
\subsection{Linear Case handling $(X,z)$ and $H$ block} 
We now consider the standard approach to handle the following system: 
\begin{equation}
\begin{pmatrix}
A_r & - r K & -I \\
-r K^T& r K^TK & K^T\\
-I& K & 0\\
\end{pmatrix}
\begin{pmatrix}
X_*\\
z_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}. 
\end{equation}
For simplicity, we consider the following block system: 
\begin{equation}
\begin{pmatrix}
\nabla G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0
\end{pmatrix}
\end{equation}
where 
\begin{equation}
\nabla G = \begin{pmatrix}
A_r & -r K\\
-r K^T & rK^T K
\end{pmatrix}, \quad \mbox{ and } \quad 
B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
We note that the Schur complement is given as follows:   
\begin{equation}
S = -B \nabla G^{*} (-B^T) 
\end{equation}
We write an equivalent form of the above equation using the Schur complement system. We shall consider to apply the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
L = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad L^{-1} = \begin{pmatrix}
A_r^{-1} & 0\\
(rK^T K)^{-1} rK^T A_r^{-1} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}

% \begin{equation}
% \begin{aligned}
%       A^{-1} & = \begin{pmatrix}
%      A_r^{-1} + A_r^{-1} rK ( rK^TK - rK^T A_r^{-1} rK)^{-1} rK^T A_r^{-1} & -  A_r^{-1} rK (rK^TK - rK^T A_r^{-1} rK)^{-1}\\
%      (rK^TK - rK^T A_r^{-1} rK)^{-1} rK^T A_r^{-1}  & (rK^TK - rK^T A_r^{-1} rK)^{-1}
%     \end{pmatrix}, \\
%      & = \begin{pmatrix}
%      A_r^{-1} + r A_r^{-1} K (rP)^{-1} K^T rA_r^{-1} & -  r A_r^{-1} K (rP)^{-1} \\
%      (rP)^{-1} rK^T A_r^{-1}  & (rP)^{-1}
%      \end{pmatrix},
% \end{aligned}
% \end{equation}
% where $P = K^TK - K^T rA_r^{-1} K$ is symmetric positive definite and has spectral radius less than 1. 

% \begin{lemma}
% The Schur complement S is symmetric positive definite. 
% \end{lemma}
% \begin{proof}
% It is shown in Lemma \ref{lemma4} that $A^{-1}$ is symmetric positive definite.
% Since $B^T$ is onto, we must have $B A^{-1} B^T$ being symmetric positive definite. 
% \end{proof}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + L^{-1} (-B^T H_k - \nabla G( U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that the exact solutions satiafy \begin{eqnarray}
U_{*} &=& U_{*} + L^{-1} (-B^T H_{*}-\nabla G (U_{*})) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray}
Thus, with the convention that $E_{k}^U = U - U_k$ and $E_{k}^H = H - H_k$, we obtain the error equations: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + L^{-1} (-\nabla G(E^U_k) -B^T E^H_k) \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray}
\begin{comment} 
In the matrix form, we have 
\begin{equation}
    \begin{pmatrix} 
     I  & 0 \\
      -\omega B & I
    \end{pmatrix} 
        \begin{pmatrix} 
      E^U_{k+1} \\
       E^H_{k+1}
    \end{pmatrix}  = 
        \begin{pmatrix} 
     I - L^{-1} A & -L^{-1} B^T \\
    0 & I
    \end{pmatrix} 
            \begin{pmatrix} 
      E^U_{k} \\
       E^H_{k}
    \end{pmatrix} 
\end{equation}
\end{comment} 
In the other direction, we have that 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + L^{-1} (- \nabla G(E^U_k) - B^T E^H_k) \\
          &=& E^U_k - L^{-1} \nabla G(E^U_k) - L^{-1} B^T E^H_k \\
E^H_{k+1} &=& E^H_k + \omega \left [ B (E^U_k + L^{-1} (- \nabla G(E_k^U) - B^T E^H_k) \right ] \\
&=& E^H_k - \omega B L^{-1} B^T E_k^H + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
\end{eqnarray}

% Directly taking $A-$norm: 
% \begin{eqnarray}
% \| E^U_{k+1} \|_A &\leq& \| I - L^{-1} \nabla G \| \|E^U_k\|_A  + \| L^{-1} B^T \|_A \|E^H_k\|_A \\
% \| E^H_{k+1}\| &\leq& \|I - \omega B L^{-1} B^T \| \|E_k^H\| + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
% \end{eqnarray}

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

\begin{equation}
    \|M \|_A \leq \sqrt{ \kappa(A)} \| M \|_2
\end{equation}
We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \lambda_{max}(A) \| L^{-1} B^T H\| \\
   & =\lambda_{max}(A) (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}
We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}
\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}
\textcolor{red}{ Here !} 

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \langle A L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix},  L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix} \rangle  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}

\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}


\subsubsection{Nonlinear Case} 
For simplicity, we consider the following block system: 
\begin{equation}
\begin{pmatrix}
\nabla G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0
\end{pmatrix}
\end{equation}
where 
\begin{equation}
\nabla G = \begin{pmatrix}
A_r & -r K\\
-r K^T & rK^T K
\end{pmatrix}, \quad \mbox{ and } \quad 
B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
We shall consider to apply the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
L = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad L^{*} = \begin{pmatrix}
A_r^{*} & 0\\
(rK^T K)^{-1} rK^T A_r^{*} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}
We note that the contraction property requires the expansion of $L$, $\nabla G$ and $L^*$. Thus, we list their computation below: 
\begin{eqnarray*}
\nabla G(U) &=& 
\begin{pmatrix}
A_r(X) - r K z \\ 
-r K^T X + r K^T K z 
\end{pmatrix} \\ 
L(U) &=& 
\begin{pmatrix}
A_r(X) \\ 
-r K^T X + r K^T K z 
\end{pmatrix} \\ 
L^*(U) &=& 
\begin{pmatrix}
A_r^*(X) \\ 
(r K^TK)^{-1}r K^T A_r^{*}(X) + r^{-1} (K^T K)^{-1} z 
\end{pmatrix} 
\end{eqnarray*}
% where $P = K^TK - K^T rA_r^{-1} K$ is symmetric positive definite and has spectral radius less than 1. 

% \begin{lemma}
% The Schur complement S is symmetric positive definite. 
% \end{lemma}
% \begin{proof}
% It is shown in Lemma \ref{lemma4} that $A^{-1}$ is symmetric positive definite.
% Since $B^T$ is onto, we must have $B A^{-1} B^T$ being symmetric positive definite. 
% \end{proof}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray*}
U_{k+1} &=& U_k + L^{*} (-B^T H_k - \nabla G(U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray*}
We note that the exact solutions satiafy 
\begin{eqnarray*}
U_{*} &=& U_{*} + L^{*} (-B^T H_{*}-\nabla G (U_{*})) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray*}
Thus, with the convention that $E_{k}^U = U - U_k$ and $E_{k}^H = H - H_k$, we obtain the error equations: 
\begin{eqnarray*}
U_* - U_{k+1} &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B (U_* - U_{k+1}). 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
U_* - U_{k+1} &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B \left [U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) \right. \\
&& \left. - L^{*} (-\nabla G(U_k) -B^T H_k) \right ]. 
\end{eqnarray*}
This gives that 
Therefore, we have that 
\begin{eqnarray*}
\langle U_* - U_{k+1}, \nabla G(U_*) - \nabla G(U_{k+1}) \rangle &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B \left [U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) \right. \\
&& \left. - L^{*} (-\nabla G(U_k) -B^T H_k) \right ]. 
\end{eqnarray*}



A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\| &\leq& \rho_U \|E_k^U\| + \|L^{-1} B^T E_k^H\| \\ 
&\leq&  \rho_U \|E_k^U\| + \frac{r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}

 
\subsection{A New Proof based on Dual operator}

Under the Uzawa framework. 
For simplicity, we denote the nonlinear operator $\nabla F(\cdot) + r I$ as $A_r$, $[z, H]^T$ as $U$,  and rewrite the matrix as follows.
\begin{equation}
\label{optimality condition aug Lag matrix form 2 by 2}
    \begin{pmatrix}
    A_r & B^T \\
    B & C
    \end{pmatrix}
    \begin{pmatrix}
    X\\
    U 
    \end{pmatrix} = 
    \begin{pmatrix}
    0 \\
    0,
    \end{pmatrix}
\end{equation}
where 
\begin{equation}
A_r = \nabla F(\cdot) + r I, \quad B = \begin{pmatrix}
-r K^T\\ -I
\end{pmatrix}, \quad C = \begin{pmatrix}
r K^T K & K^T \\
K & 0
\end{pmatrix}.
\end{equation}
We view the ADMM as a type of Uzawa iteration that solves Schur complement operator with a different iterative method. Namely, we notice that the system can be given as follows: 
\begin{subeqnarray*}
0 &=& A_r X  + B^T U \\
0 &=& \left [ B A_r^* (-B^T) (U) + CU \right ] = S(U). 
\end{subeqnarray*}
Under this setting, we can understand that the Uzawa iteration with an iterative method for the Schur complement is given in the following form: 
\begin{equation}\label{UzawaADMM}
\begin{cases}
A_r X_{k+1} + B^T U_k = 0 \quad \mbox{ or equivalently } \quad X_{k+1} = A_r^*(-B^T)( U_k) \\
U_{k+1} = U_k + N^{-1} \left( - \left [ B A_r^*(-B^T)(U_k) + CU_k \right] \right) = U_k + N^{-1} \left( - (B X_{k+1} + C U_k)\right),
\end{cases}
\end{equation}
where $N$ is some approximation of the nonlinear Schur complement operator. Note that our choice of $N$ will be given as follows: 
\begin{equation}\label{shurH} 
N = \begin{pmatrix}
r K^T K & 0\\
K & - \frac{1}{r} I
\end{pmatrix} \quad \mbox{ and } \quad  N^{-1} = \begin{pmatrix}
(r K^T K)^{-1} &  0 \\
 r K (r K^T K)^{-1} & - r I
\end{pmatrix}. 
\end{equation}
Such an iterative method corresponds to a damped Gauss-Seidel type inexact solver for the Schur complement that solves the first variable $z_{k+1}$ and then using Richardon's iteration with step size $r$ for the second variable $H_{k+1}$. We now show that this is exactly the ADMM method given in Algorithm \ref{algADMM1}.

\begin{proposition}\label{prop: Uzawa iterative solver}
The ADMM method in Algorithm \ref{algADMM1} is equivalent to the Uzawa iterations \eqref{UzawaADMM} with the choice of $N$ defined as in \eqref{shurH}.
\end{proposition}
\begin{proof}
We begin by writing the Uzawa iterations \eqref{UzawaADMM} for each variable update. We note that the second iteration in \eqref{UzawaADMM}  can be expanded as follows.

\begin{equation}
    \begin{pmatrix}
    z_{k+1}\\
    H_{k+1}
    \end{pmatrix} = \begin{pmatrix}
    z_{k}\\H_{k}
    \end{pmatrix} + \begin{pmatrix}
    (r K^T K)^{-1} & 0 \\
   r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
     r K^T X_{k+1} - r K^T K z_k - K^T H_k\\
     X_{k+1} - Kz_k
    \end{pmatrix}
\end{equation}
We can then verify that the iterations indeed are the same as ADMM iterations. Namely, we have for $X_{k+1}$ update, that $A_r X_{k+1} - r Kz_{k} - H_k = 0 $, that is 
\begin{equation}
\nabla F(X_{k+1}) + r X_{k+1} - r K z_k - H_k = 0
\end{equation}
and for $z_{k+1}$ update, that \begin{equation}
\begin{aligned}
z_{k+1} &= z_k + (r K^T K)^{-1} (r K^T X_{k+1} - K^T H_k) - z_k \\
& = (r K^T K)^{-1} (r K^T X_{k+1} - K^T H_k)
\end{aligned}
\end{equation}
and for $H_{k+1}$ update, that  \begin{equation}
\begin{aligned}
H_{k+1} & = H_k +  r K(r K^T K )^{-1}\left( r K X_{k+1} - r K^T K z_k - K^T H_k \right) -r (X_{k+1} - Kz_k) \\
                & = H_k + r K(r K^T K )^{-1} (r K X_{k+1} - K^T H_k) - r K z_k - r (X_{k+1} - Kz_k) \\
                & = H_k +r K z_{k+1} -r X_{k+1}.
\end{aligned}
\end{equation}
Thus, we have 
\begin{equation}
H_{k+1} - H_k -r(Kz_{k+1} - X_{k+1})=0.
\end{equation}
This completes the proof. 
\end{proof}
We shall now set $E_k^U = U - U_k$ and $E_k^X = X - X_k$ and then shall drive the error equation for the above algorithm. First we note that 
\begin{equation}
X = A_r^* (-B^TH) \quad \mbox{ and } \quad X_{k} = A_r^* (-B^T H_{k-1})
\end{equation}
Thus, we have that 
\begin{eqnarray*}
\|X - X_k\|^2 = \|A_r^* (-B^TU) - A_r^* (-B^T U_{k-1})\|^{2} \leq \frac{1}{c_0 + r} \|B^T( U - U_{k-1})\|^2 
\end{eqnarray*}
Therefore, we shall only need to analyze the convergence of $E_k^U$ for the convergence of $E_k^X$. On the other hand, the error propagation operator for $E_k^U$ can be given as follows: 
\begin{equation}
E_{k+1}^U = E_k^U - N^{-1} (S(U) - S(U_k)). 
\end{equation}
We now introduce an inner product defined on the space $U = \{(z_k, H_k)_{k=1,\cdots}, (z_*, H_*) \}$ by 
\begin{equation}
\langle \overline{N} U_k, U_k \rangle = \left \langle 
\begin{pmatrix}
I & 0\\
0 & -I
\end{pmatrix}
\begin{pmatrix}
r K^T K & 0\\
K & -\frac{1}{r} I
\end{pmatrix} \begin{pmatrix}
z_k \\
H_k
\end{pmatrix},  \begin{pmatrix}
z_k \\
H_k
\end{pmatrix} \right \rangle = r \langle Kz, Kz \rangle + \frac{1}{r} \langle H, H \rangle  
\end{equation}
Therefore, it makes a norm. We now notice that
\begin{eqnarray} 
\overline{N} E_{k+1}^U = \overline{N} E_k^U - (\overline{S}(U) - \overline{S}(U_k)), 
\end{eqnarray} 
where 
\begin{equation} 
\overline{S} = \begin{pmatrix}
I & 0\\
0 & -I
\end{pmatrix} S.
\end{equation} 
This indicates that 
\begin{eqnarray*}
\|\overline{N} E_{k+1}^U\|^2 = \overline{N} E_k^U - (\overline{S}(U) - \overline{S}(U_k)), 
\end{eqnarray*}

\begin{comment} 
\begin{lemma}
For the error propagation matrix, we have $\rho(I - N^{-1}S) =\eta < 1$. 
\end{lemma}
\begin{proof}
\textcolor{red}{We found this part is not accurate and need to modify.
First, we observe that $I - N^{-1}S$ is given as follows:


where $E := K^T K = n I$, $Q_A = I - r A_r^{-1}$. We note that 

By $2\times 2$ block matrix decomposition, we have the error propagation matrix is spectrally equivalent to the following matrix
\begin{equation}\label{mat1}
    \begin{pmatrix}
   S1 & 0 \\
 0   &
 S_2 + S_3   \end{pmatrix},
\end{equation}
where $S_1 = I -   E^{-1} K^T Q_A K$, $S_2 = (I - M_2)Q_A $, $S_3 = (I- M_2) Q_A K (I - E^{-1} K^T Q_A K )^{-1} E^{-1} K^T Q_A $.

It is easy to see that $S_1$ is symmetric positive and has $\rho(S_1) < 1$.
We also have $\rho(S_2) \leq \frac{C_0}{r + C_0}$, $\rho(S_3) \leq \frac{C_0}{ r+C_0}\frac{r +C_0}{r} \frac{C_0}{r +C_0 }$. So $\rho(S_2 + S_3) \leq \frac{C_0}{r} \le 1 $ for sufficiently large $r$. 

We can then take $\eta = \min \{ \rho(S_1), \frac{C_0}{r} \}$. This completes the proof.}
\end{proof}
\end{comment} 

\subsubsection{linear case}
To analyze the above Uzawa iteration, the difficulty lies in the nonlinear operation $\nabla F(\cdot)$ since F is assumed to be strongly convex and L-smooth. We first consider a simpler case when $F(X) = \frac{1}{2} X^T A X - b^T X$ is a quadratic function, where $A$ is a symmetric positive definite matrix. In this case, we have a system of linear equations. 

\begin{equation}
\label{optimality condition in linear case}
    \begin{pmatrix}
    A_r & -r K & -I \\
    -r K^T& r K^TK & K^T\\
    -I& K & 0\\
    \end{pmatrix}
    \begin{pmatrix}
    X_*\\
    z_* \\
    H_*
    \end{pmatrix} = 
    \begin{pmatrix}
    f\\
    0\\
    0
    \end{pmatrix}
\end{equation}

\begin{lemma}
$I - rA_r^{-1}$ is symmetric positive definite, and $\rho(I - rA_r^{-1}) < \frac{c_{max}}{r + c_{max}}$, where $c_{max}$ is the largest eigenvalue of $A$. $\rho(A_r^{-1}) \leq \frac{1}{r + c_{min}}$, where $c_{min}$ is the smallest eigenvalue of $A$.
\end{lemma}

\begin{theorem}[Well-posedness]
The matrix \eqref{optimality condition in linear case} is invertible. 
\end{theorem}
\begin{proof}
We know that $A_r$ is invertible since it is positive definite. 
Using the same notation as in \eqref{optimality condition aug Lag matrix form 2 by 2}, it now suffices to prove that the Schur complement $S = C - B A_{r}^{-1} B^T$ is invertible. To see this, we write out the schur complement explicitly.
We have 
\begin{equation}
    B A_r^{-1} B^T = \begin{pmatrix}
    r^2 K^T A_r^{-1} K&  r K^T A_r^{-1}\\
    r A_r^{-1} K & A_r^{-1}
    \end{pmatrix}
\end{equation}
\begin{equation}
    S = \begin{pmatrix}
    r K^T \left(I - r A_{r}^{-1} \right)K  & K^T (I -  r A_{r}^{-1} ) \\
    (I -  r A_{r}^{-1} ) K  & - A_{r}^{-1}
    \end{pmatrix}
\end{equation}
Note that in $S$, the first diagonal block is positive definite because $I - r A_{r}^{-1}$ is positive definite. 

Now it suffices to again consider the Schur complement of $S$. We have 
\begin{equation}
    \tilde{S} = -A_r^{-1} - D \left[ r K^T \left(I -  r A_{r}^{-1} \right)K  \right] D^T  ,
\end{equation}
where $D = (I - (\frac{1}{r} A_r)^{-1}) K$.
Note that $D \left[ r K^T \left(I - (\frac{1}{r} A_{r})^{-1} \right)K  \right] D^T $ is positive semi-definite. Since $A_r$ is positive definite, we have $\tilde{S}$ is negative definite. 
\end{proof}

\subsubsection{Exact Uzawa for linear problems}
We consider the following exact Uzawa iteration,
\begin{equation}
    \begin{cases}
     X_{k+1} = A_r^{-1}f - A_r^{-1}B^T U_k, \quad A_r X_{k+1} +B^T U_k = f \\
     U_{k+1} = U_k + N^{-1} \left( B A_r^{-1} f - (C - B A_r^{-1}B^T)U_k \right) = U_k + N^{-1} \left( - B X_{k+1} - C U_k\right),
    \end{cases}
\end{equation}
where $N$ is some approximation of the Schur complement operator given by 
\begin{equation}
    N = \begin{pmatrix}
    r K^T K & 0\\
     K & - \frac{1}{r} I
    \end{pmatrix} = \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix}^{-1}.
\end{equation}

The exact solution satisfies 
\begin{equation}
    \begin{cases}
     X = A_r^{-1}f - A_r^{-1}B^T U \\
     U = U + N^{-1} \left( B A_r^{-1} f - (C - B A_r^{-1}B^T)U \right) = U + N^{-1} \left( - B X - C U\right),
    \end{cases}
\end{equation}
Denoting $E^X_{k} = X_{k} - X$, $E^U_{k} = U_k - U$, we write out the error equation as follows.
\begin{eqnarray}
E^X_{k+1} &=& - A_r^{-1}B^T (U_k - U) \\
E^U_{k+1} &=& E^U_{k} + N^{-1} \left( -(C - B A_r^{-1}B^T) E^{U}_k \right) = (I - N^{-1}S) E^U_{k}
\end{eqnarray}
With the above error equations, it is not hard to see that the convergence depend on the spectral radius of the error propagation matrix $I - N^{-1}S$. 

\begin{lemma}
The following is our goal: 
$\|I - N^{-1}S\| = \delta < 1$
for some norm. 
\end{lemma}
\begin{proof}
The error transfer operator is given as follows: 
\begin{eqnarray*}
I - N^{-1}S &=& 
\left ( \begin{matrix}
I & 0 \\ 
0 & I 
\end{matrix} \right ) - \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
    r K^T \left(I - r A_{r}^{-1} \right)K  & K^T (I -  r A_{r}^{-1} ) \\
    (I -  r A_{r}^{-1} ) K  & - A_{r}^{-1}
    \end{pmatrix}   \\
&=& \begin{pmatrix}
I - (K^TK)^{-1} K^T (I - r A_r^{-1}) K&  -\frac{1}{r} (K^TK)^{-1} K^T (I - r A_r^{-1}) \\
r(I - K (K^TK)^{-1} K^T) (I - r A_r^{-1}) K  & (I - K (K^TK)^{-1} K^T) (I - r A_r^{-1})
\end{pmatrix},
\end{eqnarray*}
%where $E := K^T K = n I$, $Q_A = I - r A_r^{-1}$. We note that 
%
%By $2\times 2$ block matrix decomposition, we have the error propagation matrix is spectrally equivalent to the following matrix
%\begin{equation}\label{mat1}
%    \begin{pmatrix}
%   S1 & 0 \\
% 0   &
% S_2 + S_3   \end{pmatrix},
%\end{equation}
%where $S_1 = I -   E^{-1} K^T Q_A K$, $S_2 = (I - M_2)Q_A $, $S_3 = (I- M_2) Q_A K (I - E^{-1} K^T Q_A K )^{-1} E^{-1} K^T Q_A $.

%It is easy to see that $S_1$ is symmetric positive and has $\rho(S_1) < 1$.
%We also have $\rho(S_2) \leq \frac{C_0}{r + C_0}$, $\rho(S_3) \leq \frac{C_0}{ r+C_0}\frac{r +C_0}{r} \frac{C_0}{r +C_0 }$. So $\rho(S_2 + S_3) \leq \frac{C_0}{r} \le 1 $ for sufficiently large $r$. 

%We can then take $\eta = \min \{ \rho(S_1), \frac{C_0}{r} \}$. This completes the proof.
\end{proof}

\begin{theorem}
For quadratic objective 
\end{theorem}

Now we extend our result to a nonlinear $\nabla F(X)$. 

\subsubsection{Inexact Uzawa for linear problems}


The inexact Uzawa iteration is given as follows. 

\begin{equation}
    \begin{cases}
     X_{k+1} = X_k + N_1^{-1}(-B^T U_k - A_r X_k), \quad A_r X_{k+1} +B^T U_k = f \\
     U_{k+1} = U_k + N^{-1} \left( - B X_{k+1} - C U_k\right),
    \end{cases}
\end{equation}
where $N_1^{-1}$ is the solver used for the $X$ part, e.g. one step of gradient descent or several steps. $N^{-1}$ is the same linear solver as before. 

We assume the solver $N_1^{-1}$ satisfies  $N_1^{-1}(0) = 0$. This is true for both linear solver and some nonlinear solvers, including several steps of gradient descent.


Under this assumption, the exact solution satisfies the following equations. 

\begin{equation}
    \begin{cases}
     X = X + N_1^{-1}(-B^T U - A_r X)\\
     U = U + N^{-1} \left( - B X - C U\right),
    \end{cases}
\end{equation}
Taking the difference, one can obtain the following error equations. 

\begin{eqnarray}
 E^X_{k+1} & = & E^X_{k} + N_1^{-1} (-B^T U_k - A_r X_k) - N_1^{-1} (-B^T U - A_r X) \\
E_{k+1}^U &=& E_{k}^U + N^{-1} (- B E^X_{k+1} - C E^U_k).
\end{eqnarray}

If $N_1^{-1}$ is a linear solver, we have the follow error equation due to linearity. 
\begin{eqnarray}
 E^X_{k+1} & = & E^X_{k} + N_1^{-1} (-B^T E^U_k - A_r E^X_k) \\
 E_{k+1}^U &=& E_{k}^U + N^{-1} (- B E^X_{k+1} - C E^U_k).
\end{eqnarray}
Writing it in the matrix form, we have 
\begin{equation}
    \begin{pmatrix}
     I & 0\\
     N^{-1} B & I
    \end{pmatrix} 
    \begin{pmatrix}
    E^X_{k+1} \\
    E^U_{k+1}
    \end{pmatrix} 
  = \begin{pmatrix}
    I - N_1^{-1} A_r & - N_1^{-1} B^T \\
    0 & I - N^{-1} C
    \end{pmatrix}
    \begin{pmatrix}
    E^X_{k} \\
    E^U_{k}
    \end{pmatrix}  . 
\end{equation}
Rearranging, we have 
\begin{equation}\begin{pmatrix}
    E^X_{k+1} \\
    E^U_{k+1}
    \end{pmatrix} = 
        \begin{pmatrix}
     I & 0\\
     - N^{-1} B & I
    \end{pmatrix} 
    \begin{pmatrix}
    I - N_1^{-1} A_r & - N_1^{-1} B^T \\
    0 & I - N^{-1} C
    \end{pmatrix}
    \begin{pmatrix}
    E^X_{k} \\
    E^U_{k}
    \end{pmatrix}  
\end{equation}
Now, we analyse the spectral radius of the error propagation matrix. 

We have for an iterative solver like Richardson's iteration (sufficiently small step size), 
$\rho(I - N_1^{-1} A_r) \leq \| I -N_1^{-1} A_r \| \leq 1 $. 

Furthermore, we compute 
\begin{equation}
\begin{aligned}
      I - N^{-1} C & = I - \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
    r K^TK & K^T \\
    K & 0
    \end{pmatrix} \\
    & = I - \begin{pmatrix}
    I &   (r K^T K)^{-1} K^T\\
     0 &  rK(rK^TK)^{-1}K^T 
    \end{pmatrix}\\
    & = \begin{pmatrix}
    0 & - (r K^T K)^{-1} K^T\\
    0 & I - K(K^TK)^{-1}K^T
    \end{pmatrix}
\end{aligned}
\end{equation}



\textcolor{red}{If $N^{-1}$ is a nonlinear solver, e.g. several steps of gradient descent or iterative solvers. 
}
 
\section{Appendix} 



 
\begin{algorithm}\label{alginexact4} \caption{Uzawa for $L_r$ with a single step Gradient Descent}
ADMM updates are as follows. 
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
Apply one step GD to find $X_{k+1}$ for $L_r$: 
\begin{equation}\label{gdADMM}
\nabla F(X_{k}) - H_k - r (Kz_k - X_{k+1}) = 0 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
\end{equation} }
\State{Update the Lagrange multiplier: \begin{equation} 
H_{k+1} - H_k - r (K z_{k+1} - X_{k+1}) = 0. 
\end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Assume that $K^TH_0 = 0$. Then Algorithm produces 
$Y_k = [\overline{z_k}; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [\overline{z_*}; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|H_{k+1} - H_* \|^2 \leq \frac{1}{1+\delta} \| H_{k} - H_*\|^2,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|H_{k} - H_* \|^2_C
\end{equation}
\end{theorem}

\begin{proof}
We have the following identity holds: 
\begin{eqnarray*}
\nabla F(X_{k}) - \nabla F(X_*) &=& r( K z_{k} - K {z_{k+1}}) + H_{k+1} - H_* \\
r M_1 (X_{k+1} - X_*) &=& - H_{k+1} + H_k \\
M_2 (X_{k+1} - X_*) &=& K(z_{k+1} - z_*)
\end{eqnarray*}
Let 
\begin{equation}
X = X_{k+1} - \frac{1}{r}H_k \quad \mbox{ and } \quad Y = X_* - \frac{1}{r}H_*. 
\end{equation}
We then see that 
\begin{eqnarray*}
\|Kz_{k+1} - X_* \|^2 &=& \|M_2(X) - M_2(Y)\|^2% \langle X_{k+1} - X_*, X_{k+1} - X_* \rangle \\ 
%&=& \left \langle Kz_k + \frac{1}{r} H_k - \frac{1}{r} \nabla F(X_k) - X_*, Kz_k + \frac{1}{r} H_k - \frac{1}{r} \nabla F(X_k)\right \rangle \\
%&=& \left \langle (X_k - X_*) - \frac{1}{r} (\nabla F(X_k) - \nabla F(X_*)), (X_k - X_*) - \frac{1}{r} (\nabla F(X_k) - \nabla F(X_*))\right \rangle \\
%&\leq& \left ( 1 - \frac{\lambda}{r} \right )  \|X_k - X_*\|^2. 
\end{eqnarray*}
On the other hand, we also observe that
\begin{eqnarray*}
&& \|H_{k+1} - H_* \|^2 = \langle H_{k} + r (Kz_{k+1} - X_{k+1}) - H_*, H_{k} + r (Kz_{k+1} - X_{k+1}) - H_* \rangle \\ 
&& = \left \langle r \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - H_*, r \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - H_* \right \rangle \\
&& = r^2 \left \langle \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \frac{1}{r} H_*, \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \frac{1}{r} H_* \right \rangle \\
&&= r^2 \left \langle \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \left ( X_* - \left (X_* - \frac{1}{r} H_* \right ) \right ), \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \left ( X_* - \left (X_* - \frac{1}{r} H_* \right ) \right ) \right \rangle \\
&&= r^2 \left \langle \left( M_2(X) - X \right ) - \left ( M_2(Y) - Y \right ),  \left( M_2(X) - X \right ) - \left ( M_2(Y) - Y \right ) \right \rangle \\
&&= r^2 \|(M_2(X) - X) - (M_2(Y) - Y)\|^2.  
%X_{k+1} - \frac{1}{r}H_k \right ) - \left (  X_* - \frac{1}{r} H_* \right ) \right \|^2 = r^2 \left \| \left ( X_{k+1} - X_* \right ) - \frac{1}{r} \left ( H_k - H_* \right ) \right \|^2 \\
%&&= r^2 \|X_{k+1} - X_*\|^2 - 2r^2 \left \langle X_{k+1} - X_*, \frac{1}{r} \left ( H_k - H_* \right ) \right \rangle + \|H_k - H_*\|^2.
%&&= r^2 \|X_{k+1} - X_*\|^2 - 2 \left \langle W_{k+1} - W_* - , r \left ( H_k - H_* \right ) \right \rangle + \|H_k - H_*\|^2. 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
\|X_{k+1} - X_*\|^2 + \frac{1}{r^2} \|H_{k+1} - H_*\|^2 &\leq& \|X_{k+1} - \frac{1}{r}H_k - (X_* - \frac{1}{r} H_*))\|^2 \\
&=& \| (X_{k+1} - X_*) - \frac{1}{r}(H_k - H_*))\|^2 \\
&=& \| W_k - W_*\|^2. 
%- 2 \left \langle W_{k} - W_* + \frac{1}{r} \left ( H_k - H_* \right ), \frac{1}{r} \left ( H_k - H_* \right ) \right \rangle + \frac{1}{r^2} \|H_k - H_*\|^2 \\ 
%&=& \|W_{k} - W_*\|^2, 
\end{eqnarray*} 
where $W_k = X_k - \frac{1}{r} \nabla F(X_k)$. 
\end{proof}
\begin{remark}
The use of nonexpansiveness is to obtain $\nabla F(\cdot)$ from two different $\nabla F - H_k$ and $\nabla F - H_*$. 
\end{remark}

\section{$2\times2$ block system}
In this section, we focus on the convergence analysis for the $2\times2$ block system involving the solution of $X, z$ with $H$ being fixed. This is a subproblem from the previous $3\times3$ system.  

We consider convergence analysis of algorithms for the following system.

\begin{equation} \label{2by2}
\begin{aligned}
\nabla F(X_{*}) - H - r (Kz_{*} - X_{*}) &= 0, \\
K^T H + r K^T (Kz_{*} - X_{*}) &= 0.
\end{aligned}
\end{equation}
In operator notation, we have 
\begin{equation}
\begin{pmatrix}\label{2by2abstract}
A_r & B^T\\
B & C
\end{pmatrix} 
\begin{pmatrix}
X_*\\
z_*
\end{pmatrix}= 
\begin{pmatrix}
f  \\
g 
\end{pmatrix},
\end{equation}
where $A_r = \nabla F(\cdot) + r I $, $B = -r K^T$, $C = rK^T K $ $f = H, g = -K^T H$. 


\begin{algorithm}
\caption{Block Gauss Seidel for $2 \times 2$ system \eqref{2by2}}
\label{algexact5}
\begin{algorithmic}
\State{Given initial $X_0, z_0$.}
\For{$k=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
\begin{equation}
\nabla F(X_{k+1}) - H - r (Kz_k - X_{k+1}) = 0 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
K^T H + r K^T (Kz_{k+1} - X_{k+1}) = 0,
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
For general $\lambda-$strongly convex and $L-$smooth objective function $F(X)$, we have for Algorithm \ref{algexact5}, 
\begin{equation}
    \|X_{k+1} - X_{*} \| \leq \frac{r}{r + \lambda} \| \bar{z}_k - \bar{z}_* \|.
\end{equation}
\end{theorem}

\begin{proof}
From the updates in Algorithm \ref{algexact5} and the optimality condition, we obtain the error equation as follows. 
\begin{eqnarray}
 \nabla F(x_{k+1}) - \nabla F(x_*) & = & r (\bar{z}_k - \bar{z}_*) - r(X_{k+1} - X_*)
%  a new line & = & continues
\end{eqnarray}
By $\lambda-$strong convexity, we have 
\begin{equation*}
\begin{aligned}
    \lambda \|X_{k+1} - X_* \|^2 &\leq \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*) \rangle \\
    & = \langle X_{k+1} - X_*, r(\bar{z}_k - \bar{z}_* )\rangle - \langle X_{k+1} - X_*, r (X_{k+1} - X_*) \rangle 
\end{aligned} 
\end{equation*}
Rearranging and applying Cauchy-Schwarz inequality, we can obtain the required inequality. 
\end{proof}

% \begin{lemma}
% For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the block Gauss Seidel iteration in Algorithm \ref{algexact5} corresponds to the exact Uzawa method with the following ierative method for the Schur complement: 
% \begin{equation}
%     z_{k+1} = z_{k} + N^{-1} (g - B X_{k+1} - Cz_{k}),
% \end{equation}
% where $N = C$. 
% \end{lemma}

\begin{theorem}
For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the block Gauss Seidel iteration in Algorithm \ref{algexact5} corresponds to the exact Uzawa method with the following ierative method for the Schur complement: 
\begin{equation}
    z_{k+1} = z_{k} + N^{-1} (g - B X_{k+1} - Cz_{k}),
\end{equation}
where $N = C$. 

Furthremore, we have the following convergence result
\begin{eqnarray}
     \| z_{k+1} - z_*\| &\leq& \left( \frac{r}{r +c_0}\right)^{k+1 } \| z_0 -z_*\|, \\
    \|X_{k+1} -X_* \| &\leq& \frac{r}{r + \lambda} \left( \frac{r}{r + c_0}\right)^k \|z_0 - z_* \|,
\end{eqnarray}
where $c_0$ is the smallest eigenvalue of $A$. 
\end{theorem}
\begin{proof}
Note that $C = rK^TK$ is invertible. A direct calculation can show that it is indeed same as the update in $z_{k+1}$ in Algorithm \ref{algexact5}. 

For simplicity, we absorb $b$ into the right hand side $f$. 

Konwing that 
\begin{equation}
    X_{k+1} = A_r^{-1} f - A_r^{-1} B^T z_{k},  
\end{equation}
we eliminate $X_{k+1}$ from the construction of $z_{k+1}$ to have the iteration 
\begin{equation}
    z_{k+1} = z_{k} + C^{-1} \left (g - B A_r^{-1}f - (C - B {A_r}^{-1} B^T) z_k \right),
\end{equation}
which is the iteration applied to the Schur complement system: 
\begin{equation}
    (C - B {A_r}^{-1} B^T) z = g - B A^{-1} f.
\end{equation}
The convergence in error $\| z_{k+1}-z_* \|$ depend on  $\rho( I - C^{-1} (C - B {A_r}^{-1} B^T))$.
\begin{equation*}
\begin{split}
\rho( I - C^{-1} (C - B {A_r}^{-1} B^T)) &=  \rho ( C^{-1} B {A_r}^{-1} B^T) \\
&\leq \rho( (rK^T K)^{-1} rK^T (A + rI)^{-1} rK )   \\ 
& \leq  \frac{r}{r + c_0}.
\end{split}
\end{equation*}
Therefore, we have 
\begin{equation}
    \| z_{k+1} - z_*\| \leq \left( \frac{r}{r +c_0}\right)^{k+1 } \| z_0 -z_*\|. 
\end{equation}

\end{proof}

\begin{algorithm}
\caption{Inexact Uzawa for $2 \times 2$ system \eqref{2by2abstract} }
\label{algexact6}
\begin{algorithmic}
\State{Given initial $X_0, z_0$.}
\For{$k=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
\begin{equation}
X_{k+1} = X_k + \omega I (f - A_r X_k - B^T z_k), 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
z_{k+1} = C^{-1} (g - B X_{k+1} - Cz_k)
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

For Algorthm \ref{algexact6}, we define $E^X_{k} = X_k - X_*$, $e^z_{k} = z_k - z_*$ the error equations are given by 
\begin{eqnarray}
E^X_{k+1} &=& E^X_{k} + wI\left( -A_rE^X_{k} - B^Te^z_k \right) \label{erroreqn1}\\
e^z_{k+1} &=& -C^{-1}B E^X_{k+1} \label{erroreqn2}
\end{eqnarray}
    
\begin{theorem}
For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the Algorithm \ref{algexact5} has the following convergence results for some $\eta<1$ if $\omega$ is sufficiently small

\begin{equation}
    \|X_{k+1} - X_*\| \leq \eta^{k+1} \|X_{0} - X_* \| 
\end{equation}

\begin{equation}
    \|Kz_{k+1} - K z_*\| \leq \eta^{k+1} \|X_{0} - X_* \| 
\end{equation}
\end{theorem}

\begin{proof}
Substituting \eqref{erroreqn2} into \eqref{erroreqn1}, we have 
\begin{equation}
    E^X_{k+1} = E^X_{k} + wI\left( -A_rE^X_{k} + B^T C^{-1} B E^X_{k} \right)
\end{equation}
Then convergence of $\|E^X_{k+1} \|$ depends on $\rho(I  - w ( A_r - B^T C^{-1}B) )$. 

Note that $\lambda_{\text{min}}(A_r) = c_0 + r$, and $B^T C^{-1} B = r K (K^TK)^{-1} K^T$, which is symmetric and has spectral radius $r$. We have $A_r - B^T C^{-1} B$ is symmetric positive definite. Therefore, by choosing sufficiently small step size $\omega$ for the Richardson's iteration, we have for some $\eta < 1$
\begin{equation}
    \|E^X_{k+1}\| \leq \eta^{k+1} \|E^X_{0} \| 
\end{equation}

Furthermore, multiplying \eqref{erroreqn2} by $K$ and considering the norm, we have 
\begin{equation}
    \begin{aligned}
       \|Ke^z_{k+1} \| &= \|KC^{-1}B E^X_{k+1}\| \\
        & = \| K(K^T K)^{-1} K^T  E^X_{k+1}\| \\
        & \leq \|E^X_{k+1}\|  \leq \eta^{k+1} \|E^X_{0} \| 
    \end{aligned}
\end{equation}
\end{proof}




\begin{algorithm}\label{alg:inexactADMM2}
\caption{ADMM for $L_r$ with GD with for the first step}
ADMM updates are as follows. 
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_{t+1}$ update: 
    solve $\nabla F(X_{t+1}) - H_t - r (Kz_t - X_{t+1}) = 0$ using gradient descent as follows.
    \For{$ k = 0,..., K-1$}
      \begin{equation}\label{gd for ADMM}
        \begin{split}
            X_{t+\frac{k+1}{K}} & = X_{t+\frac{k}{K}} - \lambda \left(\nabla F(X_{t+\frac{k}{K}})  - H_k - r(K z_k - X_{t + \frac{k}{K}}) \right) \\
            & = (1 - \lambda r)X_{t+\frac{k}{K}} + \lambda r K z_k - \lambda \left(\nabla F(X_{t+\frac{k}{K}})  - H_k \right)
        \end{split}
    \end{equation}
    \EndFor
    }
    \State{$z_{t+1}$ update: 
        \begin{equation} 
        K^T H_k + r K^T (Kz_{t+1} - X_{t+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} 
        H_{t+1} - H_t - r (K z_{t+1} - X_{t+1}) = 0. 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}



\textcolor{red}{How to analyze the convergence rate in the following two cases?}

\textcolor{red}{If we consider $X$ part, and consider $z$, $H$ part together, then this is an exact Uzawa, with an iterative method for the Schur complement. See Propostition \ref{prop: Uzawa iterative solver}. }

\textcolor{red}{If we consider the $X$, $z$ together, then it is an inexact Uzawa with the first block solved by one step of block GS and the second part solved by Richardson iteration.}


\newpage 

\begin{section}{Uzawa iterations}
This section contains mathematical theory on Uzawa iterations that are related to our problem. 
\end{section}

\section{Introduction}

Data becomes increasingly decentralized and the privacy of individual data is an utmost importance in the digital age \cite{house2012consumer, cai2021deepstroke,chen2020ai,luo2020arbee,wang2020panel}. Unlike standard machine learning approaches, \textit{Federated learning} (FL) encourages each client to have a local training data set, which will not be stored to the server and  to update the local correction of the current global model maintained by the main server via the local data and local gradient descent method. {FL} has been used successfully in many different areas, which include Internet of Things (IoT) applications \cite{hwang2015iot, ferrag2021federated}. {FL} can be modeled as the optimization problem given as 
\begin{equation}\label{FL}
\min_{x \in X} \left \{ E(x) = \sum_{k=1}^N f_k (x) \right \},
\end{equation} 
where $X$ is a parameter space, $N$ is the number of clients or devices, and $f_k \colon X \rightarrow \mathbb{R}$, $1 \leq k \leq N$, is a local objective function for the $k^{\rm th}$ worker. The local objective function $f_k$ depends on the data of the $k^{\rm th}$ worker, but not on those of the other clients. The standard Federated Avgerage ({\textit{FebAvg}}) algorithm consists of three steps
\begin{enumerate}
\item  the central server broadcasts the latest model $x_t$, to all the clients;
\item every worker, say $k^{\rm th}$ worker, lets $x_t^k = x_t$ and then performs one or few local updates with learning rate $\gamma$ 
  $$x_{t+1}^k \leftarrow x_t^k - \gamma \nabla f_k(x_t^k)$$
\item the server then aggregates the local models, $x_{t+1}^1, \cdots x_{t+1}^N$, to produce the new global model $x_{t+1}$ \cite{konevcny2016federated}.
  \end{enumerate}

Figure \ref{fig:scheme} illustrates a simple single local gradient descent algorithm (Local GD) and the arrows indicate the communications, which poses a bottleneck of the algorithm because it is generally orders of magnitude more expensive than the local computations and more communications make the algorithm more vulnerable for cybersecurity. Recent methods, therefore, aim at enhancing the privacy of FL by using reduced model or even sacrificing system efficiency. However, {\textbf{providing privacy has to be carefully balanced with system efficiency}} \cite{li2020federated}. Figure \ref{fig:scheme2} shows that the Local GD without applying the shifted gradient can reach the lower accuracy faster, but it does not converge eventually. One recent algorithm, called Scaffnew or ProxSkip, is shown to achieve the best communication efficiency, without sacrificing the convergence property, until today \cite{mishchenko2022proxskip}. Scaffnew reformulates ~\cref{FL} as follows:
\begin{equation}\label{cp}
\min_{x_1, \cdots, x_N \in X} \left \{ \frac{1}{N} \sum_{i=1}^N f_i(x) + \psi(x_1,\cdots,x_N) \right \},
\end{equation} 
where $\psi$ is a proper closed convex function introduced for a consensus reformulation, which can be interpreted as some average of parameters $x_1,\cdots,x_N$, from each client. Then it applies the proximal gradient descent method. The novelty of Scaffnew is at the introduction of certain shift, denoted by $h_t$, leading to the modified grandient, i.e., $\widetilde{\nabla}f(x_t) = \nabla f(x_t) - h_t$. This can be considered as a type of the preconditioner, which leads to the solution for the consensus reformulation even with applying the proxy operator once in a while. 

%The focus of our tasks is to develop mathematical foundation of FL and to design effective training algorithms for FL using ideas from numerical analysis and mathematical optimization so as to push the boundaries of the current state of the art in FL \cite{li2019convergence,zhou2022convergence,haddadpour2019convergence,mitra2021linear}. 

The focus of this proposal is to push the boundary of theoretical convergence analysis for the currently available for {\textit{FebAvg}}. The convergence theory of Scaffnew was established for strongly convex problems \cite{mishchenko2022proxskip}. We also present improved FL models that can provide better privacy without sacrificing convergence property of scheme. These two goals will be achieved by viewing \textit{FebAvg} within the framework of subspace correction methods.  

To expedite the success of the proposed studies, we will team up with people of expertises in numerical analysis, mathematical optimization, and machine learning. The project leader Jinchao Xu will closely collaborate with Young Ju Lee (Texas State) and other close collaborators such as Qingguo Hong (Penn State), Xiaofeng Xu (Penn State) and Jongho Park (KAIST).  J. Xu's research interests include mathematical foundation of machine learning,  approximation theory for deep neural networks, design of convolutional neural networks from multigrid viewpoint, and development of training algorithms based on subspace correction --- a general framework containing a large class of optimization algorithms such as coordinate descent method and federated learning. Lee has worked on successive subspace corrections for nontrivial problem, such as nearly or singular problem, which fits in the current project since objective functionals are typically nearly singular \cite{chen2020robust,lee2009robust,LWXZ:2007}. Hong has an expertise in analysis of preconditioners for coupled nonlinear systems \cite{hong2016uniformly,hong2016robust,chen2020robust}. 
Park has several interesting results on parallel subspace correction methods for mathematical optimization problems~\cite{Park:2020,Park:2021,Park:2022}.



We consider to solve 
\begin{equation} 
\min_{x \in \Reals{d}} f(x) + \psi(x), 
\end{equation}
where $f : \Reals{d} \rightarrow \Reals{}$ is a smooth function and $\psi : \Reals{d} \rightarrow \Reals{} + \{+\infty\}$ is a proper, closed and convex regularizer. 

We consider the constrained optimization. Namely, for some $\mathcal{C} \subset \Reals{d}$, we consider to minimize  
\begin{equation}\label{main:eq}  
\min_{x \in \mathcal{C}} f(x) \quad \mbox{ or equivalently, } \quad \min_{x \in \Reals{d}} f(x) \quad \mbox{ subject to } x \in \mathcal{C}.  
\end{equation}
By defining $\psi : \Reals{d} \mapsto \Reals{}$ by 
\begin{equation}
\psi(x) :=  \left \{ \begin{array}{cc} 
0, & \mbox{ if } x \in \mathcal{C} \\
+\infty, & \mbox{ otherwise } 
\end{array} \right . 
\end{equation}
the problem \eqref{main:eq} can be formulated into 
\begin{equation} 
\min_{x \in \Reals{d}} f(x) + \psi(x).  
\end{equation}

\newpage 


\section{Federated learning and ADMM}
Notation:

$x \in \mathbb{R}^d$, $X = (x_1, x_2 ,\dots,x_n) \in \mathbb{R}^{d\times n}$, where $x_i \in \mathbb{R}^d$.

In federated learning, the objective function is $f(x) = \frac{1}{n} \sum_{i = 1}^n f_i(x)$,where each $f_i$ is the local objective function for each client $i$. 

We have the following formulations of the minimization problem in federated learning. 

\begin{itemize} 
    \item Original formulation \begin{equation}\label{Fed: original}
        \min_{x \in \mathbb{R}^d} f(x) = \frac{1}{n} \sum_{i = 1}^n f_i(x)
    \end{equation} 


\end{document} 

\subsection{Study of Convergence for the algorithm for the Total System} 
In this section, we discuss the convergence of the iterative method based on inexact Block Gauss-Seidel for $U$ block and Richardson for $H$ block. The Algorithm can be written as given in the Algorithm \ref{algADMM3}.
\begin{algorithm}
\caption{Federated Learning formulation of FL}\label{algADMM3} 
Given $H_0$ such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,K-1$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$),  
    \begin{equation} \label{Xupdateyy}
    X_{k+1} = G_{n,r}(X_k;H_k + r Kz_k),
\end{equation} }
    \State{$z_{t+1}$ update: 
        \begin{equation} \label{zupdate}
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} \label{Hupdate1}
        H_{k+1} = H_k + \omega (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}
We note that the action of the operator $G_{n,r}$ depends on $X_k$. The case when $G_{n,r}$ is the standard $n-$step GD, the algorithm can be written in a very standard way as given in Algorithm \ref{GD1}. 
\begin{algorithm}
\caption{The case that $G_{n,r}$ is the Standard GD}\label{GD1} 
Given $H_k$ and $z_k$ with $K^TH_k = 0$, update for $X_{k+1}$ is obtained as follows:  
\begin{algorithmic}
\For{$\ell=1,2,\cdots,n$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$ and $b_{H,rKz} = H_k + rKz_k$),  
    \begin{equation} \label{Xupdate0}
    X_{k+\ell/n} = X_{k+(\ell-1)/n} + \gamma (b_{H,rKz} - A_r X_{k+(\ell-1)/n})
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

In this section, we shall discuss the convergence of Algorithm 1. We first begin our discussion for the standard GD case. The standard $n-$step GD method to solve the following system for $X$:
\begin{equation}
A_r X = H_k + rKz_k. 
\end{equation}
will be given as follows: with $X_k = Kz_k$, 
\begin{subeqnarray} 
X_{k+\frac{1}{n}} &=& X_{k} + \gamma (H_k 
+ rKz_k - A_r X_k) \nonumber \\ 
X_{k+\frac{2}{n}} &=& X_{k+\frac{1}{n}} + \gamma (H_k + rKz_k - A_r(X_{k+\frac{1}{n}})) \nonumber \\
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \nonumber \\  
X_{k+\frac{n-1}{n}} &=& X_{k+\frac{n-2}{n}} + \gamma (H_k + rKz_k - A_r(X_{k + \frac{n-2}{n}})) \nonumber \\
X_{k+\frac{n}{n}} &=& X_{k+\frac{n-1}{n}} + \gamma (H_k + rKz_k - A_r(X_{k+\frac{n-1}{n}})). 
\end{subeqnarray}
The Algorithm \ref{GD1} satisfies the following identity for $n \rightarrow \infty$, i.e., $Y_*$ such that
\begin{equation} 
A(Y_*) + r Y_* = H_k + rKz_k. 
\end{equation}
In passing to the next section, we shall make a simple remark. In case $H_k = H_*$ and $z_k = z_*$, we see that both schemes lead to $X_{k+1} = X_*$ in a single iteration. Thus, we observe that for all $r \geq 0$, 
\begin{equation}
X_* = G_{n,r}(X_*; H_* +r Kz_*) = A_r^{-1}(H_* + rKz_*).  
\end{equation} 

\subsubsection{General framework of convergence analysis for Algorithm \ref{algADMM3}}

In this section, we shall discuss the basic framework to analyze the convergence of the Algorithm \ref{algADMM3}. We shall use the standard notation that for all $k \geq 0$ to discuss the convergence:  
\begin{eqnarray*}
E_k^X &=& X_* - X_k \\
E_k^Z &=& Kz_* - Kz_k \\ 
E_k^H &=& H_* - H_k. 
\end{eqnarray*}
The following is the main result in this section. 
\begin{theorem}\label{main:theorem0} 
The Algorithm \ref{algADMM3} with the inexact or exact solve, the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega (A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k)) \right \|^2. 
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM3} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& G_{n,r} (Kz_k;H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $G_{n,r}$ is an approximate of $A_r^{-1}$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z = K(K^TK)^{-1}K$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& G_{n,r} (Kz_k; H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T G_{n,r} (Kz_k;H_k + r K z_k)) = P_Z [G_{n,r} (Kz_k; H_k + r K z_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{-1} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{-1}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{-1} (H_* + rK z_*) - G_{n,r}(Kz_k; H_k + rK z_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{-1} (H_* + r K z_*) - G_{n,r}(Kz_k; H_k + r K z_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH}, and using the orthogonality between $E_i^H$ and $E_j^Z$ for all $i,j$, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k))\|^2. 
\end{eqnarray*}
This completes the proof. 

% It is important to observe that 
% \begin{equation}
% X_* = A_r^*(H_* + rKz_*) = D_r^*(H_* + rKz_*). 
% \end{equation}

% Therefore, we see that
% \begin{eqnarray*}
% && \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% && \qquad = \|H_* - H_k - \omega (D_r^* (H_* + rKz_*) - D_r^*(H_k + rKz_*))\|. 
% \end{eqnarray*}
% The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
% \begin{eqnarray*}
% -\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] \right ). 
% \end{eqnarray*}
% Lastly, for $H$, we have 
% \begin{eqnarray*}
% E_{k+1}^H &=& E_k^H + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
% &=& E_k^H - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
% &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
% && \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
% &=& E_k^H - \omega Q_Z (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \\ 
% &=& Q_Z [E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))] 
% \end{eqnarray*}
% First, we shall observe that   
% \begin{eqnarray*}
% \omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left( P_Z [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right )\| \\
% &=& \|P_Z [(H_* - H_k) - \omega \left( [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right ) ] \|
% \end{eqnarray*}
% Next, we observe that there is a close relationship between $E_k^Z$ and $E_k^H$: 
% \begin{eqnarray*}
% \|H_{*} - H_{k+1}\| = \|Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))]\|. 
% \end{eqnarray*}
% Therefore, we have that by the nonexpansiveness, 
% \begin{eqnarray*}
% \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 %\\   
% %&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% %&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
% \end{eqnarray*}
% This completes the proof. 
\end{proof}

\subsection{Convergence analysis of FL Algorithm \ref{algADMM3} with GS} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem10} 
Given 
\begin{equation}
\omega \leq r + \lambda_F, 
\end{equation}
the Algorithm \ref{algADMM3} with GS, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( 1 - \frac{1}{\kappa(A_r)} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ). 
\end{eqnarray*}
\end{theorem}
\begin{proof} 
We observe that  
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2  &\leq& \left \|E_k^H - \omega (A_r^{-1}(H_*+rKz_*) - A_{r}^{-1} (H_k + rKz_k)) \right \|^2 \\
&=&  \left \|E_k^H - \omega (A_{r}^{-1}(H_*) + A_r^{-1}(rKz_*) - A_r^{-1}(H_k) - A_r^{-1}(rKz_k) \right \|^2 \\
&=& \left \|(I - \omega A_{r}^{-1})(E_k^H) - \omega r A_r^{-1} (E_k^Z) \right. \|^2 \\
&\leq& \|(I - \omega A_r^{-1}) E_k^H \|^2 + \|\omega r A_r^{-1} E_k^Z\|^2 - 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle. 
\end{eqnarray*}
%We shall divide both sides by $\omega$, to obtain that 
%\begin{eqnarray*}
%&& \left ((1/\omega)E_{k+1}^H, E_{k+1}^H \right ) + \left ( \omega E_{k+1}^Z, E_{k+1}^Z \right ) \leq ((1/\omega)(I - \omega A_r^{-1}) E_k^H, (I - \omega A_r^{-1}) E_k^H ) \\
%&& \quad + r^2 (\omega A_r^{-1} E_k^Z, A_r^{-1} E_k^Z) - 2r  \langle \sqrt{(1/\omega)} (I - \omega A_r^{-1}) E_k^H, \sqrt{\omega} A_r^{-1} E_k^Z \rangle. 
%\end{eqnarray*}
%We observe that since $\omega = r + \lambda_F$, $\omega A_r^{-1} \leq 1$, 
%\begin{equation}
%\left ( \omega A_r^{-1} A_r E_{k+1}^Z, E_{k+1}^Z \right )
%\end{equation} 
We therefore, note that 
\begin{eqnarray*}
\|(I - \omega A_r^{-1}) E_k^H \|^2 &\leq& \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^H\|^2 \\ 
\|\omega r A_r^{-1} E_k^Z\|^2 &\leq& \left ( \frac{r}{r+\lambda_F} \right )^2 
\|E_k^Z\|_\omega^2 \\ 
- 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle &\leq& \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^H\|^2 + \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^Z\|_\omega^2.  
\end{eqnarray*}
This completes the proof for GS case. 
\end{proof} 


\subsection{Convergence analysis of FL Algorithm \ref{algADMM3} with GD} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem10} 
Given 
\begin{equation}
\omega \leq r + \lambda_F, 
\end{equation}
the Algorithm \ref{algADMM3} with GS, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( 1 - \frac{1}{\kappa(A_r)} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ). 
\end{eqnarray*}
\end{theorem}
\begin{proof} 
We observe that  
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2  &\leq& \left \|E_k^H - \omega (A_r^{-1}(H_*+rKz_*) - G_{n,r}(H_k + rKz_k)) \right \|^2 \\
&=& \left \|E_k^H - \omega (A_{r}^{-1}(H_* + rKz_*) - A_r^{-1}(H_k + rKz_*) + A_r^{-1}(H_k + rKz_* ) - G_{n,r}(H_k +rKz_k)) \right \|^2 \\
&=& \left \|(I - \omega A_{r}^{-1})(E_k^H) - \omega (A_r^{-1}(H_k + rKz_* ) - G_{n,r}(H_k +rKz_k)) \right. \|^2 \\
&=& \| E_1 - E_2 \|^2 \\ 
%&\leq& \|(I - \omega A_r^{-1}) E_k^H \|^2 + \|\omega r A_r^{-1} %E_k^Z\|^2 - 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} %E_k^Z \rangle. 
\end{eqnarray*}
%We shall divide both sides by $\omega$, to obtain that 
%\begin{eqnarray*}
%&& \left ((1/\omega)E_{k+1}^H, E_{k+1}^H \right ) + \left ( \omega E_{k+1}^Z, E_{k+1}^Z \right ) \leq ((1/\omega)(I - \omega A_r^{-1}) E_k^H, (I - \omega A_r^{-1}) E_k^H ) \\
%&& \quad + r^2 (\omega A_r^{-1} E_k^Z, A_r^{-1} E_k^Z) - 2r  \langle \sqrt{(1/\omega)} (I - \omega A_r^{-1}) E_k^H, \sqrt{\omega} A_r^{-1} E_k^Z \rangle. 
%\end{eqnarray*}
%We observe that since $\omega = r + \lambda_F$, $\omega A_r^{-1} \leq 1$, 
%\begin{equation}
%\left ( \omega A_r^{-1} A_r E_{k+1}^Z, E_{k+1}^Z \right )
%\end{equation} 
We therefore, note that 
\begin{eqnarray*}
&& \|(I - \omega A_r^{-1}) E_k^H \|^2 \leq  \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^H\|^2 \\
&& \|\omega (A_r^{-1}(H_k + rKz_* ) - G_{n,r}(H_k +rKz_k)) \|^2 \\
&& \qquad \leq \|\omega (A_r^{-1}(H_k + rKz_*) - A_r^{-1}(H_k + rKz_k) + A_r^{-1}(H_k + rKz_*) - G_{n,r}(H_k +rKz_k)) \|^2 \\
&& \qquad \leq 2 \|\omega (A_r^{-1}(H_k + rKz_* ) - A_r^{-1}(H_k + rKz_k)\|^2 + 2\|\omega(A_r^{-1}(H_k + rKz_k) - G_{n,r}(H_k +rKz_k)) \|^2 \\
&& \qquad \leq 2 \omega^2 \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - Kz_k\|^2 + 2 \omega^2 \delta^{2n} \|Y_* - Kz_k\|^2 \\ 
&& \qquad \leq 2 \omega^2 \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - Kz_k\|^2 + 2 \frac{\omega^2}{(r + \lambda_F)^2} \delta^{2n} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ) \\ 
%- 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle &\leq& \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^H\|^2 + \left (1 - \frac{1}{\kappa(A_r)} \right )^2 \|E_k^Z\|_\omega^2.  
\end{eqnarray*}
This completes the proof for GS case. 
\end{proof} 


%\begin{remark}
%There is an alternative method to obtain the same result. This is to use the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$. Namely, we have that
%\begin{eqnarray*}
%E_{k+1}^H &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
%&& \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
%- \omega E_{k+1}^Z &=& -\omega \left ( P_Z [ %A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK %z_k) ] \right ). 
%\end{eqnarray*}
%Thus, we have that 
%\begin{eqnarray*}
%E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - %\omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} %(H_k + r K z_k). 
%\end{eqnarray*}
%Now, we have that 
%\begin{eqnarray*}
%\left \|E_{k+1}^H - \omega E_{k+1}^Z \right %\|^2 &=& \left \|E_{k+1}^H \right \|^2 + %\omega^2 \left \|E_{k+1}^Z \right \|^2 \\
%&=& \left \|E_k^H - \omega \left [ A_r^{*} %(H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \right ] \right \|^2. 
%\end{eqnarray*}
%The first equality is due to the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$.  
%\end{remark}

We note that these two main theorems produce identity for the convergence estimate. The key is now to obtain the estimate of the right hand side with an appropriate choice of $\omega$ and $\gamma$. 

\subsubsection{Convergence analysis of Algorithm \ref{algADMM3} with GD given in Algorithm \ref{GD1}}
Throughout this section, we shall set 
\begin{equation}
\delta = \frac{\kappa(A_r)-1}{\kappa(A_r)}. 
\end{equation} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem04} 
Given $\gamma = \frac{1}{r + L_F}$ and \begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{r + \lambda_F} + \frac{1}{r+L_F}}, 
\end{equation}
the Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( \delta^n + (1 - \delta^n) \frac{r}{r+\lambda_F} \right )^2  + \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
\end{eqnarray*}
where $\kappa(S_r) = \frac{(r+L_F)}{(r+\lambda_F)}(1 - \delta^n).$ 
\end{theorem}
\begin{proof} 
We first recall that the following identity holds: with $\mathcal{H}_\gamma = I - \gamma A_r$, 
\begin{eqnarray*}
G_{n,r}(X;H_k + rKz_k) &=& (I - \gamma A_r)^n X + (I - (I - \gamma A_r)^n) A_r^{-1} (H_k + rKz_k) \\
&=& \mathcal{H}_\gamma^n X + (I - \mathcal{H}_\gamma^n) A_r^{-1} (H_k + rKz_k). 
\end{eqnarray*} 
With an observation that 
\begin{equation}
A_r^{-1}(H_* + rKz_*) = G_{n,r}(Kz_*;H_* + rKz_*).  
\end{equation}
According to Theorem, we shall need to estimate the following: 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad =  \left \|E_k^H - \omega (G_{n,r}(Kz_*;H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad = \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_*+rKz_*) - G_{n,r}(Kz_k; H_k+rKz_*)) \right. \\
&& \qquad\qquad \left. - \omega ( G_{n,r}(Kz_k;H_k+rKz_*) - G_{n,r}(Kz_k;H_k+rKz_k)) \right \|^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*}
We shall now investigate the following: 
\begin{subeqnarray*}
E_1 &=& E_k^H - \omega (G_{n,r}(Kz_*;H_* + rKz_*) - G_{n,r}(Kz_*;H_k + rKz_*)) \\ 
&=& E_k^H - \omega (I - \mathcal{H}_\gamma^n) A_r^{-1}(H_* - H_k) \\ 
E_2 &=& G_{n,r}(Kz_*;H_k+rKz_*) - G_{n,r} (Kz_k;H_k+rKz_k) \\ 
&=& \mathcal{H}_\gamma^n K(z_* - z_k) + (I - \mathcal{H}_\gamma^n)A_r^{-1}r(Kz_* - Kz_k) \\ 
&=& \left ( \mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r \right) K(z_* - z_k). 
\end{subeqnarray*}
This leads that 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_* + rKz_*) - G_{n,r} (Kz_k; H_k + rKz_k)) \right \|^2 \\
&& \qquad = \|E_1 - \omega E_2\|^2 \\
&& \qquad =  \|E_1\|^2 - 2  \langle E_1, \omega E_2\rangle + \|E_2\|_\omega ^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*} 
A simple choice of $\gamma$ would be $\gamma = \frac{1}{r + L_F}$. Then, we see that, in an increasing order: 
\begin{subeqnarray*}
\sigma(\mathcal{H}_\gamma^n) &=& \{(1 - \gamma (r + L_F)^n, \cdots, (1 - \gamma (r + \lambda_F)^n  \} \\ 
\sigma(I - \mathcal{H}_\gamma^n) &=& \{1 - (1 - \gamma (r + \lambda_F))^n, \cdots, 1 - (1 - \gamma (r + L_F))^n  \} \\
\rho((I - \mathcal{H}_\gamma^n)A_r^{-1}) &=& \max \left \{ \frac{(1 - (1 - \gamma (r + \lambda_F))^n)}{r+\lambda_F}, \frac{1 - (1 - \gamma (r + L_F))^n}{r + L_F}  \right \} \\ 
\rho(\mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r) &=& \max \left \{ (1 - \gamma (r + \lambda_F))^n + (1 - (1 - \gamma (r+\lambda_F))^n))\frac{r}{r+\lambda_F}, \right. \\
&& \qquad \left. (1 - \gamma (r+ L_F))^n + (1 - (1 - \gamma (r+L_F))^n)\frac{r}{r + L_F}  \right \}. 
\end{subeqnarray*}
Thus, we have that 
\begin{subeqnarray*}
\sigma(\mathcal{H}_\gamma^n) &=& \{(1 - \gamma (r+L_F))^n, \cdots, (1 - \gamma (r+\lambda_F))^n  \} \\ 
\sigma(I - \mathcal{H}_\gamma^n) &=& \{1 - (1 - \gamma (r+\lambda_F))^n, \cdots, 1 - (1 - \gamma (r+L_F))^n  \} \\
\rho((I - \mathcal{H}_\gamma^n)A_r^{-1}) &=& \max \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}  \right \} \\ 
\rho(\mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r) &=& \max \left \{ \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F}, \frac{r}{r + L_F}  \right \}. 
\end{subeqnarray*}
The easy bound would be for $E_2$. We note that 
\begin{equation}
\|E_2\| \leq \max \left \{ \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F}, \frac{r}{r + L_F}  \right \} \|Kz_* - Kz_k\|. 
\end{equation}
On the other hand, we have that with $S_r = (I - \mathcal{H}_\gamma^n)A_r^{-1}$, 
\begin{equation*} 
\lambda_{min}(S_r) = \min \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}    \right \} \quad \mbox{ and } \quad \lambda_{max}(S_r) = \max \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}    \right \}.  
\end{equation*}  
Thus, the choice of $\omega$ given as follows:  
\begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{r + \lambda_F} + \frac{1}{r+L_F}}. 
\end{equation} 
With this choice, we obtain the convergence rate given as follows: 
\begin{equation}
\|E_1\| \leq \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right ) \|H_* - H_k\|. 
\end{equation} 
We shall now make it clear by choosing $n$ small and $n$ large. First, for $n \gg 1$, we have that
\begin{equation}
\|E_2\| \leq \left ( \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F} \right ) \|Kz_* - Kz_k\| 
\end{equation}
and 
\begin{equation}
\kappa(S_{r}) = \frac{(r+L_F)}{(r+\lambda_F)} (1 - \delta^n).  
\end{equation} 
On the other hand, if $n = O(1)$, then it is unclear since there are many factors. This completes the proof. 
\end{proof} 

%\begin{corollary}\label{main:corollary} 
%Given $\gamma = \frac{1}{r + L_F}$, and $\omega = 1/\gamma$, we set $n = 1$, then we have 
%the Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
%\begin{eqnarray*}
%\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( \delta^n + (1 - \delta^n) \frac{r}{r+\lambda_F} \right )^2  + \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
%\end{eqnarray*}
%where $\kappa(S_r) = \frac{(r+L_F)}{(r+\lambda_F)}(1 - \delta^n).$ 
%\end{corollary}
%\begin{proof} 
%We have that independent of $E_2$, we have that with $\omega = 1/\gamma$, 
%\begin{eqnarray*}
%E_1 = E_k^H - \omega (I - (I - \gamma A_r)A_r^{-1} (H_* - H_k) = E_k^H - \omega \gamma (H_* - %H_k) = 0 
%\end{eqnarray*}
%This completes the proof. 
%\end{proof} 

\end{document} 
 
\subsection{Scaffold}
Using our notation, we now introduce the ProxSkip algorithm. See algorithm~\ref{alg:ProxSkip}. 

\begin{algorithm}[H]
\caption{ProxSkip}\label{alg:ProxSkip}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, $h_0$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_{t+1} = Z_{t} - \gamma (\nabla F(Z_{t}) -h_t ) $}
    \State{Flip a coin $\theta_t$, $P(\theta_t = 1) = p $}
    \If{$\theta_t = 1$} 
    \State{$Z_{t+1} = {\rm prox}_{\frac{\gamma}{p}\psi } 
    \left ( X_{t+1} - \frac{\gamma}{p} h_t \right )$} 
    \Else
        \State{$Z_{t+1} = X_{t+1}$}
    \EndIf 
    \State{$h_{t+1} = h_t + \frac{p}{\gamma} (Z_{t+1} - X_{t+1})$} 
\EndFor
\end{algorithmic}
\end{algorithm}

A deterministic version of ProxSkip is given in algorithm \ref{alg:ProxSkip deterministic}. 

\begin{algorithm}[H]
\caption{ProxSkip Deterministic (SCAFFOLD)}\label{alg:ProxSkip deterministic}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_t = Z_t$}
    \For{$k = 0, 1,\dots, N-1$}
    \State{$X_{t + \frac{k+1}{N}} = X_{t + \frac{k}{N}} - \gamma ( \nabla F(X_{t + \frac{k}{N}} ) - H_t)$ }
    % \State{$Z_{t + \frac{k+1}{N}} =X_{t + \frac{k+1}{N}} $}
    \EndFor    
    \State{$Z_{t+1} = \text{prox}_{N\gamma \psi}(X_{t+1} - N\gamma H_t)$}
    \State{$H_{t+1} = H_t + \frac{1}{N \gamma }(Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}


\section{Federated Learning for $F$ being a quadratic functional; Linear Case} 

We restrict our discussion of federated learning algorithm for the linear case only. 

\subsection{Problem Description} 

We want to solve the following problem: 
\begin{equation}
\begin{pmatrix}
A & 0 & -I \\ 
0 & 0 & K^T \\
-I & K & 0 
\end{pmatrix} 
\begin{pmatrix}
X_* \\ 
z_*  \\ 
H_* 
\end{pmatrix} 
= \begin{pmatrix}
f_1 \\ 
f_2 \\ 
0 
\end{pmatrix},  
\end{equation} 
where 
\begin{equation}
K = \textbf{1} \otimes I. 
\end{equation}

We introduce some notation:
\begin{equation}
\mathcal{A} = \begin{pmatrix} 
A & 0 \\ 
0 & 0 
\end{pmatrix} 
,\quad \mathcal{B} = \begin{pmatrix} -I & K \end{pmatrix}, 
\quad f = \begin{pmatrix} f_1 \\ f_2 \end{pmatrix}, \quad \mbox{ and } \quad U = \begin{pmatrix} X \\ z \end{pmatrix}. 
\end{equation}
This problem can be written as follows: 
\begin{subeqnarray}
\mathcal{A} U_* + \mathcal{B}^T H_* &=& f, \\ 
\mathcal{B} U_* &=& 0.
\end{subeqnarray}
The Augmented Lagrangian Uzawa is based on the addition of the penalty term: 
\begin{subeqnarray}
(\mathcal{A} + r \mathcal{B}^T \mathcal{B}) U_* + \mathcal{B}^T H_* &=& f \\ 
\mathcal{B} U_* &=& 0.
\end{subeqnarray}
We shall denote $\mathcal{A}_r = \mathcal{A} + r \mathcal{B}^T \mathcal{B}$ and $A_r = A + rI$. 

We note that in a full matrix notation, it reads as follows: 
\begin{equation}
\begin{pmatrix}
A_r & - r K & -I \\
-r K^T& r K^TK & K^T\\
-I& K & 0\\
\end{pmatrix}
\begin{pmatrix}
X_*\\
z_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
f_1 \\
f_2 \\
g
\end{pmatrix},
\end{equation}

\subsection{Augmented Lagrangian Uzawa method and its convergence} 

By removing $U$, we get the following system for $H$: 
\begin{equation}
\mathcal{S}_r H_* = \mathcal{B} \mathcal{A}_r^{-1} \mathcal{B}^T H_* = \mathcal{B} \mathcal{A}_r^{-1} f. 
\end{equation} 

Therefore, the system can be given as follows: 
\begin{subeqnarray}
(\mathcal{A} + r \mathcal{B}^T \mathcal{B}) U_* + \mathcal{B}^T H_* &=& f \\ 
\mathcal{S}_r H_* &=& \mathcal{B} \mathcal{A}_r^{-1} f. 
\end{subeqnarray}
We can apply the Richardson method to update $H$, whose parameter will be denoted by $\omega$. Thus, we arrive at the following Augmented Lagrangian Uzawa method: 
\begin{subeqnarray*} 
(\mathcal{A} + r \mathcal{B}^T \mathcal{B} ) U_{k+1} &=& f - \mathcal{B}^T H_k \\ 
H_{k+1} &=& H_k + \omega ( \mathcal{B} \mathcal{A}_r^{-1} f - \mathcal{S}_r H_k) \\
&=& H_k + \omega \mathcal{B} U_{k+1}.  
\end{subeqnarray*}
This is summarized as an algorithm. 

\begin{algorithm}\label{aglu}
\begin{algorithmic}
\For{$k=0, 1,2,\cdots $}
\State{Update of $U_{k+1}$: 
\begin{equation}\label{Srsolv}
(\mathcal{A} + r \mathcal{B}^T \mathcal{B} ) U_{k+1} = f - \mathcal{B}^T H_k 
\end{equation}}
\State{Update of $H_{k+1}$: 
\begin{equation} 
H_{k+1} = H_k + \omega \mathcal{B} U_{k+1}.  
\end{equation}}
\EndFor
\end{algorithmic}\caption{Augmented Lagrangian Uzawa}\label{algo1}
\end{algorithm}

\begin{theorem}
Let $\sigma(A) \in [\lambda_F, L_F]$. Then, the algorithm \ref{algo1} converges with the convergence rate given as follows:
\begin{enumerate}
\item For $0 < \omega < 2/\rho(\mathcal{S}_r)$, we have the following convergence:
\begin{equation}
\|H_* - H_k\| \leq \rho(I - \omega \mathcal{S}_r)^k \|H_* - H_0\|. 
\end{equation}
Furthermore, we have that 
\begin{equation}
\|X_* - X_k\|_A = |U_* - U_k|_{\mathcal{A}} \leq \sqrt{1/r}\rho(I - \omega \mathcal{S}_r)^k \|H_* - H_0\|. 
\end{equation} 
\item For $\omega = r$, we have the convergence rate given as follows: 
\begin{equation}
\|H_* - H_k\| \leq \left ( \frac{1}{1 + r/L_F}\right )^k \|H_* - H_0\| 
\end{equation} 
and 
\begin{equation}
\|X_* - X_k\|_A = |U_* - U_k|_{\mathcal{A}} \leq \sqrt{1/r} \left ( \frac{1}{1 + r/ L_F}\right )^k \|H_* - H_0\|,  
\end{equation} 
where $\mu_0 = 1/L_F$ is the smallest eigenvalue of $\mathcal{B} \mathcal{A}^{\dag} \mathcal{B}^T$.
\item For $\omega = \frac{2}{ \frac{\lambda_F}{1 + \lambda_F r} + \frac{L_F}{1 + L_F r}}$, we have 
\begin{equation}
\|H_* - H_k\| \leq \left ( \frac{L_F - \lambda_F}{L_F + \lambda_F + 2L_F \lambda_F r} \right )^k \|H_* - H_0\|. 
\end{equation}
Furthermore, we have that 
\begin{equation}
\|X_* - X_k\|_A = |U_* - U_k|_{\mathcal{A}} \leq \sqrt{1/r} \left ( \frac{L_F - \lambda_F}{L_F + \lambda_F + 2L_F \lambda_F r} \right )^k \|H_* - H_0\|. 
\end{equation} 
\end{enumerate} 
\end{theorem} 
\begin{proof}
The convergence of the Augmented Lagrangian Uzawa relies on the spectrum of the Schur complement operator. This leads to the choice of paramters $\omega$. The Schur compliment operator is given as follows: 
\begin{equation}
\mathcal{S}_r = \mathcal{B}( \mathcal{A} + r \mathcal{B}^T \mathcal{B})^{-1} \mathcal{B}^T. 
\end{equation} 
We note that 
\begin{equation}
{\rm Null}(\mathcal{A}) \cap {\rm Null}(\mathcal{B}) = \{ 0\}. 
\end{equation}
Further, we can show that $\mathcal{B} \mathcal{A}^\dag \mathcal{B}^T$ is symmetric positive definite. We note that 
\begin{equation}
\mathcal{A}^\dag = \begin{pmatrix} A^{-1} & 0 \\ 0 & 0 \end{pmatrix} 
\end{equation} 
and thus 
\begin{equation}
\mathcal{B}\mathcal{A}^\dag \mathcal{B}^T = \begin{pmatrix} -I & K \end{pmatrix}  \begin{pmatrix} A^{-1} & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} -I \\ K^T \end{pmatrix} = \begin{pmatrix} -I & K \end{pmatrix} \begin{pmatrix} -(\nabla^2 F)^{-1} \\ 0 \end{pmatrix} = \nabla^2 F^{-1}.
\end{equation} 
This means, that 
\begin{equation}
\frac{1}{L_F} I \leq \mathcal{B} \mathcal{A}^\dag \mathcal{B}^T \leq \frac{1}{\lambda_F} I.   
\end{equation} 
By applying the Sherman-Morrison-Woodbury formula, we have  
\textcolor{red}{\begin{equation}
\mathcal{S}_r^{-1} = rI + (\mathcal{B} \mathcal{A}^\dag \mathcal{B}^T)^{-1} = rI + A = A_r.    
\end{equation}}
Thus, the spectrum of $\mathcal{S}_r$ is given by 
\begin{equation}
\sigma(\mathcal{S}_r) = \left \{\frac{\eta}{1 + r \eta} : \eta \in \sigma(\mathcal{B}\mathcal{A}^\dag \mathcal{B}^T) \right \}.
\end{equation}
Thus, the spectral radius of $\mathcal{S}_r$ has the upper bound i.e., $\rho(\mathcal{S}_r) < 1/r$ for any $r > 0$. Therefore, since the convergence of Richardson method will be guaranteed if $0 < \omega < 2/\rho(\mathcal{S}_r) = 2r$, a simple choice could be $\omega = r$ for the convergence. While it is not the optimal choice, the convergence can be shown as follows: The Augmented Lagrangian Uzawa can be shown to behave as follows: 
\begin{equation}
\|H_* - H_k\| \leq \left ( \frac{1}{1 + r/L_F}\right )^k \|H_* - H_0\| 
\end{equation} 
and 
\begin{equation}
|U_* - U_k|_{\mathcal{A}} \leq \sqrt{1/r} \left ( \frac{1}{1 + r/ L_F}\right )^k \|H_* - H_0\|,  
\end{equation} 
where $\mu_0 = 1/L_F$ is the smallest eigenvalue of $\mathcal{B} \mathcal{A}^{\dag} \mathcal{B}^T$. Now, we shall consider more detailed discussion on optimal choice of $\omega$. We note that 
\begin{equation}
\sigma(\mathcal{S}_r) \in \left [ \frac{1}{\frac{1}{\lambda_F} + r},\frac{1}{\frac{1}{L_F} + r}\right ]
\end{equation} 
Then, the optimum convergence rate is given as follows for $\omega = \frac{2}{\lambda_{min}(\mathcal{S}_r) + \lambda_{max}(\mathcal{S}_r)}$: 
\begin{equation}
\frac{\kappa(\mathcal{S}_r) - 1}{\kappa(\mathcal{S}_r) + 1} = \frac{L - \lambda}{L + \lambda + 2 L \lambda r}.
\end{equation} 
This completes the proof. 
\end{proof} 

To clarify the discussion, we assume that $N_c = 1$ and the original $3 \times 3$ system can be written as 
\begin{equation}
\begin{pmatrix}
\nabla^2 G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
b \\
0
\end{pmatrix}, \qquad B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
\begin{lemma}
The matrix $\nabla^2G(X,z)$ is symmetric positive definite and the spectrum is given by 
\begin{equation}
\sigma(H(G)) \subset \left \{ \min \left \{r + \lambda_F, \frac{r\lambda_F}{r + \lambda_F} \right \}, \max \left \{r + L_F, \frac{rL_F}{r + L_F} \right \} \right \}.  
\end{equation}
\end{lemma}
\begin{proof}
We begin with the spectrally equivalent matrix to $H(G)$, given as follows: 
\begin{equation*}
\begin{pmatrix}
A_r & 0 \\
0   & r K^T K - rK^T A_r^{-1} r K  
\end{pmatrix} = \begin{pmatrix}
A_r & 0 \\
0   & r K^T ( I - r A_r^{-1}) K  
\end{pmatrix} 
\end{equation*}
Due to the $\lambda_F$-strong convexity and $L_F$-smoothness of $F$, it is easy to see 
\begin{subeqnarray*}
&&\sigma(A_r) \subset \left \{ r + \lambda_F, L + \lambda_F \right \} \\
&&\sigma \left (I - r A_r^{-1} \right ) \subset \left \{ 1 - \frac{r}{r+\lambda_F}, 1 - \frac{r}{r + L_F} \right \} = \left \{ \frac{\lambda_F}{r+\lambda_F}, \frac{L_F}{r + L_F} \right \}
\end{subeqnarray*}
Thus, we have that
\begin{subeqnarray*}
\sigma \left (rI - r^2 A_r^{-1} \right ) \subset \left \{ \frac{r\lambda_F}{r+\lambda_F}, \frac{r L_F}{r + L_F} \right \}
\end{subeqnarray*}
and 
\begin{equation}
\sigma(H(G)) \subset \left \{ \min \left \{r + \lambda_F, \frac{r\lambda_F}{r + \lambda_F} \right \}, \max \left \{r + L_F, \frac{rL_F}{r + L_F} \right \} \right \}.  
\end{equation}
This completes the proof. 
%We also note that $K^TK = N_c I$, where $N_c$ is the number of clients and further note that for any $v$, we have 
%\begin{equation}
%\frac{(K^T (I - r A_r^{-1}) K v , v)}{(v,v)} = n \frac{((I - r A_r^{-1}) K v , Kv)}{(K v, Kv)}.   
%\end{equation}
%Thus, we have that 
%\begin{equation}\label{lambdaG}
%\lambda_G = n \left ( 1 - \frac{r}{r + \lambda_F} \right )\leq n \frac{((I - r A_r^{-1}) K v , Kv)}{(K v, Kv)} \leq n \left ( 1 - \frac{r}{r + L_F} \right ) = L_G.       
%\end{equation}
%    This completes the proof.
\end{proof}


\subsection{Convergence of Exact and Inexact Block Gauss-Seidel Method for the $U$ system} 

In this section, we discuss the solution by Gauss-Seidel for the following system: 
\begin{equation}
\mathcal{A}_r \begin{pmatrix} X_* \\ z_* \end{pmatrix} = \begin{pmatrix} 
A + rI & - r K \\ - r K^T & r K^T K   
\end{pmatrix} \begin{pmatrix} X_* \\ z_* \end{pmatrix} = \begin{pmatrix} f_1 \\ f_2 \end{pmatrix}. 
\end{equation} 
We shall consider both exact block Gauss-Seidel method and inexact block Gauss-Seidel method. To handle these at the same time, it would be good to introduce a general framework. We denote $R$ by the modification of $D^{-1}$. Then, the modified block Gauss-Seidel method is given as follows: 
\begin{equation}\label{gsmethod} 
U_{k+1} = U_k + (R^{-1} + L)^{-1} (f - \mathcal{A}_r U_{k}), \quad k = 1,2,\cdots. 
\end{equation}
Note that 
\begin{subeqnarray} 
D &=& \begin{pmatrix} A + r I & 0 \\ 0 & rN I \end{pmatrix} \\
L &=& \begin{pmatrix} 0 & 0 \\ -r K^T & 0 \end{pmatrix}. 
\end{subeqnarray} 
We note that the modified block Gauss-Seidel method converges if 
\begin{equation}
\overline{R} = R^T + R - R^T D R > 0. 
\end{equation} 
Furthermore, the convergence rate can be obtained as follows: 
\begin{equation}
\|I - (R^{-1} + L)^{-1} \mathcal{A}_r\|_{\mathcal{A}_r}^2 = 1 - \frac{1}{1 + c_0(r)}, 
\end{equation} 
where 
\begin{equation}
c_0(r) = \sup_{\|v\|_{\mathcal{A}_r}=1} \langle \overline{R}^{-1} R^T ( D+ U - R^{-1}) v, R^T (D + U - R^{-1}) v \rangle. 
\end{equation}

\begin{remark}
The above framework can handle the case when $n-$step Gradient descent method is used to handle $A_r = A + rI$ block. Note that in such an occasion, the $n-$step Gradient descent requires the initial guess and thus, we can set it as $X_k$. 
The question arises here if we can use the total of $m-$step iteration to define $U_{k+1}$. Namely, we can apply the total of $n-$step inner iteration while we use $m-$step outer iteration. 
\end{remark} 
In the next two sections, we shall consider the case $R = D^{-1}$ and $R$ is an approximate block solve for $A_r = A + rI$. We begin with the first case. 


\subsection{Study of Convergence for the algorithm for the Total System} 
In this section, we discuss the convergence of the iterative method based on inexact Block Gauss-Seidel for $U$ block and Richardson for $H$ block. The Algorithm can be written as given in the Algorithm \ref{algADMM3}.
\begin{algorithm}
\caption{Federated Learning formulation of FL}\label{algADMM3} 
Given $H_0$ such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,K-1$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$),  
    \begin{equation} \label{Xupdate2}
    X_{k+1} = G_{n,r}(X_k;H_k + r Kz_k),
\end{equation} }
    \State{$z_{t+1}$ update: 
        \begin{equation} \label{zupdatexx}
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} \label{Hupdate}
        H_{k+1} = H_k + \omega (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}
We note that the action of the operator $G_{n,r}$ depends on $X_k$. The case when $G_{n,r}$ is the standard $n-$step GD, the algorithm can be written in a very standard way as given in Algorithm \ref{GD1}. On the other hand, the ScaffOld method case is not really the Gradient descent method in the usual sense, which is presented in Algorithm \ref{GD2}. These are discussed next.
\begin{algorithm}
\caption{The case that $G_{n,r}$ is the Standard GD}\label{GD1} 
Given $H_k$ and $z_k$ with $K^TH_k = 0$, update for $X_{k+1}$ is obtained as follows:  
\begin{algorithmic}
\For{$\ell=1,2,\cdots,n$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$ and $b_{H,rKz} = H_k + rKz_k$),  
    \begin{equation} \label{Xupdate4}
    X_{k+\ell/n} = X_{k+(\ell-1)/n} + \gamma (b_{H,rKz} - A_r X_{k+(\ell-1)/n})
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{The case that $G_{n,0}$ is from the Scaffold algorithm}\label{GD2} 
Given $H_k$ and $z_k$ with $K^TH_k = 0$, update for $X_{k+1}$ is obtained as follows:  
\begin{algorithmic}
\For{$\ell=1,2,\cdots,n$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$ and $b_H = H_k$),  
    \begin{equation} \label{Xupdate}
    X_{k+\ell/n} = X_{k+(\ell-1)/n} + \gamma (b_H - A X_{k+(\ell-1)/n}),
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

In this section, we shall discuss the convergence of Algorithm 1. We first begin our discussion for the standard GD case. The standard $n-$step GD method to solve the following system for $X$:
\begin{equation}
A_r X = H_k + rKz_k. 
\end{equation}
will be given as follows: with $X_k = Kz_k$, 
\begin{subeqnarray} 
X_{k+\frac{1}{n}} &=& X_{k} + \gamma (H_k 
+ rKz_k - A_r X_k) \nonumber \\ 
X_{k+\frac{2}{n}} &=& X_{k+\frac{1}{n}} + \gamma (H_k + rKz_k - A_r(X_{k+\frac{1}{n}})) \nonumber \\
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \nonumber \\  
X_{k+\frac{n-1}{n}} &=& X_{k+\frac{n-2}{n}} + \gamma (H_k + rKz_k - A_r(X_{k + \frac{n-2}{n}})) \nonumber \\
X_{k+\frac{n}{n}} &=& X_{k+\frac{n-1}{n}} + \gamma (H_k + rKz_k - A_r(X_{k+\frac{n-1}{n}})). 
\end{subeqnarray}
On the other hand, the Algorithm \ref{alg:SCAFFOLD} presented in \cite{mishchenko2022proxskip} takes $X_k = Kz_k$ when the GD step starts for the variable $X$ and it can be written as follows: with $\gamma > 0$,   
\begin{eqnarray*} 
X_{k+\frac{1}{n}} &=& X_{k} + \gamma (H_k - A(X_k)) \\ % = [(I - \gamma \nabla F)(X_k) + \gamma H_k] \\ 
X_{k+\frac{2}{n}} &=& X_{k+\frac{1}{n}} + \gamma (H_k - A(X_{k+\frac{1}{n}})) \\ % = [(I - \gamma \nabla F)(X_{k+\frac{1}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{n-1}{n}} &=& X_{k+\frac{n-2}{n}} + \gamma (H_k - A(X_{k+\frac{n-2}{n}})) \\ %=  [(I - \gamma A)(X_{k+\frac{N-2}{N}}) + \gamma H_k] \\  \\
X_{k+\frac{n}{n}} &=& X_{k+\frac{n-1}{n}} + \gamma (H_k - A(X_{k+\frac{n-1}{n}})).
% = [(I - \gamma A)(X_{k+\frac{N-1}{N}}) + \gamma H_k] \\ 
\end{eqnarray*}
Namely, the update of $X$ in the Algorithm \ref{alg:SCAFFOLD} is considering the following system: 
\begin{equation}
A X = H_k, 
\end{equation}
which corresponds to the system without the penalty term, i.e., $r=0$. Note that at the limit when $n$ approaches $\infty$, we have that the limit $X_{k+1}$, satisfies the following identity: 
\begin{equation} 
X_{k+1} = X_{k+1} - \gamma (A(X_{k+1}) - H_k) \quad \Leftrightarrow \quad A(X_{k+1}) = H_k.  
\end{equation}
On the other hand, the Algorithm \ref{GD1} satisfies the following identity for $n \rightarrow \infty$, i.e., $Y_*$ such that
\begin{equation} 
A(Y_*) + r Y_* = H_k + rKz_k. 
\end{equation}
Of course, if $r = 0$, then both algorithm produces the same iterates. However, it would be considered unnatural to set $r = 0$ only for the update of $X$ in the Algorithm \ref{algADMM3}

In passing to the next section, we shall make a simple remark. In case $H_k = H_*$ and $z_k = z_*$, we see that both schemes lead to $X_{k+1} = X_*$ in a single iteration. Thus, we observe that for all $r \geq 0$, 
\begin{equation}
X_* = G_{n,r}(X_*; H_* +r Kz_*) = A_r^{-1}(H_* + rKz_*).  
\end{equation} 

\subsubsection{General framework of convergence analysis for Algorithm \ref{algADMM3}}

In this section, we shall discuss the basic framework to analyze the convergence of the Algorithm \ref{algADMM3}. We shall use the standard notation that for all $k \geq 0$ to discuss the convergence:  
\begin{eqnarray*}
E_k^X &=& X_* - X_k \\
E_k^Z &=& Kz_* - Kz_k \\ 
E_k^H &=& H_* - H_k. 
\end{eqnarray*}
The following is the main result in this section. 
\begin{theorem}\label{main:theorem0} 
The Algorithm \ref{algADMM3} with the standard $n-$ step GD, the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega (A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k)) \right \|^2. 
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM3} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& G_{n,r} (Kz_k;H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $G_{n,r}$ is an approximate of $A_r^{-1}$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z = K(K^TK)^{-1}K$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& G_{n,r} (Kz_k; H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T G_{n,r} (Kz_k;H_k + r K z_k)) = P_Z [G_{n,r} (Kz_k; H_k + r K z_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{-1} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{-1}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{-1} (H_* + rK z_*) - G_{n,r}(Kz_k; H_k + rK z_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{-1} (H_* + r K z_*) - G_{n,r}(Kz_k; H_k + r K z_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH}, and using the orthogonality between $E_i^H$ and $E_j^Z$ for all $i,j$, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{-1} (H_* + r K z_*) - G_{n,r} (Kz_k; H_k + r K z_k))\|^2. 
\end{eqnarray*}
This completes the proof. 

% It is important to observe that 
% \begin{equation}
% X_* = A_r^*(H_* + rKz_*) = D_r^*(H_* + rKz_*). 
% \end{equation}

% Therefore, we see that
% \begin{eqnarray*}
% && \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% && \qquad = \|H_* - H_k - \omega (D_r^* (H_* + rKz_*) - D_r^*(H_k + rKz_*))\|. 
% \end{eqnarray*}
% The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
% \begin{eqnarray*}
% -\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] \right ). 
% \end{eqnarray*}
% Lastly, for $H$, we have 
% \begin{eqnarray*}
% E_{k+1}^H &=& E_k^H + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
% &=& E_k^H - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
% &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
% && \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
% &=& E_k^H - \omega Q_Z (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k)) \\ 
% &=& Q_Z [E_k^H - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))] 
% \end{eqnarray*}
% First, we shall observe that   
% \begin{eqnarray*}
% \omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left( P_Z [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right )\| \\
% &=& \|P_Z [(H_* - H_k) - \omega \left( [A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k)] \right ) ] \|
% \end{eqnarray*}
% Next, we observe that there is a close relationship between $E_k^Z$ and $E_k^H$: 
% \begin{eqnarray*}
% \|H_{*} - H_{k+1}\| = \|Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))]\|. 
% \end{eqnarray*}
% Therefore, we have that by the nonexpansiveness, 
% \begin{eqnarray*}
% \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 %\\   
% %&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
% %&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
% \end{eqnarray*}
% This completes the proof. 
\end{proof}

We now present the case when the specific case when the Algorithm \ref{GD2} is used, i.e., $G_{n,r} = G_{n,0}$.

\begin{theorem}\label{main:theorem01} 
The Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD2}, produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega (A^{-1} (H_*) - G_{n,0} (Kz_k; H_k)) \right \|^2. 
\end{eqnarray*}
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM3} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& G_{n,0} (Kz_k;H_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $G_{n,0}$ is an approximate of $A^{-1}$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z = K(K^TK)^{-1}K$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& G_{n,0} (Kz_k; H_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T G_{n,0} (Kz_k;H_k)) = P_Z [G_{n,0} (Kz_k; H_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A^{-1} (H_*) \\
Kz_{*} &=& P_Z [A^{-1}(H_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A^{-1} (H_*) - G_{n,0} (Kz_k; H_k) \\
E_{k+1}^Z &=& P_Z [ A^{-1} (H_*) - G_{n,0}(Kz_k; H_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH2}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A^{-1} (H_*) - G_{n,0}(Kz_k; H_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH2}, and using the orthogonality between $E_i^H$ and $E_j^Z$ for all $i,j$, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A^{-1} (H_*) - G_{n,0} (Kz_k; H_k))\|^2. 
\end{eqnarray*}
This completes the proof. 
\end{proof}

%\begin{remark}
%There is an alternative method to obtain the same result. This is to use the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$. Namely, we have that
%\begin{eqnarray*}
%E_{k+1}^H &=& E_k^H - \omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \\
%&& \qquad \qquad - P_Z [ A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK z_k) ] ] \\
%- \omega E_{k+1}^Z &=& -\omega \left ( P_Z [ %A_r^{*} (H_* + rK z_*) - D_r^{*} (H_k + rK %z_k) ] \right ). 
%\end{eqnarray*}
%Thus, we have that 
%\begin{eqnarray*}
%E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - %\omega [ A_r^{*} (H_* + r K z_*) - D_r^{*} %(H_k + r K z_k). 
%\end{eqnarray*}
%Now, we have that 
%\begin{eqnarray*}
%\left \|E_{k+1}^H - \omega E_{k+1}^Z \right %\|^2 &=& \left \|E_{k+1}^H \right \|^2 + %\omega^2 \left \|E_{k+1}^Z \right \|^2 \\
%&=& \left \|E_k^H - \omega \left [ A_r^{*} %(H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \right ] \right \|^2. 
%\end{eqnarray*}
%The first equality is due to the orthogonality between $E_{k+1}^H$ and $E_{k+1}^Z$.  
%\end{remark}

We note that these two main theorems produce identity for the convergence estimate. The key is now to obtain the estimate of the right hand side with an appropriate choice of $\omega$ and $\gamma$. We first note that the choice of $\gamma$ is solely dependent on the operator. For $G_{n,0}$, it should consider the operator $A$ while it should consider the operator $A_r$ for $G_{n,r}$. It is evident that $\gamma$ should not be dependent on the iteration count. On the other hand, the parameter $\omega$ may depend on $n$. These will be clarified. 

\subsubsection{Convergence analysis of Algorithm \ref{algADMM3} with GD given in Algorithm \ref{GD1}}
Throughout this section, we shall set 
\begin{equation}
\delta = \frac{\kappa(A_r)-1}{\kappa(A_r)}. 
\end{equation} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem04} 
Given $\gamma = \frac{1}{r + L_F}$ and \begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{r + \lambda_F} + \frac{1}{r+L_F}}, 
\end{equation}
the Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( \delta^n + (1 - \delta^n) \frac{r}{r+\lambda_F} \right )^2  + \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
\end{eqnarray*}
where $\kappa(S_r) = \frac{(r+L_F)}{(r+\lambda_F)}(1 - \delta^n).$ 
\end{theorem}
\begin{proof} 
We first recall that the following identity holds: with $\mathcal{H}_\gamma = I - \gamma A_r$, 
\begin{eqnarray*}
G_{n,r}(X;H_k + rKz_k) &=& (I - \gamma A_r)^n X + (I - (I - \gamma A_r)^n) A_r^{-1} (H_k + rKz_k) \\
&=& \mathcal{H}_\gamma^n X + (I - \mathcal{H}_\gamma^n) A_r^{-1} (H_k + rKz_k). 
\end{eqnarray*} 
With an observation that 
\begin{equation}
A_r^{-1}(H_* + rKz_*) = G_{n,r}(Kz_*;H_* + rKz_*).  
\end{equation}
According to Theorem, we shall need to estimate the following: 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad =  \left \|E_k^H - \omega (G_{n,r}(Kz_*;H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad = \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_*+rKz_*) - G_{n,r}(Kz_k; H_k+rKz_*)) \right. \\
&& \qquad\qquad \left. - \omega ( G_{n,r}(Kz_k;H_k+rKz_*) - G_{n,r}(Kz_k;H_k+rKz_k)) \right \|^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*}
We shall now investigate the following: 
\begin{subeqnarray*}
E_1 &=& E_k^H - \omega (G_{n,r}(Kz_*;H_* + rKz_*) - G_{n,r}(Kz_*;H_k + rKz_*)) \\ 
&=& E_k^H - \omega (I - \mathcal{H}_\gamma^n) A_r^{-1}(H_* - H_k) \\ 
E_2 &=& G_{n,r}(Kz_*;H_k+rKz_*) - G_{n,r} (Kz_k;H_k+rKz_k) \\ 
&=& \mathcal{H}_\gamma^n K(z_* - z_k) + (I - \mathcal{H}_\gamma^n)A_r^{-1}r(Kz_* - Kz_k) \\ 
&=& \left ( \mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r \right) K(z_* - z_k). 
\end{subeqnarray*}
This leads that 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,r}(Kz_*; H_* + rKz_*) - G_{n,r} (Kz_k; H_k + rKz_k)) \right \|^2 \\
&& \qquad = \|E_1 - \omega E_2\|^2 \\
&& \qquad =  \|E_1\|^2 - 2  \langle E_1, \omega E_2\rangle + \|E_2\|_\omega ^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*} 
A simple choice of $\gamma$ would be $\gamma = \frac{1}{r + L_F}$. Then, we see that, in an increasing order: 
\begin{subeqnarray*}
\sigma(\mathcal{H}_\gamma^n) &=& \{(1 - \gamma (r + L_F)^n, \cdots, (1 - \gamma (r + \lambda_F)^n  \} \\ 
\sigma(I - \mathcal{H}_\gamma^n) &=& \{1 - (1 - \gamma (r + \lambda_F))^n, \cdots, 1 - (1 - \gamma (r + L_F))^n  \} \\
\rho((I - \mathcal{H}_\gamma^n)A_r^{-1}) &=& \max \left \{ \frac{(1 - (1 - \gamma (r + \lambda_F))^n)}{r+\lambda_F}, \frac{1 - (1 - \gamma (r + L_F))^n}{r + L_F}  \right \} \\ 
\rho(\mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r) &=& \max \left \{ (1 - \gamma (r + \lambda_F))^n + (1 - (1 - \gamma (r+\lambda_F))^n))\frac{r}{r+\lambda_F}, \right. \\
&& \qquad \left. (1 - \gamma (r+ L_F))^n + (1 - (1 - \gamma (r+L_F))^n)\frac{r}{r + L_F}  \right \}. 
\end{subeqnarray*}
Thus, we have that 
\begin{subeqnarray*}
\sigma(\mathcal{H}_\gamma^n) &=& \{(1 - \gamma (r+L_F))^n, \cdots, (1 - \gamma (r+\lambda_F))^n  \} \\ 
\sigma(I - \mathcal{H}_\gamma^n) &=& \{1 - (1 - \gamma (r+\lambda_F))^n, \cdots, 1 - (1 - \gamma (r+L_F))^n  \} \\
\rho((I - \mathcal{H}_\gamma^n)A_r^{-1}) &=& \max \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}  \right \} \\ 
\rho(\mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r) &=& \max \left \{ \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F}, \frac{r}{r + L_F}  \right \}. 
\end{subeqnarray*}
The easy bound would be for $E_2$. We note that 
\begin{equation}
\|E_2\| \leq \max \left \{ \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F}, \frac{r}{r + L_F}  \right \} \|Kz_* - Kz_k\|. 
\end{equation}
On the other hand, we have that with $S_r = (I - \mathcal{H}_\gamma^n)A_r^{-1}$, 
\begin{equation*} 
\lambda_{min}(S_r) = \min \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}    \right \} \quad \mbox{ and } \quad \lambda_{max}(S_r) = \max \left \{ \frac{(1 - \delta^n)}{r+\lambda_F}, \frac{1}{r + L_F}    \right \}.  
\end{equation*}  
Thus, the choice of $\omega$ given as follows:  
\begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{r + \lambda_F} + \frac{1}{r+L_F}}. 
\end{equation} 
With this choice, we obtain the convergence rate given as follows: 
\begin{equation}
\|E_1\| \leq \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right ) \|H_* - H_k\|. 
\end{equation} 
We shall now make it clear by choosing $n$ small and $n$ large. First, for $n \gg 1$, we have that
\begin{equation}
\|E_2\| \leq \left ( \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F} \right ) \|Kz_* - Kz_k\| 
\end{equation}
and 
\begin{equation}
\kappa(S_{r}) = \frac{(r+L_F)}{(r+\lambda_F)} (1 - \delta^n).  
\end{equation} 
On the other hand, if $n = O(1)$, then it is unclear since there are many factors. This completes the proof. 
\end{proof} 

\subsubsection{Different Type : Convergence analysis of Algorithm \ref{algADMM3} with GS} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem10} 
Given 
\begin{equation}
\omega \leq r + \lambda_F, 
\end{equation}
the Algorithm \ref{algADMM3} with GS, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \left ( \delta^n + (1 - \delta^n) \frac{r}{r+\lambda_F} \right )^2  + \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
\end{eqnarray*}
where $\kappa(S_r) = \frac{(r+L_F)}{(r+\lambda_F)}(1 - \delta^n).$ 
\end{theorem}
\begin{proof} 
We observe that  
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2  &\leq& \left \|E_k^H - \omega (A_r^{-1}(H_*+rKz_*) - A_{r}^{-1} (H_k + rKz_k)) \right \|^2 \\
&& \qquad =  \left \|E_k^H - \omega (A_{r}^{-1}(H_*) + A_r^{-1}(rKz_*) - A_r^{-1}(H_k) - A_r^{-1}(rKz_k) \right \|^2 \\
&& \qquad = \left \|(I - \omega A_{r}^{-1})(E_k^H) - \omega r A_r^{-1} (E_k^Z) \right. \|^2 \\
&& \quad  \leq \|(I - \omega A_r^{-1}) E_k^H \|^2 + \|\omega r A_r^{-1} E_k^Z\|^2 - 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle. 
\end{eqnarray*}
We shall divide both sides by $\omega$, to obtain that 
\begin{eqnarray*}
&& \left ((1/\omega)E_{k+1}^H, E_{k+1}^H \right ) + \left ( \omega E_{k+1}^Z, E_{k+1}^Z \right ) \leq ((1/\omega)(I - \omega A_r^{-1}) E_k^H, (I - \omega A_r^{-1}) E_k^H ) \\
&& \quad + r^2 (\omega A_r^{-1} E_k^Z, A_r^{-1} E_k^Z) - 2r  \langle \sqrt{(1/\omega)} (I - \omega A_r^{-1}) E_k^H, \sqrt{\omega} A_r^{-1} E_k^Z \rangle. 
\end{eqnarray*}
We observe that since $\omega = r + \lambda_F$, $\omega A_r^{-1} \leq 1$, 
\begin{equation}
\left ( \omega A_r^{-1} A_r E_{k+1}^Z, E_{k+1}^Z \right )
\end{equation} 

Therefore, we obtain the following: 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (A_r^{-1}(H_*+rKz_*) - A_{r}^{-1} (H_k+rKz_k)) \right \|^2 \\
&& \qquad =  \left \|E_k^H - \omega (A_{r}^{-1}(H_*) + A_r^{-1}(rKz_*) - A_r^{-1}(H_k) - A_r^{-1}(rKz_k) \right \|^2 \\
&& \qquad = \left \|(I - \omega A_{r}^{-1})(E_k^H) - \omega r A_r^{-1} (E_k^Z) \right. \|^2 \\
&& \quad  \leq \|(I - \omega A_r^{-1}) E_k^H \|^2 + \|\omega r A_r^{-1} E_k^Z\|^2 - 2 \langle(I - \omega A_r^{-1}) E_k^H, \omega r A_r^{-1} E_k^Z \rangle. 
%\left. - \omega (A_{r}^{-1}(H_k+rKz_*) - G_{n,r}(Kz_k;H_k+rKz_k)) \right \|^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*}


\begin{eqnarray*}
\|E_{k+1}^X\|^2 &=& \|A_{r}^{-1}(H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k))\|^2 \\
 &\leq&  \left \|E_k^H - \omega (G_{n,r}(H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad = \left \|E_k^H - \omega (G_{n,r}(H_*+rKz_*) - G_{n,r}( H_k+rKz_*)) \right. \\
&& \qquad\qquad \left. - \omega (G_{n,r}(H_k+rKz_*) - G_{n,r}(Kz_k;H_k+rKz_k)) \right \|^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*}

According to Theorem, we shall need to estimate the following: 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (A_{r}^{-1}(H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad =  \left \|E_k^H - \omega (G_{n,r}(H_*+rKz_*) - G_{n,r} (Kz_k; H_k+rKz_k)) \right \|^2 \\
&& \qquad = \left \|E_k^H - \omega (G_{n,r}(H_*+rKz_*) - G_{n,r}( H_k+rKz_*)) \right. \\
&& \qquad\qquad \left. - \omega (G_{n,r}(H_k+rKz_*) - G_{n,r}(Kz_k;H_k+rKz_k)) \right \|^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*}
We shall now investigate the following: 
\begin{subeqnarray*}
E_1 &=& E_k^H - \omega (A_{r}^{-1}(H_* + rKz_*) - A_{r}^{-1}(H_k + rKz_*)) \\ 
&=& E_k^H - \omega A_r^{-1}(H_* - H_k) \\ 
E_2 &=& A_{r}^{-1}(H_k+rKz_*) - G_{n,r} (Kz_k;H_k+rKz_k). 
%&=& \mathcal{H}_\gamma^n K(z_* - z_k) + (I - \mathcal{H}_\gamma^n)A_r^{-1}r(Kz_* - Kz_k) \\ 
%&=& \left ( \mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r \right) K(z_* - z_k). 
\end{subeqnarray*}
This leads that 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (A_{r}^{-1}(H_* + rKz_*) - G_{n,r}(Kz_k; H_k + rKz_k)) \right \|^2 \\
&& \qquad = \|E_1 - \omega E_2\|^2 \\
&& \qquad =  \|E_1\|^2 - 2  \langle E_1, \omega E_2\rangle + \|E_2\|_\omega ^2. 
\end{eqnarray*} 
We then have the following: for $\omega \leq r + \lambda_F$, 
\begin{subeqnarray*}
\|E_1\|_{1/\omega} &=& \|E_k^H - \omega (A_{r}^{-1}(H_* + rKz_*) - A_{r}^{-1}(H_k + rKz_*))\|_{1/\omega} \\ 
&=& \|(I - \omega A_r^{-1}) E_k^H\|_{1/\omega} \leq \rho(I - \omega A_r^{-1}) \|E_k^H\|_{1/\omega} = \left ( 1 - \frac{1}{\kappa(A_r)} \right ) \|E_k^H\|_{1/\omega}. 
\end{subeqnarray*}
where $\rho(I - \omega A_r^{-1})$ is given as follows:
\begin{equation}
\sigma (I - \omega A_r^{-1}) = \left \{1 - \frac{r + \lambda_F}{r + \lambda} : \lambda \in \sigma (A_r^{-1}) \right \}. 
\end{equation} 
On the other hand, for $E_2$, we have that 
\begin{subeqnarray*}
\|E_2\|_{\omega} &=& \|\omega (A_r^{-1}(H_k + rKz_*) - G_{n,r} (Kz_k;H_k+rKz_k))\| \\
&\leq& \|(A_r^{-1}(H_k + rKz_*) - G_{n,r}^{-1}(H_k + rKz_k))A_r\| \\
&=& \|H_k + rKz_* - G_{n,r}^{-1}(H_k + rKz_k) A_r \| \\ 
&=& \frac{r}{\sqrt{r + \lambda_F}} \|E_k^Z\|_{A_r} + 
%&\leq& r \|E_k^Z\| + \frac{r}{r+\lambda_F} r \delta^n \|E_k^Z\|_{A_r}^2.   
\end{subeqnarray*}
With this choice, we obtain the convergence rate given as follows: 
\begin{equation}
\|E_1\| \leq \left ( \frac{\kappa(S_r) - 1}{\kappa(S_r) + 1} \right ) \|H_* - H_k\|. 
\end{equation} 
We shall now make it clear by choosing $n$ small and $n$ large. First, for $n \gg 1$, we have that
\begin{equation}
\|E_2\| \leq \left ( \delta^n + (1 - \delta^n)\frac{r}{r+\lambda_F} \right ) \|Kz_* - Kz_k\| 
\end{equation}
and 
\begin{equation}
\kappa(S_{r}) = \frac{(r+L_F)}{(r+\lambda_F)} (1 - \delta^n).  
\end{equation} 
On the other hand, if $n = O(1)$, then it is unclear since there are many factors. This completes the proof. 
\end{proof} 



\subsubsection{Convergence analysis of Algorithm \ref{algADMM3} with GD given in Algorithm \ref{GD2}}
Throughout this section, we shall set 
\begin{equation}
\delta = \frac{\kappa(A)-1}{\kappa(A)}. 
\end{equation} 
In this section, we shall establish that the following holds: 
\begin{theorem}\label{main:theorem04} 
Given $\gamma = \frac{1}{L_F}$ and \begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{\lambda_F} + \frac{1}{L_F}}, 
\end{equation}
the Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD1}, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: for $n$ sufficiently large, we have 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \delta^{2n}   + \left ( \frac{\kappa(S) - 1}{\kappa(S) + 1} \right )^2 \right ) \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ), 
\end{eqnarray*}
where $\kappa(S) = \frac{L_F}{\lambda_F}(1 - \delta^n).$ 
\end{theorem}
\begin{proof} 
We first recall that the following identity holds: with $\mathcal{H}_\gamma = I - \gamma A$, 
\begin{eqnarray*}
G_{n,0}(Kz_k;H_k) &=& (I - \gamma A)^n Kz_k + (I - (I - \gamma A)^n) A^{-1}(H_k) \\
&=& \mathcal{H}_\gamma^n Kz_k + (I - \mathcal{H}_\gamma^n) A^{-1}(H_k). 
\end{eqnarray*} 
With an observation that 
\begin{equation}
A^{-1}(H_*) = G_{n,0}(Kz_*;H_*).  
\end{equation}
According to Theorem, we shall need to estimate the following: 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,0}(Kz_*; H_*) - G_{n,0} (Kz_k;H_k)) \right \|^2 \\
&& \qquad =  \left \|E_k^H - \omega (G_{n,0}(Kz_*;H_*) - G_{n,0}(Kz_k;H_k)) \right \|^2 \\
&& \qquad = \left \|E_k^H - \omega (G_{n,0}(Kz_*;H_*) - G_{n,0}(Kz_*;H_k)) - \omega ( G_{n,0}(Kz_*;H_k) - G_{n,0}(Kz_k;H_k)) \right \|^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*}
We shall now investigate the following: 
\begin{subeqnarray*}
E_1 &=& E_k^H - \omega (G_{n,0}(Kz_*;H_*) - G_{n,0}(Kz_*;H_k)) \\ 
&=& E_k^H - \omega (I - \mathcal{H}_\gamma^n) A^{-1}(H_* - H_k) \\ 
E_2 &=& G_{n,0}(Kz_*;H_k) - G_{n,0} (Kz_k;H_k) \\ 
&=& \mathcal{H}_\gamma^n K(z_* - z_k) %+ (I - \mathcal{H}_\gamma^n)A_r^{-1}r(Kz_* - Kz_k) \\ 
%&=& \left ( \mathcal{H}_\gamma^n + (I - \mathcal{H}_\gamma^n)A_r^{-1}r \right) K(z_* - z_k). 
\end{subeqnarray*}
This leads that 
\begin{eqnarray*}
&& \left \|E_k^H - \omega (G_{n,0}(Kz_*; H_*) - G_{n,0} (Kz_k; H_k)) \right \|^2 \\
&& \qquad = \|E_1 - \omega E_2\|^2 \\
&& \qquad =  \|E_1\|^2 - 2  \langle E_1, \omega E_2\rangle + \|E_2\|_\omega ^2. 
%&& \qquad = \left \| E_k^H - \omega (A^{-1} H_* - \mathcal{H}_\gamma^n Kz_* - (I - \mathcal{H}_\gamma^n )A^{-1} H_k +  Kz_k \right \| \\ 
%&& \qquad %- (I - (I - \gamma A)^n A^{-1} H_k) \right \|^2 \\
\end{eqnarray*} 
A simple choice of $\gamma$ would be $\gamma = \frac{1}{L_F}$. Then, we see that, in an increasing order: 
\begin{subeqnarray*}
\sigma(\mathcal{H}_\gamma^n) &=& \{(1 - \gamma L_F)^n, \cdots, (1 - \gamma \lambda_F)^n  \} \\ 
\sigma(I - \mathcal{H}_\gamma^n) &=& \{1 - (1 - \gamma \lambda_F)^n, \cdots, 1 - (1 - \gamma L_F)^n  \} \\
\rho((I - \mathcal{H}_\gamma^n)A^{-1}) &=& \max \left \{ \frac{(1 - (1 - \gamma \lambda_F)^n)}{\lambda_F}, \frac{1 - (1 - \gamma L_F)^n}{L_F}  \right \} \\ 
\rho(\mathcal{H}_\gamma^n) &=& \delta^n. 
\end{subeqnarray*}
The easy bound would be for $E_2$. We note that 
\begin{equation}
\|E_2\| \leq \delta^n \|Kz_* - Kz_k\|. 
\end{equation}
On the other hand, we have that with $S = (I - \mathcal{H}_\gamma^n)A^{-1}$, 
\begin{equation*} 
\lambda_{min}(S) = \min \left \{ \frac{(1 - \delta^n)}{\lambda_F}, \frac{1}{L_F}    \right \} \quad \mbox{ and } \quad \lambda_{max}(S) = \max \left \{ \frac{(1 - \delta^n)}{\lambda_F}, \frac{1}{L_F}    \right \}.  
\end{equation*} 
Thus, the choice of $\omega$ given as follows:  
\begin{equation}
\omega = \frac{2}{\frac{1 - \delta^n}{\lambda_F} + \frac{1}{L_F}}. 
\end{equation} 
With this choice, we obtain the convergence rate given as follows: 
\begin{equation}
\|E_1\| \leq \left ( \frac{\kappa(S) - 1}{\kappa(S) + 1} \right ) \|H_* - H_k\|. 
\end{equation} 
We shall now make it clear by choosing $n$ small and $n$ large. First, for $n \gg 1$, we have that
\begin{equation}
\|E_2\| \leq \delta^n \|Kz_* - Kz_k\| 
\end{equation}
and 
\begin{equation}
\kappa(S) = \frac{L_F}{\lambda_F} (1 - \delta^n).  
\end{equation} 
On the other hand, if $n = O(1)$, then it is unclear since there are many factors. This completes the proof. 
\end{proof} 
\begin{remark}
We remark that the best choice for $\gamma$ would be $\gamma = \frac{2}{\lambda_F + L_F}$, then we have 
\begin{equation}
\|E_2\| \leq \left ( \frac{\kappa(A) - 1}{\kappa(A) + 1} \right )^n \|Kz_* - Kz_k\|.
\end{equation}
On the other hand, such a choice makes $\mathcal{H}_\gamma$ indefinite. Thus, the analysis of $H$ error becomes nontrivial. 
\end{remark} 

\begin{remark} 
We shall now discuss the choice of $\omega$ in Algorithm \ref{alg:SCAFFOLD}. The choice of $\omega$ was given as follows: 
\begin{equation} 
\omega = \frac{1}{n\gamma },
\end{equation}
where $n$ is the GD steps with $\gamma = 1/L_F$. Especially, for the linear problem, the convergence is guaranteed for $\omega < 2L_F$ assuming that $n$ is sufficiently large since if not, it may happen that 
\begin{equation}
\rho(S) = \frac{1 - \delta^n}{\lambda_F}. 
\end{equation}
Thus, assuming that $n$ is sufficiently large and 
\begin{equation}
\omega = \frac{L_F}{n},
\end{equation}
we see that the following holds: 
\begin{equation}
\rho(I - \omega S) = 1 - \frac{\omega}{L_F} = 1 - \frac{1}{n} = \frac{n - 1}{n},  
\end{equation} 
which is because 
\begin{equation}
1 - \frac{\omega}{L_F} > 1 - \frac{\omega(1 - \delta^n)}{\lambda_F}.  
\end{equation}
This means, the convergence is much deteriorate when $n$ gets larger. Furthermore, if we choose $n = \sqrt{\kappa(A)}$, then we get an optimal convergence rate just like the conjugate gradient method. This argument agrees with what is observed in \cite{mishchenko2022proxskip} for the stochastic case. On the other hand, for the case when $r \neq 0$, we have to analyze the spectral radius of $S_r = (I - (I - \gamma A_r)^n)A_r^{-1}$. With $\gamma = \frac{1}{r + L_F}$, we have that
\begin{equation}
\lambda_{min}(S_r) = (1 - \delta^n) \frac{1}{r + \lambda_F} \quad \mbox{ and } \quad \lambda_{max}(S_r) = \frac{1}{r+L_F}. 
\end{equation} 
Therefore, in a similar manner, if we choose $\omega = \frac{r + L_F}{n}$, then we have that 
\begin{equation}
\rho(I - \omega S_r) = \frac{n-1}{n}. 
\end{equation}
Therefore, we can choose $n = \sqrt{\kappa(A_r)}$. This can give faster convergence. Therefore, we can apply larger $r$, which will reduce the number of steps for GD, still leading to the faster convergence. On the other hand, we notice that there is an additional factor, dependent on $\frac{r}{r+\lambda_F}$, which deteriorates as $r$ increases. 
\end{remark}


\begin{corollary}
Let $n = 1$. Given $\gamma = \frac{1}{L_F}$ and $\omega = L_F$, 
the Algorithm \ref{algADMM3} with GD given in the Algorithm \ref{GD2}, produces iterate $(X_k, z_k, H_k)$, for which the following convergence rate is valid: 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq  \left ( \frac{\kappa(A) - 1}{\kappa(A)} \right )^{2} \left (\left \|E_{k}^H \right \|^2 + \left \|E_{k}^Z \right \|_\omega^2 \right ). 
\end{eqnarray*}
\end{corollary}
\begin{proof} 
We first recall that the following identity holds: 
\begin{equation}
G_{1,0}(Kz_k;H_k) = (I - \gamma A) Kz_k + \gamma H_k = \mathcal{H}_\gamma Kz_k + 
\gamma H_k. 
\end{equation} 
We observe that with $\omega = 1/\gamma$, we have 
\begin{subeqnarray*}
E_1 = E_k^H - \omega (G_{1,0}(Kz_*;H_*) - G_{1,0}(Kz_*;H_k))
= E_k^H - \omega \gamma (H_* - H_k) = 0.  
\end{subeqnarray*}
Thus, we only need to estimate $E_2$ given as follows: 
\begin{subeqnarray*}
E_2 &=& G_{1,0}(Kz_*;H_k) - G_{1,0} (Kz_k;H_k) \\ 
&=& \mathcal{H}_\gamma K(z_* - z_k). 
\end{subeqnarray*}
This leads that 
\begin{eqnarray*}
\left \|E_k^H - \omega (G_{1,0}(Kz_*; H_*) - G_{1,0} (Kz_k; H_k)) \right \|^2 = \|E_2\|_\omega^2.
\end{eqnarray*} 
Note that the choice of $\gamma = 1/L_F$ gives 
\begin{equation}
0 \leq \mathcal{H}_\gamma = \left ( I - \gamma A \right ) \leq \left ( 1 - \frac{\lambda_F}{L_F} \right ) = \left ( 1 - \frac{1}{\kappa(A)} \right ) = \left ( \frac{\kappa(A) - 1}{\kappa(A)} \right ). 
\end{equation} 
Thus, we have that
\begin{equation}
\|E_2\| \leq \left ( \frac{\kappa(A) - 1}{\kappa(A)} \right ) \|Kz_* - Kz_k\|. 
\end{equation}
This completes the proof. 
\end{proof}  

\section{Alternative Proof} 
We now present the convergence analysis based on $A-$ norm of $E_k^X$ and the convexity of $A$ due to \cite{shi2014linear}. The federated learning algorithm is given as in Algorithm \ref{algADMM4}.  
\begin{algorithm}
\caption{Federated Learning formulation of FL}\label{algADMM4} 
Given $H_0$ such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,K-1$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$),  
    \begin{equation} \label{Xupdatezz}
    A X_{k+1} + rX_{k+1} = H_k + r Kz_k,
\end{equation} }
    \State{$z_{t+1}$ update: 
        \begin{equation} \label{zupdate1}
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} \label{Hupdate}
        H_{k+1} = H_k + \omega (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}
We shall adapt the proof originated from \cite{shi2014linear} to establish the result of linear convergence.  
\begin{lemma} 
The Algorithm \ref{algADMM4} produces iterates $(X_k,z_k,H_k)$ such that the following identity holds: for all $k=0,1,2\cdots$, 
\begin{eqnarray*}
A(X_{k+1}) - H_{k+1} + (r - \omega) X_{k+1} + (\omega Kz_{k+1} - r Kz_k) &=& 0, \\
H_{k+1} - H_{k} + \omega \left( I - P_Z \right) X_{k+1} &=& 0,\\
K z_{k+1} - P_Z X_{k+1} &=& 0.
\end{eqnarray*}
\end{lemma} 
\begin{proof} 
First of all, we note that $K^TH = 0$ with $H = H_k$ or $H = H_*$ and also 
\begin{equation}
Kz = P_Z X. 
\end{equation}
This is then easy to see that 
\begin{subeqnarray*}\label{equalform1}
A(X_{k+1}) - H_{k+1} + (r - \omega) X_{k+1} + (\omega Kz_{k+1} - r Kz_k) &=& 0, \\
K^T H_{k+1} &=& 0, \\
H_{k+1} - H_{k}  - \omega (K z_{k+1} - X_{k+1}) &=& 0. 
\end{subeqnarray*}
Multiplying the third equation, by $K^T$, we obtain the desired equation. This completes the proof. 
\end{proof}

We shall obtain a simple but important lemma: 
\begin{lemma} 
With $r = \omega$, the following identity holds: 
\begin{eqnarray}\label{haha} 
A(X_{k+1}) - A(X_*) &=& r (Kz_{k} - Kz_{k+1}) + H_{k+1} - H_*  \label{difference1} \\
H_{k+1} - H_k &=& - \omega Q_Z (X_{k+1} - X_*)  \label{difference2} \\
Kz_{k+1} - Kz_* &=& P_Z (X_{k+1} - X_*). \label{difference3}    
\end{eqnarray}
\end{lemma} 
\begin{proof} 
We recall the optimality condition, which can be given as follows:
\begin{subeqnarray*}
A(X_*) - H_* &=& 0 \\ 
K^T H_* &=& 0 \\
K z_* - X_* &=& 0 
\end{subeqnarray*}
However, using the property of $Q_Z$ and $P_Z$, the optimality condition implies that it holds true 
\begin{subeqnarray}\label{KKT}
0 &=& P_Z (K z_* - X_{*}) = K z_* - P_Z X_* \\ 
0 &=& Q_Z (Kz_* - X_*) = - Q_Z X_*. 
\end{subeqnarray}
Subtracting the optimality conditions \eqref{KKT} from \eqref{haha},  completes the proof. 
\end{proof}
The main theorem considers the convergence of a vector $U$ that combines the primal variable $Kz$ and the dual variable $H$,
\begin{equation}
U = \begin{pmatrix}
Kz \\
H
\end{pmatrix} \quad \mbox{ and } \quad C = \begin{pmatrix}
r I & 0 \\
0 & \frac{1}{r} I 
\end{pmatrix}
\end{equation}
We also define a $C-$norm on $U = (Kz,H)^T$ by 
\begin{eqnarray*}
\|U\|_C^2 &=& r \|Kz\|^2 + \frac{1}{r}\|H\|^2 \\ 
&=& r (Kz, Kz) + \frac{1}{r} (H, H) = (rK^TK z, z) + \frac{1}{r}(H,H).  
\end{eqnarray*}
We shall need the following identity: 
\begin{lemma}
We have the following identity:
\begin{eqnarray*}
2 \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \|Y_{k} - Y_*\|^2 - \|Y_{k+1} - Y_* \|^2 - \|Y_{k+1} - Y_k\|^2. 
\end{eqnarray*} 
\end{lemma} 
\begin{proof} 

\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\ 
&=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\
&=&  \langle Y_{k} - Y_*, Y_{k+1} - Y_* \rangle - \| Y_{k+1} - Y_* \|^2
\end{eqnarray*} 
On the other hand, we have 
\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_k + Y_k - Y_* \rangle \\ 
&=& - \|Y_{k+1} - Y_{k}\|^2 + \langle Y_{k} - Y_{k+1}, Y_k - Y_* \rangle . 
\end{eqnarray*} 
By adding these two identities, we obtain the result. This completes the proof. 
\end{proof}
\begin{theorem}
Assume that $K^TH_0 = 0$. Then the ADMM iterations produces $U_k = [Kz_k; H_k]$ that is linearly convergent to the optimal solution  $U_* = [Kz_*; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|U_{k+1} - U_* \|^2_C \leq \frac{1}{1+\delta} \|U_{k} - U_*\|^2_C,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda_F} \|U_{k} - U_* \|^2_C
\end{equation}
\end{theorem}
\begin{proof}
We begin with using the $\lambda_F-$strongly convexity condition for $F$ as follows: 
\begin{eqnarray*}
\lambda_F \| X_{k+1} - X_* \|^2 &\leq& \langle X_{k+1} - X_*, A(X_{k+1}) - A(X_*)\rangle \\
&\leq& \langle X_{k+1} - X_*, r K(z_k - z_{k+1}) \rangle + \langle X_{k+1} - X_*, H_{k+1} - H_* \rangle \\
&=& r \langle X_{k+1} - X_*, P_Z (K(z_k - z_{k+1}))  \rangle + \langle X_{k+1} - X_*, Q_Z(H_{k+1} - H_*) \rangle \\
&=& r \langle  P_Z (X_{k+1} - X_*) , Kz_k - Kz_{k+1}   \rangle + \langle Q_Z (X_{k+1} - X_*), H_{k+1} - H_* \rangle  \\
&=& r \langle Kz_k - Kz_{k+1}, Kz_{k+1} - Kz_*  \rangle + \frac{1}{r} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle \\
&=& (U_{k} - U_{k+1})^T C (U_{k+1} - U_*),  
\end{eqnarray*}
where 
\begin{equation}
U_k = \begin{pmatrix} 
      Kz_k \\
      H_k 
      \end{pmatrix}, \quad U_{k+1} = \begin{pmatrix} 
      Kz_{k+1} \\
      H_{k+1}  
      \end{pmatrix}
,\quad U_{*} = \begin{pmatrix} 
      Kz_{*} \\
      H_{*}  
      \end{pmatrix}
\end{equation}
and the matrix 
\begin{equation}
    C = \begin{pmatrix}
r I & 0 \\
0 &  \frac{1}{r} I
\end{pmatrix}
\end{equation}
This implies
\begin{equation}
\lambda_F  \| X_{k+1} - X_* \|^2 \leq  \frac{1}{2} \|U_k - U_* \|^2_C - \frac{1}{2}  \|U_{k+1} - U_*\|^2_C -  \frac{1}{2}  \|U_k - U_{k+1}\|^2_C. 
\end{equation}
Now, by rearranging terms, we have 
\begin{equation}\label{ineq from strong convexity}
2\lambda_F \| X_{k+1} - X_* \|^2 + \|U_k - U_{k+1}\|^2_C + \|U_{k+1} - U_*\|^2_C \leq  \|U_k - U_* \|^2_C.
\end{equation}
This immediately, leads to 
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda_F} \|U_{k} - U_*\|^2_C.
\end{equation}
Having \eqref{ineq from strong convexity}, it suffices to show for some $\delta > 0$, we have 
\begin{equation}\label{claim}
\delta \|U_{k+1} - U_* \|^2_C \leq 2 \lambda_F \|X_{k+1} - X_* \|^2 + \|U_k - U_{k+1}\|^2_C,
\end{equation}
or equivalently,
\begin{eqnarray*}\label{claimequivalence}
\delta \left( r  \|Kz_{k+1}- Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*} \|^2 \right) &\leq&  2 \lambda_F \|X_{k+1} - X_* \|^2 \\
&+& r \|Kz_{k}- Kz_{k+1} \|^2 + \frac{1}{r} \|H_k - H_{k+1} \|^2, 
\end{eqnarray*}
which will imply the desired inequality: 
\begin{equation}
\|U_{k+1} - U_*\|^2_C \leq \frac{1}{1 + \delta} \|U_{k} - U_*\|^2_C.
\end{equation}
To prove the inequality \eqref{claim}, first, we observe that the following holds true:   
\begin{equation}\label{bound on z-z*}
\|Kz_{k+1} - Kz_* \|^2 \leq \|X_{k+1} -X_* \|^2.  
\end{equation}
Further, from \eqref{difference1} and using $L_F$-smoothness of $F$, we have  
\begin{equation}
\|H_{k+1} - H_* \| \leq r \|K z_k - K z_{k+1} \| + L_F \| X_{k+1} -  X_*\|
\end{equation}
This implies 
\begin{equation}\label{bound on H-H*}
\begin{split}
\|H_{k+1} - H_*\|^2 & \leq \left(r \|Kz_k - Kz_{k+1} \| + L_F\|X_{k+1} -  X_*\| \right)^2 \\  & \leq 2 \left( r^2 \| Kz_k - Kz_{k+1}\|^2 + L_F^2 \| X_{k+1} -  X_*\|^2 \right). 
\end{split}
\end{equation}
Thus, we have 
\begin{equation}
\frac{1}{r} \|H_{k+1} - H_*\|^2 \leq 2 \left( r \| Kz_k - Kz_{k+1}\|^2 + \frac{L_F^2}{r} \| X_{k+1} -  X_*\|^2 \right).
\end{equation}
Substituting \eqref{bound on z-z*} and \eqref{bound on H-H*} into left hand side of \eqref{claimequivalence} and rearranging, we have for 
\begin{equation}
\delta = \min \left \{ \frac{1}{2}, \frac{2\lambda_F}{ r + \frac{2L_F^2}{r} } \right \}, 
\end{equation} 
we have 
\begin{eqnarray*}
\delta \left(r\|Kz_{k+1} - Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*}\|^2 \right) &\leq& \delta \left( r + \frac{2L_F^2}{r} \right ) \| X_{k+1} -  X_*\|^2 + 2\delta r \|Kz_k - Kz_{k+1} \|^2 \\
&\leq& 2\lambda_F \|X_{k+1} - X_* \|^2 + r \|K z_{k}- Kz_{k+1}\|^2 \\
&& + \frac{1}{r}\|H_k - H_{k+1} \|^2,
\end{eqnarray*}
by making $\delta$ sufficiently small such that 
\begin{equation}
\delta \leq \frac{2\lambda_F}{ r + \frac{2L_F^2}{r}} = \frac{2r\lambda_F}{r^2 + 2L_F^2} \quad and \quad \delta \leq \frac{1}{2}.
\end{equation}
The convergence rate is then given as follows: 
\begin{equation}
\frac{1}{1 + \frac{2r\lambda_F}{r^2 + 2L_F^2}} = \frac{r^2 + 2L_F^2}{r^2 + 2L_F^2 + 2r\lambda_F} \quad \approx \quad \frac{r}{r + \lambda_F} \quad \mbox{ for } \quad r \gg 1. 
\end{equation}
%Therefore, for $r = 0$, we get 
%\begin{equation}
%\rho = 1. 
%\end{equation} 
%\textcolor{red}{I guess this is not of our main interest here !}
This completes the proof. 
\end{proof}
\begin{remark}
This approach is difficult to apply for inexact GS case even when $F$ is a quadratic functional or when $\omega \neq r$. Also, it is different from the proposed approach since it can not show the convergence for $r = 0$. For $r = 0$, we have that $\rho = 1$. Thus, the convergence can not be attained. 
\end{remark}

\section{On the Linear Convergence of ProxSkip} 

\begin{algorithm}
\caption{ProxSkip}\label{Prox} 
Given $\gamma > 0$ and probability $p > 0$, initial iterate $Kz_0$ and initial control variate, such that $K^TH_0 = 0$, updates are obtained as follows:  
\begin{algorithmic}
\For{$k=0, 1,2,\cdots,T$}
    \State{$X_{k+1}$ update: (with $X_k = Kz_k$),  
    \begin{equation}
    X_{k+1} = X_k - \gamma (A(X_k) - H_k), 
    \end{equation}}
    \State{Flip a coin $\theta_k \in \{0,1\}$ where ${\rm Prob}(\theta_k = 1) = p$}
    \If{$\theta_k = 1$}
    \State{
    \begin{equation}
        K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
    \end{equation} }
    \Else{}
    \State{
    \begin{equation}
        Kz_{k+1} = X_{k+1},         
    \end{equation} }
    \EndIf
    \State{   
    \begin{equation} \label{Hupdate}
        H_{k+1} = H_k + \frac{p}{\gamma} (K z_{k+1} - X_{k+1}). 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}


For the convergence measure, we introduce the so-called Lyapunov function:  
\begin{equation} 
\Psi_k := \|Kz_k - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2.
\end{equation} 
We further define
\begin{subeqnarray}
w_k &=& Kz_k - A (Kz_k) \\ 
w_* &=& Kz_* - A (X_*). 
\end{subeqnarray}
We note that since $\omega = 1/(\gamma n)$ and $p = 1/n$. $\omega^2 = p^2/\gamma^2$. Thus, the weights in the Lyapunov function is nothing else than, 
\begin{equation} 
\omega^2 \Psi_k := \omega^2 \|Kz_k - X_*\|^2 + \|H_k - H_*\|^2.
\end{equation}

Under these settings, we shall then show that ProxSkip generates iterates 
$\{Kz_t\}_{t = 1,\cdots}$ converges linearly in the sense that 
\begin{equation}
\mathbb{E}(\Psi_T) \leq (1 - \zeta)^T \Psi_0, 
\end{equation}
where $\zeta = \gamma \lambda_F$. Here $\gamma = 1/L_F$. 
%Given $\psi : \Reals{d} \mapsto \Reals{}$, we define $\psi^*(y) := \sup_{x \in \Reals{d}} \{ \langle x, y \rangle - \psi(x)\}$ to be its Fenchel conjugate.  
%
%The proximity operator of $\psi^*$ satisfies for any $\tau > 0$. 
%\begin{equation} 
%u = {\rm prox}_{\tau \psi^*} (y) \mbox{ implies } u \in y - \tau %\partial \psi^*(u). 
%\end{equation} 
\begin{theorem} 
For $\gamma > 0$ and $0 < p \leq 1$, we have 
\begin{equation} 
\mathbb{E}(\Psi_{k+1}) \leq (1 - \min \{ \gamma \lambda_F, p^2 \}) \Psi_k. 
\end{equation} 
where the expectation is taken over the $\theta_t$ in the Algorithm 1. 
\end{theorem}  
\begin{proof} 
We let $Kz = P_Z(X)$.   
\begin{equation}
X := X_{k+1} - \frac{\gamma}{p} H_k \quad \mbox{ and } \quad Y = X_* - \frac{\gamma}{p} H_*. 
\end{equation}
Then since $P_Z H_k = 0$ and $P_Z H_* = 0$, we have 
\begin{equation} 
X_* = P_Z(Y). 
\end{equation} 
The method reads as follows: 
\begin{equation}
Kz_{k+1} = \left \{ \begin{array}{ll} P_Z(X) & \mbox{ with probability } p \\ X_{k+1} & \mbox{ with probability } 1 - p \end{array}  \right. 
\end{equation}
Furthermore, we have that 
\begin{equation}
H_{k+1} = H_k + \frac{p}{\gamma} (Kz_{k+1} - X_{k+1}) = 
\left \{ \begin{array}{ll} 
H_k + \frac{p}{\gamma} \left ( P_Z(X_{k+1}) - X_{k+1} \right ) & \mbox{ with  } p \\ H_k & \mbox{ with } 1 - p \end{array}  \right. 
\end{equation}
Now, we compute the expected value of the Lyapunov function 
\begin{equation}
\Psi_k := \|Kz_k - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 
\end{equation}
at the time step $k+1$, with respect to the coin toss at iteration $k$, which is 
\begin{eqnarray}
\mathbb{E}(\Psi_{k+1}) &=& p \left ( \|P_Z(X_{k+1}) - X_*\|^2 + \frac{\gamma^2}{p^2} \left \|H_k + \frac{p}{\gamma} (P_Z(X_{k+1}) - X_{k+1})  - H_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&=& p \left ( \|P_Z(X) - P_Z(Y) \|^2 + \left \| \frac{\gamma}{p} H_k + (P_Z(X) - X_{k+1})  - \frac{\gamma}{p} H_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{t+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&=& p \left ( \|P_Z(X) - P_Z(Y) \|^2 + \left \|P_Z(X) - (X_{k+1} - \frac{\gamma}{p} H_k )  + (Y - X_*) \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&=& p \left ( \|P_Z(X) - P_Z(Y) \|^2 + \left \|Q_Z(X) - Q_Z(Y) \right \|^2 \right ) \\
&& + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\
&\leq& p \left \|\left ( X_{k+1} - \frac{\gamma}{p} H_k \right ) - \left ( X_* - \frac{\gamma}{p} H_* \right ) \right \|^2 \\
&& \quad + (1-p) \left ( \|X_{k+1} - X_*\|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2  \right ) \\ 
&=& \|X_{k+1} - X_* \|^2 + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 - 
2 \gamma \langle X_{k+1} - X_*, H_k - H_* \rangle \\ 
&=& \|(X_k - \gamma A X_k) - (X_* - \gamma AX_*)\|^2 -  \gamma^2 \|H_k - H_*\|^2 \\
&& \quad + \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 \\
&\leq& (1 - \gamma \lambda_F) \|X_K - X_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|H_k - H_*\|^2 \\
&\leq& \max \{ (1 - \gamma \lambda_F), (1 - p^2) \} \Psi_k \\
&=& (1 - \min \{ \gamma \lambda_F, p^2 \} ) \Psi_k. 
\end{eqnarray}
This completes the proof. 
\end{proof} 
\begin{remark}
This results in the choice of $p$, should be not too small. This is misleading since the Gauss-Seidel iteration means that it uses a lot of iterations. But, it can lead to the convergence of the algorithm. We now ask if we can replace GD 1step by GS method. 
\end{remark} 

\subsection{U block analysis in standard way}

We consider to solve the following system:
\begin{subeqnarray*}
A_r X - rKz &=& f \\
-rK^T X + rK^TKz &=& 0.
\end{subeqnarray*}
The Gauss-Seidel method leads 
\begin{subeqnarray*}
A_r X_{k+1} - rKz_k &=& f \\
-rK^T X_{k+1} + rK^TKz_{k+1} &=& 0.
\end{subeqnarray*}
This is equivalent to say that 
\begin{subeqnarray*}
X_{k+1} &=& f + rKz_k \\
K z_{k+1} &=& K (rK^TK)^{-1} rK^T X_{k+1}. 
\end{subeqnarray*}
This can be rewritten as follows: with $B_r = rK^TK$, 
\begin{subeqnarray*}
X_{k+1} &=& X_k + A_r^{-1} (f + rKz_k - A_r X_k) \\ 
Kz_{k+1} &=& Kz_k + K B_r^{-1} (rK^T X_{k+1} - B_r z_k),
\end{subeqnarray*}
where $B_r = r K^T K$. On the other hand, we have the optimality condition that
\begin{subeqnarray*}
X_{*} &=& X_* + A_r^{-1} (f + rKz_* - A_r X_*) \\ 
Kz_{*} &=& Kz_* + K B_r^{-1} (rK^T X_{*} - B_r z_*),
\end{subeqnarray*}
Therefore, the error analysis is given as follows:
\begin{subeqnarray*}
E_{k+1}^X &=& E_k^X + R_r(rE_k^Z - A_r E_k^X) = (I - R_rA_r)E_k^X + r R_r E_k^Z \\ 
E_{k+1}^Z &=& E_k^Z + K B_r^{-1} (rK^T X_{*} - B_r z_*) \\ 
&=& E_k^Z + KB_r^{-1} rK^T E_{k+1}^X - E_k^Z =KB_r^{-1} rK^T E_{k+1}^X \\
&=& P_Z E_{k+1}^X = P_Z ((I - R_rA_r) E_k^X + rR_rE_k^Z)
\end{subeqnarray*}
Note that $P_Z = K (K^TK)^{-1} K^T$. In a matrix form, we have that
\begin{eqnarray*}
\begin{pmatrix} E_{k+1}^X \\ E_{k+1}^Z \end{pmatrix} = \begin{pmatrix} I - R_r A_r & r R_r \\
P_Z (I - R_r A_r) & P_Z (r R_r) \end{pmatrix} 
\begin{pmatrix} E_{k}^X \\ E_k^Z \end{pmatrix}. 
\end{eqnarray*}
\begin{subeqnarray*}
R_r &:=& ( I - (I - \gamma A_r)^n) A_r^{-1} = ( I - (I - \gamma A_r)^n) A_r^{-1}.  
\end{subeqnarray*}
For $n$ large enough, we shall consider two operators: with $\gamma = 1/(r + L_F)$, 
\begin{eqnarray*}
I - R_r A_r &=& (I - \gamma A_r)^n = \rho^n \\
\sigma(R_r) &=& \left ( 1 - \left ( 1 - \frac{r + \lambda}{r + L_F} \right )^n \right ) \frac{1}{r+\lambda} \\
\rho(R_r) &\leq& (1 - \delta^n) \frac{1}{r+\lambda_F}. 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
\begin{pmatrix} \|E_{k+1}^X\| \\ \|E_{k+1}^Z\| \end{pmatrix} = \begin{pmatrix} \delta^n & (1 - \delta^n) \frac{r}{r + \lambda_F} \\
\delta^n & (1 - \delta^n) \frac{r}{r + \lambda_F} \end{pmatrix} 
\begin{pmatrix} \|E_{k}^X\| \\ \|E_k^Z\| \end{pmatrix}. 
\end{eqnarray*}










%\begin{remark} 
%Since we have that with $\omega = \frac{2}{r + L_F + r + \lambda_F}$, 
%\begin{eqnarray*}
%\lambda_{max}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+L_F} = \frac{r + L_F - \omega}{r + L_F} \\  
%\lambda_{min}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+\lambda_F} = \frac{r %+ \lambda_F - \omega}{r + \lambda_F}.
%\end{eqnarray*}
%Thus, we have that
%\begin{equation}
%\frac{\kappa(A_1) - 1}{\kappa(A_1) + 1} = \frac{ \frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} - 1}{\frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} + 1} =\frac{ \frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} - 1}{\frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} + 1} = \frac{\frac{r + \lambda_F}{r + L_F}  - \frac{\lambda_F}{L_F}}{\frac{r + \lambda_F}{r + L_F} + \frac{\lambda_F}{L_F}}  
%\end{equation} 
%and 
%\begin{equation}
%\frac{\kappa(A_2) - 1}{\kappa(A_2) + 1} = \frac{ \frac{r + \lambda_F}{r + L_F} - 1}{\frac{r + \lambda_F}{r + L_F}  + 1}  
%\end{equation}
%On the other hand, we have that 
%\begin{equation}
%\sin (\theta_1 + \theta_2) = 
%\end{equation}
%and $\lambda_{min} = 1 - \frac{\omega}{r + \lambda_F} = \frac{\lambda_F}{r + \lambda_F}$ while $H_{A_r} = 1/(r + L_F)$ and $1/(r + \lambda_F)$. Therefore, if $r$ is sufficiently large, then $\sin\theta$ is quite small. 
%\begin{theorem}
%For $A_1$ and $A_2$, symmetric positive definite matrices, we have 
%\begin{equation}
%(x,A_1A_2y) \leq \sin (\theta_1 + %\theta_2) \|A_1x\|\|A_2y\|,  
%\end{equation}
%where $\sin \theta_1 = \frac{\kappa(A_1) - 1}{\kappa(A_2) + 1}$ and $\sin \theta_2 = \frac{\kappa(A_2) - 1}{\kappa(A_2) + 1}$. 
%\end{theorem}
%\end{remark}

\begin{comment} 
\subsubsection{Convergence analysis of Algorithm \ref{algADMM3} with $G_{n,r} = A_r^{-m}$}
In this section, we shall discuss the case when the exact solver for $A_r$ is applied a number of times, say $m$. The main motivation behind this investigation is to use the action of the parameter $r$. 

\begin{theorem}\label{main:theorem04} 
The Algorithm \ref{algADMM3} with $G_{n,r} = A_r^{-m}$ produces iterate $(X_k, z_k, H_k)$, for which the following error bound holds true: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 + \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM3} leads to iterates, given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^{-m}(H_k + rKz_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ). 
\end{eqnarray*}
We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. This is due to the proximal operator $P_Z = K(K^TK)^{-1}K$. Therefore, we have 
\begin{eqnarray*}
X_{k+1} &=& A_r^{-m}(H_k + rKz_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T A_r^{-m}(H_k + rKz_k)) = P_Z [A_r^{-m} (H_k + rKz_k)]  \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{-m} (H_*+rKz_*) \\
Kz_{*} &=& P_Z [A_r^{-m}(H_*+rKz_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
E_{k+1}^X &=& A_r^{-m} (H_* + rKz_*) - A_r^{-m}(H_k + rKz_k) \\
E_{k+1}^Z &=& P_Z [ A_r^{-m} (H_*+rKz_*) - A_r^{-m}(H_k + rKz_k) ] \\
E_{k+1}^H &=& H_* - H_k + \omega (-X_* + X_{k+1} + Kz_{*} - K z_{k+1})
\end{eqnarray*}
Rearranging the error in $H$ variable, we have 
\begin{equation}\label{errorH2}
E_{k+1}^H - \omega E_{k+1}^Z = E_k^H - \omega E_{k+1}^X = E_k^H - \omega \left( A_r^{-m} (H_*+rKz_*) - A_r^{-m}(H_k +rKz_k) \right).    
\end{equation}
Taking the squared norm on both sides of the equation \eqref{errorH2}, and using the orthogonality between $E_i^H$ and $E_j^Z$ for all $i,j$, we have 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{-m}(H_*+rKz_*) - A_r^{-m}(H_k+rKz_k))\|^2. 
\end{eqnarray*}
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^{-m}(H_k + rKz_*) - A_r^{-m}(H_k + rKz_k). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{-m} (H_* + r K z_*) - A_r^{-m} (H_k + r K z_k))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^{-m}} = 1/(r + L_F)^m$ and $L_{G^{-m}} = 1/(r + \lambda_F)^m$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{(r + L_F)^m} + \frac{1}{(r + \lambda_F)^m}} = \frac{2 (r + \lambda_F)^m(r + L_F)^m}{(r + L_F)^m + (r + \lambda_F)^m}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right ) \frac{r}{(r+\lambda_F)^m} \|E_k^H\|\|E_k^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right ) \frac{r}{(r+\lambda_F)^m}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^{-m}} + L_{G^{-m}})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{-m} (H_* + rK z_*) - A_r^{-m}(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^{-m}(H_k + r Kz_*) - A_r^{-m} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{(r+\lambda_F)^m} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} \right )^2 + \left ( \frac{r}{(r + \lambda_F)^m} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\end{comment} 

\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}

We remark that 
\begin{equation}
\frac{\kappa(G^{-m}) - 1}{\kappa(G^{-m}) + 1} = \frac{\frac{(r+L_F)^m}{(r+\lambda_F)^m} - 1}{\frac{(r+L_F)^m}{(r+\lambda_F)^m}+1} = \frac{(r+L_F)^m - (r+\lambda_F)^m}{(r+L_F)^m + (r+\lambda_F)^m}. 
\end{equation} 
This provides the convergence rate that speeds up with the larger $r$ now. 
\end{proof} 


\end{comment} 
\end{document} 

\section{Convergence analysis of  Algorithm \ref{algADMM1}}

In this section, we shall present the convergence of the Algorithm \ref{algADMM1}. This section consists of two parts. One part is the convergence when $D_r^* = A_r^*$ and the other part is the convergence when $D_r^*$ is the $N-$step GD method. 

\subsection{Linear Convergence of Algorithm \ref{algADMM1} for $D_r = A_r$}

In this section, we shall establish the linear convergence of the exact ADMM method with $D_r = A_r$.
\begin{comment} 
\begin{lemma}
The following holds true: for all $Y, Y_k$ in $\Reals{N_x}$, 
\begin{eqnarray}
\|A_r^*(Y) - A_r^*(Y_k) \| &\leq& \frac{1}{r + \lambda_F} \|Y- Y_k\|, \\ 
\|(I - \omega A_r^*) (Y) - (I - \omega A_r^*) (Y_k)   \| &\leq& \left(1 - \frac{\omega}{r + L_F} \right) \|Y -Y_k \|
\end{eqnarray}
\end{lemma}
\begin{proof}
    Note that $A_r^*(Y)$ corresponds to the gradient of the function $F_r^*(Y)$ that is the dual of $F_r(X) = F(X) + \frac{r}{2} \| X\|^2$. 
    $F_r^*$ is $\frac{1}{r +L_F}$-strongly convex and $\frac{1}{r + \lambda_F}$-smooth since $F_r$ is $(r + \lambda)$-strongly convex and $(r + L_F)$-smooth.
    Therefore, we have 
    \begin{equation}
        \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|. 
    \end{equation}

For $ (I -  \omega A_r^*)(Y)$, we have
 that the 
 \begin{equation}
 \begin{aligned}
    \|  I - \omega \nabla^2F^*_r(Y) \| 
 = \rho ( I - \omega \nabla^2F^*_r(Y)) & \leq \max \left\{|1 - \omega \lambda_{min}(\nabla^2 F_r^*(Y))|, | 1-\omega \lambda_{max} \nabla^2 F_r^*(Y)| \right\} \\
 & \leq \max \left\{  |1 - \frac{\omega}{r + L_F} |, | 1-\frac{\omega}{r + \lambda_F}| \right\}
 \end{aligned}
 \end{equation}
 where $\omega < \frac{2}{ \frac{1}{r + \lambda_F}}$ for any $Y$. 
 For optimal $\omega$ is $\frac{2}{ \frac{\omega}{r + \lambda_F}+ \frac{\omega}{r + L_F}}$, which leads to 
 \begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa + 1} \|Y - Y_k \|,
\end{equation}
where $\kappa = \frac{r+L_F}{r+\lambda_F}$. 

In other analysis where we restrict $\omega \leq \frac{1}{ \frac{1}{r + \lambda_F} }$, we have the following optimal bound
\begin{equation}
    \| (I - \omega A_r^*)(Y) - (I - \omega A_r^*)(Y) \| \leq  \frac{\kappa - 1}{\kappa} \|Y - Y_k \|.
\end{equation}
\end{proof}
\begin{remark}
    This Hessian argument always requires $L$-smoothness of the objective thanks to the mollifier argument. See Lemma \ref{lemmaGD}.
\end{remark}
\end{comment} 

% \begin{proof}
% Let $X = A_r^*(Y)$, $X_k = A_r^*(Y_k)$, that is $Y  = A_r(X)$, $Y_k = A_r(X)$
% \begin{eqnarray*}
% (r+\lambda) \|X - X_{k}\|^2 &\leq& \langle X - X_{k+1}, A_r(X) - A_r(X_{k+1}) \rangle \\
% &=& \langle X - X_k, Y - Y_k \rangle \\ 
% &\leq& \|X - X_k\|  \|Y - Y_k\|. 
% \end{eqnarray*}
% Hence, 
% \begin{eqnarray*}
% \|A_r^*(Y) - A_r^*(Y_k) \| \leq \frac{1}{r + \lambda_F} \|Y - Y_k\|.   
% \end{eqnarray*}
% This completes the proof.
% \end{proof}

\begin{theorem}\label{thm:GS}
The Algorithm \ref{algADMM1} with $D_r = A_r$ and $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ has the convergence rate given as follows: 
\begin{eqnarray}\label{gsrate}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \rho^2_{GS}(r,L_F,\lambda_F) \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ), 
\end{eqnarray}
where with $\kappa(G) = \frac{r+L_F}{r + \lambda_F}$, 
\begin{equation} 
\rho^2_{GS}(r,L_F,\lambda_F) =  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2. 
\end{equation} 
\textcolor{red}{The first one is from error in Schur Complement and the second one is from error between exact $X$ and the $X_k$. The expression is from the case when GS is used. I am thinking how it will be like for linear analysis.}


We also have that 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ). 
\end{eqnarray*}
Furthermore, there exists a single optimal $r > 0$ which gives the optimal convergence rate and the convergence factor is always smaller than one for all $r \geq 0$. 
\end{theorem}
\begin{proof} 
The Algorithm \ref{algADMM1} produces iterates given as follows: 
\begin{eqnarray*}
X_{k+1} &=& A_r^* (H_k + r K z_k) \\
Kz_{k+1} &=& K(rK^TK)^{-1} (rK^T X_{k+1} - K^TH_k) \\ 
H_{k+1} &=& H_k + \omega (-X_{k+1} + Kz_{k+1} ), 
\end{eqnarray*}
where $A_r^*$ is the Fenchel-dual conjugate of $A_r$. We first notice that if $K^TH_0 = 0$, then $K^TH_k = 0$ and also $K^TH_* = 0$. Now due to Lemma \ref{main:lem1}, we have that with $D_r = A_r$,  
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \omega^2 \|E_{k+1}^Z\|^2 = \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 %\\   
%&=& \|H_* - H_k - \omega (A_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*))\| \\
%&& + \omega \|D_r^*(H_k + r Kz_*) - D_r^{*} (H_k + r K z_k)\|
\end{eqnarray*}
\begin{comment} 
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& A_r^{*} (H_* + r K z_*) \\
Kz_{*} &=& P_Z [A_r^{*}(H_* + rK z_*)] \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ]. 
\end{eqnarray*}
The trick is to multiply $-\omega$ for $E_{k+1}^Z$ error term and to obtain 
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] \right ). 
\end{eqnarray*}
Lastly, for $H$, we have 
\begin{eqnarray*}
H_{*} - H_{k+1} &=& H_* - H_k + \omega ( -X_* + X_{k+1} + Kz_* - K z_{k+1} ) \\ 
&=& H_* - H_k - \omega [ X_* - X_{k+1} - (Kz_* - K z_{k+1}) ] \\  
&=& H_* - H_k - \omega [ A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k) \\
&& - P_Z [ A_r^{*} (H_* + rK z_*) - A_r^{*} (H_k + rK z_k) ] ] \\
&=& H_* - H_k - \omega Q_Z (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)) \\ 
&=& Q_Z [H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))] 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
H_{*} - H_{k+1} - \omega (Kz_* - K z_{k+1}) = H_* - H_k - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k)). 
\end{eqnarray*}
\end{comment} 
We now define two important quantities: 
\begin{eqnarray}
A_{H_*,H_k}^{Z_*} &:=& A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*) \\ 
A_{Z_*,Z_k}^{H_k} &=& A_r^*(H_k + rKz_*) - A_r^*(H_k + rKz_k). 
\end{eqnarray}
%We note that the cross term is the problematic term given as follows: 
%\begin{eqnarray*}
%2 \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, \omega A_{Z_*,Z_k}^{H_k} \right \rangle = 2 \omega \left \langle H_* - H_k - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.     
%\end{eqnarray*}
Then, we have that  
\begin{eqnarray*}
\|E_{k+1}^H - \omega E_{k+1}^Z\|^2 &=& \|E_k^H - \omega (A_r^{*} (H_* + r K z_*) - A_r^{*} (H_k + r K z_k))\|^2 \\ 
&=& \|E_k^H - \omega (A_{H_*,H_k}^{Z_*} + A_{Z_*,Z_k}^{H_k})\|^2,  \\
&\leq& \|E_k^H - \omega A_{H_*,H_k}^{Z_*}\|^2 + \omega^2 \|A_{Z_*,Z_k}^{H_k}\|^2 \\
&& -2 \omega \left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle.  
\end{eqnarray*}
Since $\lambda_{G^*} = 1/(r + L_F)$ and $L_{G^*} = 1/(r + \lambda_F)$, we have that 
\begin{equation}
\omega = \frac{2}{\lambda_{G^*} + L_{G^*}} = \frac{2}{\frac{1}{r + L_F} + \frac{1}{r + \lambda_F}} = \frac{2 (r + \lambda_F)(r + L_F)}{2r + L_F + \lambda_F}  
\end{equation} 
and 
\begin{eqnarray*}
\left \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \right \rangle \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F} \|E_k^H\|\|E_k^z\|.  
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray}\label{main:ineq}
&& - 2 \omega \langle E_k^H - \omega A_{H_*,H_k}^{Z_*}, A_{Z_*,Z_k}^{H_k} \rangle \leq 2 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right ) \frac{r}{r+\lambda_F}  \|E_k^H\|\|E_k^Z\|_{\omega} \label{main:1eq} \\ 
&& \qquad \leq \left ( \frac{r}{r + \lambda_F} \right )^2 \|E_k^H\|^2 + \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^Z\|_{\omega}^2. \label{main:2eq}   
\end{eqnarray}
Again with $\omega = 2/(\lambda_{G^*} + L_{G^*})$, we have that 
\begin{eqnarray*}
&& \|E_k^H - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\  && \qquad \leq \|(H_* + rKz_*) - (H_k + rKz_*) - \omega (A_r^{*} (H_* + rK z_*) - A_r^*(H_k + rKz_*))\|^2 \\
&& \qquad \leq \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 \|E_k^H\|^2. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\omega^2 \|A_r^*(H_k + r Kz_*) - A_r^{*} (H_k + r K z_k)\|^2 \leq \left ( \frac{r}{r+\lambda_F} \right )^2 \|E_k^Z\|_\omega^2. 
\end{eqnarray*}
Therefore, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} \leq \left \{ \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r + \lambda_F} \right )^2 \right \} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2_{\omega} \right ).  
\end{eqnarray*}
\begin{comment} 
By applying the simple long division, we obtain that 
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2}{(r + \lambda_F)^2} = 1 + \frac{-2\lambda_F r -\lambda_F^2 + (L_F - \lambda_F)^2}{(r + \lambda_F)^2} = 1 - f(r), 
\end{eqnarray*}
where $f(r)$ is given as follows:
\begin{equation}
f(r) = \frac{2\lambda_F(r + L_F) - L_F^2}{(r + \lambda_F)^2}.
\end{equation} 
\begin{figure}[h]
\centering
\includegraphics[width=10cm,height=7cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
The graphs of $f(r)$ and $g(r)$ are presented in Figure \ref{exam} and a simple calculation shows that 
\begin{equation}
\frac{(L_F-\lambda_F)^2}{\lambda_F} = {\rm arg}\max_{r \geq 0} f(r). 
\end{equation} 
Furthermore, we have that
\begin{eqnarray*}
\frac{(L_F - \lambda_F)^2 + r^2 }{(2r + L_F + \lambda_F)^2} = \frac{1}{4} - g(r), 
\end{eqnarray*}
where 
\begin{eqnarray*}
g(r) = \frac{(L_F + \lambda_F)r - (L_F - \lambda_F)^2 + (L_F + \lambda_F)^2/4}{(2r + L_F + \lambda_F)^2}.  
\end{eqnarray*}
It is easy to see that 
\begin{equation}
2\frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} = {\rm arg} \min_{r \geq 0} g(r) \quad \mbox{ and } \quad g \left (2 \frac{(L_F - \lambda_F)^2}{L_F + \lambda_F} \right ) < \frac{1}{4}.    
\end{equation}
Note also that it holds true that 
\begin{equation}
{\rm arg}\max_{r} g(r) \leq {\rm arg}\max_r f(r). 
\end{equation} 
Thus, the convergence rate can be estimated as follows: 
\begin{eqnarray*}
\|E_{k+1}^H\|^2 + \|E_{k+1}^Z\|^2_{\omega} &\leq& \max \left \{ \frac{1}{4}, \left ( 1 - \frac{L_F^2 \lambda_F^2 - 2\lambda_F^2L_F + 2\lambda_F^4}{((L_F - \lambda_F)^2 + \lambda_F^2)^2} \right ) \right \} \left ( \|E_{k}^H\|^2 + \|E_{k}^Z\|^2_{\omega}  \right ). 
\end{eqnarray*}
\end{comment} 
This provides the convergence rate. Finally, we notice that for all $r \geq 0$, 
\begin{eqnarray*}
\frac{r^2}{\omega^2} = \frac{r^2 (2r + L_F + \lambda_F)^2}{4(r + \lambda_F)^2(r + L_F)^2} < 1. 
\end{eqnarray*}
Thus, we obtain that due to the orthogonality, 
\begin{eqnarray*}
\|E_{k+1}^X\|^2 &=& \|A_r^*(H_* + rKz_*) - A_r^*(H_k + r K z_k) \|^2 \\
&\leq& \frac{1}{(r + \lambda_F)^2}\|E_k^H - r E_k^Z\|^2 = \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + r^2 \|E_k^Z\|^2 \right ) \\
&=& \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \frac{r^2}{\omega^2} \|E_k^Z\|_\omega ^2 \right ) \leq \frac{1}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) 
\end{eqnarray*}
We now discuss the convergence factor denoted by $\rho_{GS}^2$ and given by 
\begin{equation}
f(r) = \rho^2_{GS}(r, L_F, \lambda_F) = \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 = \left ( \frac{L_F - \lambda_F}{2r + L_F + \lambda_F} \right )^2 + \left ( \frac{r}{r+\lambda_F} \right )^2 
\end{equation}
We note that the simple calculation shows that the derivative of $f(r)$ is given as follows: 
\begin{equation}
f'(r) = \frac{-4(L-\lambda)^2}{(2r + L_F + \lambda_F)^3} + \frac{2r \lambda_F}{(r + \lambda_F)^3}. 
\end{equation}
Therefore, for small $r$, it takes the negative sign, but it changes its sign for larger $r$ and continues to be positive. Thus, there exists a single critical point, which gives the optimal $r_{\rm opt}$. On the other hand, it is continuously increasing (see Figure \ref{exam}). Since $\lim_{r \rightarrow \infty} f(r) = 1$ and thus, for any $r \geq 0$, the convergence factor is smaller than one. Thus, it converges linearly for all fixed $r \geq 0$. This completes the proof.
\end{proof} 
\begin{figure}[h]
\centering
\includegraphics[width=12cm,height=6cm]{plot1.png}
\caption{Graphs of the convergence factor as a function of $r$ for $L_F = 5$ and $\lambda_F = 0.5$}\label{exam} 
\end{figure}
\begin{comment} 

\begin{remark}[Convergence rate when $\omega = r$]
In our current convergence analysis, we require $\beta \in \left( 0,\frac{1}{2} \right )$. We shall make a more precise convergence analysis as follows.
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{ L_F r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{L_F }{r + L_F} \right )^2 \right ) &=& \left ( \frac{ L_F  r^{2\alpha} (r + L_F)}{(r + L_F)^2(r + \lambda_F)} + \frac{L_F^2 (r+ \lambda_F) }{(r + L_F)^2 (r + \lambda_F)} \right ). 
%&& \Longleftrightarrow ((r + L)(r + \lambda) + L r^2 ) L^2 < (r + L)^3 (r + \lambda). 
\end{eqnarray*} 
The convergence rate in $z$-error is given by 
\begin{eqnarray*}
    \left ( \frac{L_F r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) &=&    \left ( \frac{L_F }{(r^{1 - 2 \beta} + L_F/r^{2\beta})(r + \lambda_F)} + \left ( \frac{1}{1 + \lambda_F/r} \right )^2 \right )   
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, we have that 
\begin{equation}
\frac{L_F }{(r^{1 - 2 \beta} + L_F r^{-2\beta})(r + \lambda_F)} = \frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta} + \lambda_F L_F r^{-2\beta}}    
\end{equation}
and we have 
\begin{equation}
    \left ( 1 + \lambda_F/r \right )^{-2}  \leq 1 - \frac{2 \lambda_F}{r} + \frac{3 \lambda_F^2}{(1 -\varepsilon)^4 r^2},  
\end{equation}
since it holds that 
\begin{eqnarray*}
(1 + x)^{-2} &=& 1 - 2x + \frac{3}{ (1 + \xi )^4} x^2, \quad x \in (-\varepsilon,1), \quad \xi \in[-\varepsilon, x] \\ 
&\leq&   1 - 2x + \frac{3}{ (1 -\varepsilon)^4} x^2 \end{eqnarray*}
Thus, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}

\textbf{Case 2: $r \ll 1$}.
For error in $H$, the convergence rate is bounded below by $\left( \frac{L_F}{r + L_F} \right)^2$. The convergence in error $H$ deteriorates as $r \to 0$. 

For error in $z$, the convergence rate can be made arbitrarily small if $r$ is sufficiently small. 

\textbf{Therefore, we conclude that when $\omega = r$. For sufficiently large $r$, the convergence in $H$ error becomes better while the convergence in $z$ error deteriorates when $r$ becomes larger. The asymptotic rate in $z$ error is $1 - \frac{2 \lambda_F}{r}$. 
On the other hand, for small $r \ll 1$, the convergence in $H$ deteriorates while the convergence in $z$ error becomes better. }
\end{remark}

\begin{remark}[Convergence rate when $\omega = r + \lambda_F$]
The convergence rate in $H$-error is given by 
\begin{eqnarray*}
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\alpha}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{(L_F - \textcolor{red}{\lambda_F})}{r + L_F} \right )^2 \right ) 
\end{eqnarray*} 

The convergence rate in $z$-error is given by 
\begin{eqnarray*} 
\left ( \frac{(L_F - \textcolor{red}{\lambda_F}) r^{2\beta}}{(r + L_F)(r + \lambda_F)} + \left ( \frac{r}{r + \lambda_F} \right )^2 \right ) 
\end{eqnarray*}

\textbf{Case 1: $r \gg 1$}. 
The analysis is similar to that when $\omega = r$.

For $H$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently large.

For $z$ error, the convergence rate is bounded by 
\begin{equation}
1 - c(r,\beta) = 1 - \left (\frac{2 \lambda_F}{r} -\frac{L_F - \lambda_F}{ r^{2-2\beta} + (\lambda_F + L_F) r^{1-2 \beta}} -\frac{3 \lambda_F^2}{ (1 -\varepsilon)^4 r^2} \right ). 
\end{equation}
The asymptotic convergence rate is $1 - \frac{2\lambda_F}{r}$. 

\textbf{Case 2: $r \ll 1$}.
For $H$ error, the convergence rate is bounded below by $\left( \frac{L_F - \lambda_F}{r + L_F} \right)^2 $. Asymptotically, it is $\left( \frac{L_F - \lambda_F}{L_F} \right)^2$.

For $z$ error, the convergence rate can be made arbitrarily small if $r$ is sufficiently small.
\end{remark}

\begin{remark}
    In fact, from standard asymptotic analysis, one should always choose $\omega > r$, say, $\omega = r + \lambda_F$, with sufficiently small $r$.
\end{remark}
\end{comment} 

% \begin{remark}
% We note that the larger the $r$, the convergence of $H_k$ to $H_*$ seems to be faster while the convergence of $Kz_k$ to $Kz_*$ deteriorates.% On the other hand, the larger $r$ guarantees the sufficiently near orthogonality for the cross terms.  
% \end{remark}
%\begin{remark}
%It is interesting to note that if $\omega = r + L_F$, then the cross term disappears. Thus, the error analysis reduces to the following form: 
%\begin{eqnarray*}
%\|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} &\leq& \left ( \frac{r}{r + \lambda_F} \right )^2 \|Kz_* - K z_k\|^2_{\omega} \\ 
%&=&  \left ( \frac{r}{r + \lambda_F} \right )^2 \left ( \|H_{*} - H_{k}\|^2 + \|Kz_* - K z_k\|^2_{\omega} \right %).
%\end{eqnarray*}
%\end{remark} 

\subsection{Analysis based on $E_k^X$ and the convexity of $F$ due to \cite{shi2014linear}} 
In this section, we shall adapt the proof originated from \cite{shi2014linear} to establish the result of linear convergence.
\begin{lemma} 
The algorithm \ref{algADMM1} produces iterates $(X_k,z_k,H_k)$ such that the following identity holds: for all $k=0,1,2\cdots$, 
\begin{eqnarray*}\label{equal form 2}
\nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &=& 0, \\
H_{k+1} - H_{k} + r \left( I - P_Z \right) X_{k+1} &=& 0,\\
K z_{k+1} - P_Z X_{k+1} &=& 0.
\end{eqnarray*}
\end{lemma} 
\begin{proof} 
First of all, we note that $K^TH = 0$ with $H = H_k$ or $H = H_*$ and also 
\begin{equation}
Kz = P_Z X. 
\end{equation}
Now, subtracting \eqref{H update} from \eqref{X update}, multiplying $K^T$ to \eqref{H update} and adding it to \eqref{z update}, we have
\begin{equation}\label{equal form 1}
    \begin{split}
        \nabla F(X_{k+1}) - H_{k+1} + r K(z_{k+1} - z_{k}) &= 0, \\
        K^T H_{k+1} &= 0, \\
        H_{k+1} - H_{k}  - r (K z_{k+1} - X_{k+1}) &= 0. 
    \end{split}
\end{equation}
Multiplying the third equation in \eqref{equal form 1} by $K^T$ and using the second equation, we complete the proof. 
\end{proof}





We shall obtain a simple but important lemma: 
\begin{lemma} 
The following identity holds: 
\begin{eqnarray}
\nabla F(X_{k+1}) - \nabla F(X_*) &=& r K (z_{k} - z_{k+1}) + H_{k+1} - H_*  \label{difference1}\\
H_{k+1} - H_k &=& - r Q_Z (X_{k+1} - X_*)  \label{difference2} \\
K(z_{k+1} - z_*) &=& P_Z (X_{k+1} - X_*) \label{difference3}    
\end{eqnarray}
\end{lemma} 
\begin{proof} 
We recall the optimality condition, which can be given as follows:
\begin{subeqnarray*}
\nabla F(X_*) - H_* &=& 0 \\ 
K^T H_* &=& 0 \\
K z_* - X_* &=& 0 
\end{subeqnarray*}
However, using the property of $Q_Z$ and $P_Z$, the optimality condition implies that it holds true 
\begin{subeqnarray}\label{KKT}
0 &=& P_Z (K z_* - X_{*}) = K z_* - P_Z X_* \\ 
0 &=& Q_Z (Kz_* - X_*) = - Q_Z X_*. 
\end{subeqnarray}
Subtracting the optimality conditions \eqref{KKT} from \eqref{equal form 2}. This completes the proof. 
\end{proof} 
%\begin{proposition}
%If each $F_i$ is $\lambda_i$-strongly convex, and $L_i$-smooth, then $F(X) = \frac{1}{n}\sum_{i = 1}^n F_i(x_i)$ is $\lambda$-strongly convex and $\frac{M_f}{n}$-smooth, where $\lambda = \frac{\min_{i} m_i}{n}$, $M_f = \max_i M_i$.  
%\end{proposition}
The main theorem considers the convergence of a vector $Y$ that combines the primal variable $Kz$ and the dual variable $H$,
\begin{equation}
Y = \begin{pmatrix}
Kz \\
H
\end{pmatrix} \quad \mbox{ and } \quad C = \begin{pmatrix}
r I & 0 \\
0 & \frac{1}{r} I 
\end{pmatrix}
\end{equation}
We also define a $C-$norm on $Y = (y_1,y_2)^T$ by 
\begin{eqnarray*}
\|Y\|_C^2 &=& (CY, Y) = r \|y_1\|^2 + \frac{1}{r}\|y_2\|^2 \\ 
&=& r (Kz, Kz) + \frac{1}{r} (H, H) = (rK^TK z, z) + \frac{1}{r}(H,H).   
\end{eqnarray*}

We shall need the following lemma: 
\begin{lemma}
We have the following identity:
\begin{eqnarray*}
2 \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \|Y_{k} - Y_*\|^2 - \|Y_{k+1} - Y_* \|^2 - \|Y_{k+1} - Y_k\|^2. 
\end{eqnarray*} 
\end{lemma} 
\begin{proof} 

\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\ 
&=& \langle Y_{k} - Y_* - (Y_{k+1} - Y_*), Y_{k+1} - Y_* \rangle \\
&=&  \langle Y_{k} - Y_*, Y_{k+1} - Y_* \rangle - \| Y_{k+1} - Y_* \|^2
\end{eqnarray*} 
On the other hand, we have 
\begin{eqnarray*}
\langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_* \rangle &=& \langle Y_{k} - Y_{k+1}, Y_{k+1} - Y_k + Y_k - Y_* \rangle \\ 
&=& - \|Y_{k+1} - Y_{k}\|^2 + \langle Y_{k} - Y_{k+1}, Y_k - Y_* \rangle . 
\end{eqnarray*} 
By adding these two identities, we obtain the result. This completes the proof. 
\end{proof}
\begin{theorem}
Assume that $K^TH_0 = 0$. Then the ADMM iterations produces $Y_k = [Kz_k; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [Kz_*; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1+\delta} \| Y_{k} - Y_*\|^2_C,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C
\end{equation}
\end{theorem}
\begin{proof}
We begin with using the $\lambda-$strongly convexity condition for $F$ as follows: 
\begin{eqnarray*}
\lambda \| X_{k+1} - X_* \|^2 &\leq& \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*)\rangle \\
&\leq& \langle X_{k+1} - X_*, r K(z_k - z_{k+1}) \rangle + \langle X_{k+1} - X_*, H_{k+1} - H_* \rangle \\
&=& r \langle X_{k+1} - X_*, P_Z (K(z_k - z_{k+1}))  \rangle + \langle X_{k+1} - X_*, Q_Z(H_{k+1} - H_*) \rangle \\
&=& r \langle  P_Z (X_{k+1} - X_*) , Kz_k - Kz_{k+1}   \rangle + \langle Q_Z (X_{k+1} - X_*), H_{k+1} - H_* \rangle  \\
&=& r \langle Kz_k - Kz_{k+1}, Kz_{k+1} - Kz_*  \rangle + \frac{1}{r} \langle H_{k} - H_{k+1}, H_{k+1} - H_* \rangle \\
&=& (Y_{k} - Y_{k+1})^T C (Y_{k+1} - Y_*),  
\end{eqnarray*}
where 
\begin{equation}
Y_k = \begin{pmatrix} 
      Kz_k \\
      H_k 
      \end{pmatrix}, \quad Y_{k+1} = \begin{pmatrix} 
      Kz_{k+1} \\
      H_{k+1}  
      \end{pmatrix}
,\quad Y_{*} = \begin{pmatrix} 
      Kz_{*} \\
      H_{*}  
      \end{pmatrix}
\end{equation}

and the matrix 
\begin{equation}
    C = \begin{pmatrix}
r I & 0 \\
0 &  \frac{1}{r} I
\end{pmatrix}
\end{equation}
This implies
\begin{equation}
    \lambda  \| X_{k+1} - X_* \|^2 \leq  \frac{1}{2} \|Y_k -Y_* \|^2_C - \frac{1}{2}  \|Y_{k+1} - Y_* \|^2_C -  \frac{1}{2}  \| Y_k - Y_{k+1}\|^2_C. 
\end{equation}
Now, by rearranging terms, we have 
\begin{equation}\label{ineq from strong convexity}
2\lambda \| X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C + \|Y_{k+1} - Y_* \|^2_C \leq  \|Y_k -Y_* \|^2_C.
\end{equation}
This immediately, leads to 
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|Y_{k} - Y_* \|^2_C.
\end{equation}
Having \eqref{ineq from strong convexity}, it suffices to show for some $\delta > 0$, we have 
\begin{equation}\label{claim}
\delta \|Y_{k+1} - Y_* \|^2_C \leq 2 \lambda \|X_{k+1} - X_* \|^2 + \| Y_k - Y_{k+1}\|^2_C,
\end{equation}
or equivalently,
\begin{eqnarray*}\label{claimequivalence}
\delta \left( r  \|Kz_{k+1}- Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*} \|^2 \right) &\leq&  2 \lambda \|X_{k+1} - X_* \|^2 \\
&+& r \|Kz_{k}- Kz_{k+1} \|^2 + \frac{1}{r} \|H_k - H_{k+1} \|^2, 
\end{eqnarray*}
which will imply the desired inequality: 
\begin{equation}
\|Y_{k+1} - Y_* \|^2_C \leq \frac{1}{1 +\delta} \|Y_{k} - Y_* \|^2_C.
\end{equation}
To prove the inequality \eqref{claim}, first, we observe that the following holds true:   
\begin{equation}\label{bound on z-z*}
\|Kz_{k+1} - Kz_* \|^2 \leq \|X_{k+1} -X_* \|^2.  
\end{equation}
Further, from \eqref{difference1} and using $L$-smoothness of $F$, we have  
\begin{equation}
\|H_{k+1} - H_* \| \leq r \|K z_k - K z_{k+1} \| + L \| X_{k+1} -  X_*\|
\end{equation}
This implies 
\begin{equation}\label{bound on H-H*}
\begin{split}
\|H_{k+1} - H_*\|^2 & \leq \left(r \|Kz_k - Kz_{k+1} \| + L\|X_{k+1} -  X_*\| \right)^2 \\  & \leq 2 \left( r^2 \| Kz_k - Kz_{k+1}\|^2 + L^2 \| X_{k+1} -  X_*\|^2 \right). 
\end{split}
\end{equation}
Thus, we have 
\begin{equation}
\frac{1}{r} \|H_{k+1} - H_*\|^2 \leq 2 \left( r \| Kz_k - Kz_{k+1}\|^2 + \frac{L^2}{r} \| X_{k+1} -  X_*\|^2 \right).
\end{equation}
Substituting \eqref{bound on z-z*} and \eqref{bound on H-H*} into left hand side of \eqref{claimequivalence} and rearranging, we have 
\begin{eqnarray*}
\delta \left(r\|Kz_{k+1} - Kz_{*} \|^2 + \frac{1}{r} \|H_{k+1} - H_{*}\|^2 \right) &\leq& \delta \left( r + \frac{2L^2}{r} \right ) \| X_{k+1} -  X_*\|^2  \\
&& + 2\delta r \|Kz_k - Kz_{k+1} \|^2 \\
&\leq& 2\lambda \|X_{k+1} - X_* \|^2 + r \|K z_{k}- Kz_{k+1}\|^2 \\
&& + \frac{1}{r}\|H_k - H_{k+1} \|^2,
\end{eqnarray*}
by making $\delta$ sufficiently small such that 
\begin{equation}
\begin{aligned}
   \delta \leq \frac{2\lambda}{ r + \frac{2L^2}{r}} \quad and \quad \delta \leq \frac{1}{2}.
\end{aligned}
\end{equation}
%To maximize $\delta$, one can choose $r = \sqrt{2} L$. Then $\delta \leq \frac{1}{\sqrt{2}} \frac{\lambda}{L}$. 
%\textcolor{red}{I guess this is not of our main interest here !}
This completes the proof. 
\end{proof}
\end{comment} 
%\begin{remark} 
%Since we have that with $\omega = \frac{2}{r + L_F + r + \lambda_F}$, 
%\begin{eqnarray*}
%\lambda_{max}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+L_F} = \frac{r + L_F - \omega}{r + L_F} \\  
%\lambda_{min}(I - \omega H_{A_r}) &=& 1 %- \frac{\omega}{r+\lambda_F} = \frac{r %+ \lambda_F - \omega}{r + \lambda_F}.
%\end{eqnarray*}
%Thus, we have that
%\begin{equation}
%\frac{\kappa(A_1) - 1}{\kappa(A_1) + 1} = \frac{ \frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} - 1}{\frac{L_F}{(r + L_F)}/\frac{\lambda_F}{(r + \lambda_F)} + 1} =\frac{ \frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} - 1}{\frac{r + \lambda_F}{r + L_F} \frac{L_F}{\lambda_F} + 1} = \frac{\frac{r + \lambda_F}{r + L_F}  - \frac{\lambda_F}{L_F}}{\frac{r + \lambda_F}{r + L_F} + \frac{\lambda_F}{L_F}}  
%\end{equation} 
%and 
%\begin{equation}
%\frac{\kappa(A_2) - 1}{\kappa(A_2) + 1} = \frac{ \frac{r + \lambda_F}{r + L_F} - 1}{\frac{r + \lambda_F}{r + L_F}  + 1}  
%\end{equation}
%On the other hand, we have that 
%\begin{equation}
%\sin (\theta_1 + \theta_2) = 
%\end{equation}
%and $\lambda_{min} = 1 - \frac{\omega}{r + \lambda_F} = \frac{\lambda_F}{r + \lambda_F}$ while $H_{A_r} = 1/(r + L_F)$ and $1/(r + \lambda_F)$. Therefore, if $r$ is sufficiently large, then $\sin\theta$ is quite small. 
%\begin{theorem}
%For $A_1$ and $A_2$, symmetric positive definite matrices, we have 
%\begin{equation}
%(x,A_1A_2y) \leq \sin (\theta_1 + %\theta_2) \|A_1x\|\|A_2y\|,  
%\end{equation}
%where $\sin \theta_1 = \frac{\kappa(A_1) - 1}{\kappa(A_2) + 1}$ and $\sin \theta_2 = \frac{\kappa(A_2) - 1}{\kappa(A_2) + 1}$. 
%\end{theorem}
%\end{remark}
\begin{remark}
We remark that it is more natural to write the convergence rate in terms of $\kappa(G^*)$, the condition number of $G^*$ since the choice of $\omega$ is made for solving the system relevant to $G^*$. However, it is also fine to use $\kappa(G)$ since it is more relvant to the problem to be solved and we have that
\begin{equation}
\kappa(G^*) = \kappa(G) \quad \rightarrow \quad 
\frac{\kappa(G) - 1}{\kappa(G) + 1} = \frac{\kappa(G^*) - 1}{\kappa(G^*) + 1}. 
\end{equation}
\end{remark} 

\subsection{Convergence analysis of Algorithm \ref{algADMM1} for $D_r^* \neq A_r^*$}\label{gdn} 

In this section, our goal is to establish the convergence of inexact Uzawa with Gauss-Seidel method, $D_r = A_r$, is replaced by inexact local solve for $X$-variable. This includes the $N-$step of Gradient Descent iteration. We recall that the Gauss-Seidel method can be interpreted as to solve the following optimization exactly in $X$-update \eqref{Xupdate} of the Algorithm \ref{algADMM1}:  
\begin{equation}
\min_{X} L_r(X,z_k,H_k).  
\end{equation}
We now let $G(X) = L_r(X,z_k,H_k)$, i.e.,  
\begin{equation}
G(X) = F(X) + \langle H_k, Kz_k - X\rangle + \frac{r}{2}\|Kz_k - X\|^2 
\end{equation}
and let $Y_*  = {\rm arg}\min_X G(X)$. Note that we reserve $X_{k+1}$ as the result of $N-$step of GD method. Then the following statements hold true: 
\begin{enumerate} 
\item $Y_* = A_r^*(H_k + rKz_k)$ 
\item $\nabla F(Y_*) + r Y_* = H_k + rKz_k$ 
\item $\nabla G(Y_*) = 0$. 
\end{enumerate} 
Note that it is easy to show that $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex. Our discussion is for general inexact local solve and the outcome will be denoted by $X_{k+1}$, i.e., 
\begin{equation}
X_{k+1} = D_r^*(H_k + rKz_k). 
\end{equation}
For the convergence analysis, the exact Gauss-Seidel case will be used as an intermediate step. Thus, naturally, the convergence estimate could be made to be sharper. We begin our discussion by introducing two quantities:
\begin{eqnarray*}
\textsf{E}_1 &:=& E_k^H - \omega \left \{A_r^{*}(H_* + rKz_*) - A_r^*(H_k + rKz_k) \right \}\\
\textsf{E}_2 &:=& A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k).  
\end{eqnarray*}
It is evident that $\textsf{E}_1$ is for the error of the Algorithm \ref{algADMM1} with $D_r^* = A_r^*$ and $\textsf{E}_2$ represents the difference between two iterates, one from the inexact solve for $X-$variable and the other from the exact solve. We shall now see that Lemma \ref{main:lem1} can lead to the following estimate easily. 
\begin{eqnarray*}
&& \left \|E_{k+1}^H \right \|^2 + \omega^2 \left \|E_{k+1}^Z \right \|^2 = \left \|E_k^H - \omega \{ A_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k) \} \right \|^2 \\
&& \qquad \leq \|\textsf{E}_1\|^2 + \omega^2 \|\textsf{E}_2\|^2 + 2 \omega \|\textsf{E}_1\| \|\textsf{E}_2\|. 
\end{eqnarray*}
The first term is relevant to the Algorithm \ref{algADMM1} with $D_r = A_r$. We shall assume that there exists $\delta < 1$ such that 
\begin{equation}\label{main:cvdelta}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ).
\end{equation}
Namely, the inexact solve provides a reasonably close solution to $Y_*$. Under this assumption, we shall establish the convergence of Algorithm \ref{algADMM1}. How small $\delta$ can be, for still allowing the convergence is determined by the close investigation of the convergence of Algorithm \ref{algADMM1} with exact local solve after incorporating $\delta$.   

We are now in a position to provide a main instrumental lemma in this section. 
\begin{theorem}\label{main:ins}
Let $\delta_{GS}$ be the convergence rate for the Algorithm \ref{algADMM1}, when $D_r = A_r$, as given in equation \eqref{gsrate}, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be such that the equation \eqref{main:cvdelta} hold. Then, we have the following estimate:   
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 \leq \left ( \delta_{GS} + \delta \right )^2 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{theorem} 
\begin{proof}
We shall let $E^2 = \|E_k^H\|^2 + \|E_k^Z\|_\omega^2$. Due to the equation \eqref{main:cvdelta}, we have that 
\begin{equation}
\|\textsf{E}_2\|_\omega^2 \leq \delta^2 E^2. 
\end{equation}
We then obtain that 
\begin{eqnarray*}
\left \|E_{k+1}^H \right \|^2 + \left \|E_{k+1}^Z \right \|_\omega^2 &\leq& \|\textsf{E}_1\|^2 + \|\textsf{E}_2\|_\omega^2 + 2 \|\textsf{E}_1\| \|\textsf{E}_2\|_\omega  \\ 
&=& \delta_{GS}^2 E^2 + \delta^2 E^2 + 2 \delta_{GS} \delta E^2 = (\delta + \delta_{GS})^2 E^2. 
\end{eqnarray*}
%Here we have invoked a simple Cauchy-Schwarz inequality that 
%\begin{equation*}
%ab \leq \frac{1}{2\varepsilon} a^2 + \frac{\varepsilon}{2} b^2, \quad \forall a, b\in \Reals{}.  
%\end{equation*}
%Finally, we observe that $\delta_{GS}$
This completes the proof. 
\end{proof}

We also note that the $N-$step of GD method is basically, given as follows: for a given step size $\gamma > 0$ and $X_k = Kz_k$, 
\begin{subeqnarray}\label{ngd1}
X_{k+\frac{1}{N}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} - \gamma \nabla G(X_{k+\frac{1}{N}}) \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
%X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
%X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} - \gamma \nabla G(X_{k + \frac{N-2}{N}}) \\
X_{k+\frac{N}{N}} &=& 
X_{k+\frac{N-1}{N}} - \gamma \nabla G(X_{k+\frac{N-1}{N}}), 
\end{subeqnarray}
where $\nabla G$ is given as follows: 
\begin{equation} 
\nabla G(X) = \nabla F(X) + rX - H_k - rKz_k. 
\end{equation}

We shall now present a simple but important lemma: 
\begin{lemma}\label{main:lm} 
Let $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ and $D_r^*(H_k + rKz_k)$ be the $N$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \leq  \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \|Y_* - X_k\|^2. 
\end{eqnarray*}
\end{lemma}
\begin{proof}
Let $Y_* = {\rm arg} \min_{X} G(X)$. On the other hand, $X_{k+1}$ is obtained by the following iteration: 
\begin{eqnarray*} 
X_{k+\frac{1}{N}} &=& X_{k} - \gamma \nabla G(X_k) \\
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} - \gamma \nabla G(X_{k+\frac{1}{N}}) \\ 
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} - \gamma \nabla G(X_{k + \frac{N-2}{N}}) \\
X_{k+\frac{N}{N}} &=& 
X_{k+\frac{N-1}{N}} - \gamma \nabla G(X_{k+\frac{N-1}{N}}).  
\end{eqnarray*}
Since $G$ is $r+L_F$ smooth and $r+\lambda_F$ strongly convex, we see that by Lemma \ref{lemmaGD}, we have that 
\begin{eqnarray*}
\|Y_* - X_{k+1}\|^2 &\leq& \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \|Y_* - X_k\|^2. 
\end{eqnarray*}
%
%&\leq& 2 \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \|Y_* - X_*\|^2 + \|X_* - X_k\|^2 \right ) \\
%&\leq& c \left ( \frac{\kappa(G)-1}{\kappa(G)+1} \right )^{2N} \left ( \frac{1}{r + \lambda_F} \left ( \|E_k^H\|^2 + \|E_k^Z\|^2 \right ) + \|X_* - X_k\|^2 \right ). 
%\end{eqnarray*} 
This completes the proof. 
\end{proof}
\begin{remark} 
The choice of $\gamma = \frac{2}{\lambda_{G} + L_{G}}$ is optimal for GD and the convergence rate can be calculated. Since $\kappa(G) = \frac{r + L_F}{r + \lambda_F}$, we have that 
\begin{equation}
\frac{\kappa(G)-1}{\kappa(G)+1} = \frac{L_F - \lambda_F}{2r + L_F + \lambda_F}. 
\end{equation}
The convergence is even faster for large $r$ and large $N$.
\end{remark}
We shall now see that the norm of $\textsf{E}_2$ can be made to be quite small. 
\begin{lemma}
Let $\gamma = \frac{2}{\lambda_G + L_G}$, $\omega = \frac{2}{\lambda_{G^*} + L_{G^*}}$ and $D_r^*(H_k + rKz_k)$ be the $N$-step GD, as given in the equation \eqref{ngd1}, then it holds true that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 \leq 4 \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega^2 \right ). 
\end{eqnarray*}
\end{lemma}
\begin{proof}
By Lemma \ref{main:lm}, we have that 
\begin{eqnarray*}
\|\textsf{E}_2\|_\omega^2 &=& \omega^2 \|A_r^*(H_k + rKz_k) - D_r^*(H_k + rKz_k)\|^2 \\
&\leq& \left ( \frac{\kappa(G) - 1}{\kappa(G) + 1} \right )^{2N} \omega^2 \|Y_* - X_k\|^2. 
\end{eqnarray*}
On the other hand, we have that since $X_k = Kz_k$ and due to Theorem \ref{thm:GS},  
\begin{eqnarray*}
\omega^2 \|Y_* - X_k\|^2 &\leq& 2 \omega^2 \|Y_* - X_*\|^2 + 2 \|X_* - X_k\|_\omega^2 \\ 
&\leq& \frac{2 \omega^2}{(r + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|X_* - X_k\|_\omega^2 \\
&\leq& \frac{8(r + L_F)^2}{(2r + L_F + \lambda_F)^2} \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) + 2 \|E_k^Z\|_\omega^2 \\ 
&\leq& 4 \left ( \|E_k^H\|^2 + \|E_k^Z\|_\omega ^2 \right ) \end{eqnarray*} 
The last inequality is due to the fact that $X_k = Kz_k$ and 
\begin{equation}
\omega = \frac{2(r + \lambda_F)(r + L_F)}{(2r + L_F + \lambda_F)},
\end{equation} 
thus, 
\begin{equation}
\frac{2 \omega^2}{(r + \lambda_F)^2} = \frac{8(r+L_F)^2}{(2r + L_F + \lambda_F)^2} \leq 2.  
\end{equation}
This completes the proof. 
\end{proof}
This result gives that if $N$ is large enough, then we can obtain the convergence of $N-$step GD based FL. 


%We shall compute %\begin{equation}
%I - \gamma A_r =^{???} \frac{\kappa(A_r) - 1}{\kappa(A_r)+1} I. 
%\end{equation}



%Therefore, we have that 
%\begin{eqnarray*}
%c_1(r) &=& \sup_{v = (X,z)} \frac{ \left \langle \overline{R}^{-1} \begin{pmatrix} X - r \gamma Kz \\ z \end{pmatrix},\begin{pmatrix} X - r \gamma Kz \\ z \end{pmatrix} \right \rangle }{(A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)} \\
%&=& \sup_{v = (X,z)} \frac{ \langle C (X - r \gamma Kz), (X - r \gamma Kz) \rangle + \langle rK^T K z, z \rangle }{(A X,X) + r (X,X) - 2r (Kz,X) + r(Kz,Kz)}, 
%\end{eqnarray*}
%where $C = [\gamma I + \gamma (I - \gamma A_r^{-1})]^{-1}$. We note that if $\gamma 
%\ll 1$, then 


\end{document} 

\section{Gauss-Seidel Solve for the Linear Case for two blocks $U = (X,z)$ and $H$} 

We recall that 
\begin{equation}
\mathcal{A}_r = \begin{pmatrix} 
A_0 + rI & - rK \\ -rK^T & rK^TK 
\end{pmatrix} 
\quad \mbox{ and } \quad 
\mathcal{B} = \begin{pmatrix} 
-I & K \end{pmatrix}. 
\end{equation} 
In this section, we shall consider the Gauss-Seidel solve for the matrix $ \nabla G = \mathcal{A}_r$. We write an equivalent form of the above equation using the Schur complement system. We shall consider applying the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
\mathcal{L} = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad \psi = \mathcal{L}^{-1} = \begin{pmatrix}
A_r^{-1} & 0\\
(rK^T K)^{-1} rK^T A_r^{-1} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + \mathcal{L}^{-1} (f - B^T H_k - \mathcal{A}_r U_k) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that the exact solutions satiafy \begin{eqnarray}
U_{*} &=& U_{*} + \mathcal{L}^{-1} (f - B^T H_{*} - \mathcal{A}_r U_{*}) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray}
Thus, with the convention that $E_{k}^U = U_* - U_k$ and $E_{k}^H = H_* - H_k$, we obtain the error equations: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{L}^{-1} (- \mathcal{A}_r E^U_k - B^T E^H_k) \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray}
In the other direction, we have that 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + \mathcal{L}^{-1} (- \mathcal{A}_r  E^U_k - B^T E^H_k) \\
          &=& E^U_k - \mathcal{L}^{-1} \mathcal{A}_r E^U_k - \mathcal{L}^{-1} B^T E^H_k \\
          &=& (I - \mathcal{L}^{-1} \mathcal{A}_r) E^U_k - \mathcal{L}^{-1} B^T E_k^H \\ 
E^H_{k+1} &=& E^H_k + \omega B ((I - \mathcal{L}^{-1} \mathcal{A}_r) E^U_k - \mathcal{L}^{-1} B^T E_k^H ) \\
&=& (I - \omega B \mathcal{L}^{-1} B^T) E_k^H + \omega B (I - \mathcal{L}^{-1} \mathcal{A}_r)(E_k^U). 
\end{eqnarray} 
In the matrix form, we have 
\begin{equation}
\begin{pmatrix} 
I  & 0 \\
-\omega B & I
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k+1} \\
E^H_{k+1}
\end{pmatrix}  = 
\begin{pmatrix} 
I - \mathcal{L}^{-1} \mathcal{A}_r & - \mathcal{L}^{-1} B^T \\
0 & I
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k} \\
E^H_{k}
\end{pmatrix} 
\end{equation}
Or, we have 
\begin{equation}
\begin{pmatrix} 
E^U_{k+1} \\
E^H_{k+1}
\end{pmatrix}  = 
\begin{pmatrix} 
I - \mathcal{L}^{-1} \mathcal{A}_r & - \mathcal{L}^{-1} B^T \\
\omega B (I - \mathcal{L}^{-1} \mathcal{A}_r) & I - \omega B \mathcal{L}^{-1} B^T
\end{pmatrix} 
\begin{pmatrix} 
E^U_{k} \\
E^H_{k}
\end{pmatrix}.  
\end{equation}

We shall denote $Q_B = 1/\omega$ and noticed that this expression is very similar to what Bramble has in his paper. Following Bramble, we shall assume there are two contraction paratmers, $\delta$ and $\gamma$ which are defined by the following inequality:
\begin{subeqnarray}
\|(\mathcal{A}_r^{-1} - \psi)(\phi)\|_{\mathcal{A}_r} \leq \delta \|\phi\|_{\mathcal{A}_r^{-1}} \\ 
\|(I - \omega \mathcal{S}_r)v\|_{Q_B} \leq \gamma \|v\|_{Q_B}. 
\end{subeqnarray}
Namely, $\delta$ is the convergence rate for Gauss-Seidel method for example and $\gamma$ is the convergence rate for the Schur complement solve. We assume that $\delta$ satisfies the following:
\begin{equation}
\delta < \frac{1-\gamma}{3-\gamma}. 
\end{equation} 
Bramble showed the following. We see that the following holds: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k - \psi (\mathcal{A}_r  E^U_k + B^T E^H_k) \\
          &=& (\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E^U_k + B^T E_k^H) - \mathcal{A}_r^{-1} B^T E^H_k \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray} 
By taking $\mathcal{A}_r$ norm, we arrive at 
\begin{eqnarray}
\|E^U_{k+1}\|_{\mathcal{A}_r} &=& \|(\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E^U_k + B^T E_k^H) - \mathcal{A}_r^{-1} B^T E^H_k \|_{\mathcal{A}_r}. 
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray} 
This gives that 
\begin{eqnarray}
\|E^U_{k+1}\|_{\mathcal{A}_r} &\leq& \delta \|E_k^U\|_{\mathcal{A}_r} + (1 + \delta) \|E_k^H\|_{Q_B}. 
\end{eqnarray}
We note that 
\begin{eqnarray}
E^H_{k+1} &=& (I - \omega B \psi B^T) E_k^H + \omega B (I - \psi \mathcal{A}_r)(E_k^U) \\ 
&=& (I - \omega B \mathcal{A}_r^{-1} B^T) E_k^H + \omega B (\mathcal{A}_r^{-1} - \psi)(\mathcal{A}_r E_k^U + B^T E_k^H)  
\end{eqnarray} 
Thus we have that 
\begin{eqnarray}
\|E^H_{k+1}\|_{Q_B} &\leq& (\gamma + \delta) \|E_k^H\|_{Q_B} + \delta \|E_k^U\|_{\mathcal{A}_r}. 
\end{eqnarray} 
Therefore, we have that in a matrix form: 
\begin{equation}
\begin{pmatrix} 
\|E^U_{k+1}\|_{\mathcal{A}_r} \\
\|E^H_{k+1}\|_{Q_B}
\end{pmatrix} \leq M^{k+1}  
\begin{pmatrix} 
\|E^U_{0}\|_{\mathcal{A}_r} \\
\|E^H_{0}\|_{Q_B}
\end{pmatrix}  
\end{equation}
Here 
\begin{equation}
M = \begin{pmatrix} 
\delta & 1 + \delta \\ \delta & \gamma + \delta 
\end{pmatrix}. 
\end{equation}
This matrix is symmetric with respect to the inner product defined as follows: 
\begin{equation}
\left [ \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, \begin{pmatrix}  x_2 \\ y_2 \end{pmatrix} \right ] = \frac{\delta}{1+\delta} x_1 x_2 + y_1 y_2. 
\end{equation} 
Since $M$ is symmetric, its norm is bounded by its spectral radius and it is roots of 
\begin{equation}
\lambda^2 - (2\delta + \gamma)\lambda - \delta (1 - \gamma) = 0. 
\end{equation} 
It is observed that the spectral radius $\rho$ is an increasing function of $\delta$ for any fixed $\gamma \in [0,1]$. Moreover $\rho = 1$ for $\delta = (1-\gamma)/(3-\gamma)$. The convergence factor is then given by 
\begin{equation}
\rho = \frac{2\delta + \gamma + \sqrt{ (2\delta + \gamma)^2 + 4\delta(1-\gamma)}}{2}. 
\end{equation} 
Note that here $\gamma$ is the convergence rate for Richardson while $\delta$ is the convergence rate for Gauss-Seidel method. The convergence estimate is given as follows: 
\begin{eqnarray*}
\left ( \frac{\delta}{1 + \delta} (\mathcal{A}_r E_k^U, E_k^U) + (Q_B E_k^H, E_k^H) \right ) \leq \rho^{2k} \left ( \frac{\delta}{1 + \delta} (\mathcal{A}_r E_k^U, E_k^U) + (Q_B E_k^H, E_k^H) \right ). 
\end{eqnarray*}

\end{document} 

% Directly taking $A-$norm: 
% \begin{eqnarray}
% \| E^U_{k+1} \|_A &\leq& \| I - L^{-1} \nabla G \| \|E^U_k\|_A  + \| L^{-1} B^T \|_A \|E^H_k\|_A \\
% \| E^H_{k+1}\| &\leq& \|I - \omega B L^{-1} B^T \| \|E_k^H\| + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
% \end{eqnarray}

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

\begin{equation}
    \|M \|_A \leq \sqrt{ \kappa(A)} \| M \|_2
\end{equation}
We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \lambda_{max}(A) \| L^{-1} B^T H\| \\
   & =\lambda_{max}(A) (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}
We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}
\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}
\textcolor{red}{ Here !} 

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}

%\end{document} 

\section{Appendix} 

\begin{lemma} 
The GD three step can be shown to be convergent linearly if one chooses $\gamma = \frac{1}{r}$ with $\omega = r$, sufficiently small enough, then it converges linearly.
\end{lemma} 
\begin{proof} 
For GD three step case, we have that with $N = 3$: 
\begin{eqnarray*}
D_r^*(H_k + rKz_*) = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}
We also note that  
\begin{equation*}
D_r^*(H_k + rKz_k) = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{equation*}
Let us define 
\begin{eqnarray*}
D_{H_*,H_k}^{Z_*} &:=& D_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*) \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*) + \gamma H_* \\
&& \qquad - ((I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma H_k)  \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*) \\
&& \qquad - (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k) + \gamma (H_* - H_k) \\ 
&=& (I - \gamma A)(\xi_1) ((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - ((I - \gamma A)((I - \gamma A)(X_*)) + \gamma H_k)) \\
&& \qquad + \gamma (I + (I - \gamma A)(\xi_1)) (H_* - H_k) \\
&=& \gamma [(I - \gamma A)(\xi_1)(I - \gamma A)(\xi_2) + I + (I - \gamma A)(\xi_1))] (H_* - H_k) \\
&=& \gamma [I + A_\gamma (\xi_1) + A_\gamma(\xi_1) A_\gamma(\xi_2)] (H_* - H_k). 
\end{eqnarray*}
\begin{eqnarray*}
D_{Z_*,Z_k}^{H_k} &=& D_r^*(H_k + rKz_*) - D_r^*(H_k + rKz_k) \\
&=& (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) + \gamma H_k)) \\
&& \qquad - (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k)) \\ 
&=& A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k). 
\end{eqnarray*}
We now observe the following estimate holds: 
\begin{eqnarray*}
&& \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (D_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 \\ 
&& \quad = \|H_* - H_k - \omega (D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k})\|^2,  \\
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k} \rangle + \omega^2 \langle  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k},  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k}\rangle \\ 
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} \rangle + \omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{H_*,H_k}^{Z_*}\rangle \\ 
&& \qquad - 2 \omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2 \omega^2 \langle  
D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k} \rangle \\
&& \qquad + \omega^2 \langle D_{Z_*,Z_k}^{H_k},  D_{Z_*,Z_k}^{H_k} \rangle. 
\end{eqnarray*}
On the other hand, we see that 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad = -2\omega \langle H_* - H_k, A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle \\ 
&& \qquad + 2 \omega^2 \langle \gamma (I + A_\gamma (\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2)) (H_* - H_k), A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k)  \rangle \\
&& \qquad = - 2\omega \langle (I - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))(H_* - H_k), A_{\eta_1} A_{\eta_2} A_{\eta_3} (X_*  - X_k) \rangle. 
%&& \qquad \leq 2\omega|\omega\gamma - 1| \sin^2 \theta (1 - \gamma \lambda)^2 \|H_* - H_k\| \|X_* - X_k\| \\
%&& \qquad + 2\omega^2 \gamma (1 - \gamma \lambda)^3 \sin^3 \theta \|H_* - H_k\|\|X_* - X_k\|.   
\end{eqnarray*}
Therefore, we have that with $\alpha + \beta = 1$, 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad \leq (1 - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))^{2\alpha} \rho_1^3 \|H_* - H_k\|^2 \\
&& + (1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2))^{2\beta} \rho_1^3 \|X_* - X_k\|_\omega^2. 
\end{eqnarray*}
On the other hand, we have 
\begin{eqnarray*}
\|H_* - H_k - \omega D_{H_*,H_k}^{Z_*} \|^2 &\leq& 
\left (1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2} \|H_* - H_k\|^2 \\ 
\|D_{Z_*,Z_k}^{H_k}\|^2 &=& (1 - \gamma \lambda)^6 \|X_* - X_k\|^2.   
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} \leq \rho_{E^H} \|H_* - H_k\|^2  \\
&& \qquad + \rho_{E^X} \|X_* - X_k\|_{\omega}^2, 
\end{eqnarray*}
where 
\begin{eqnarray*}
\rho_{E^H} &=& \left ( 1 + \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2\alpha} \rho^3 \right ) \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2} \\
\rho_{E^X} &=& \left ( 1 + \left ( 1 - \omega \gamma (I +  A_\gamma(\xi_1) + A_\gamma (\xi_1)A_\gamma(\xi_2) \right )^{2\beta} \rho^3 \right ) (1 - \gamma \lambda)^6. 
\end{eqnarray*}
We now choose $\alpha = 0$ and $\beta = 1$. Further, we choose $\omega = 1/(3\gamma)$. Then, we see that 
\begin{eqnarray*}
I - \omega \gamma (I + A_\gamma(\xi_1) + A_\gamma (\xi_1) A_\gamma(\xi_2)) &=& I - (I + (I - \gamma A(\xi_1) + (I - \gamma A(\xi_1))(I - \gamma A(\xi_2))))/3  \\
&=& [2\gamma A(\xi_1) + \gamma A(\xi_2) - \gamma^2 A(\xi_1) A(\xi_2)]/3 = \gamma + O(\gamma^2). 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
\rho_{E^H} &=& \left ( 1 + (\gamma + O(\gamma^2))^{2\alpha} \rho^3 \right )  O(\gamma^2) \\
\rho_{E^X} &=& \left ( 1 + (\gamma + O(\gamma^2))^{2\beta} \rho^3 \right ) (1 - \gamma \lambda)^6. 
\end{eqnarray*}
We note that with $r_1 \rightarrow r - \lambda$, we have 
\begin{equation} 
1 - \gamma \lambda = 1 - \frac{\lambda}{r} = \frac{r - \lambda}{r} = \frac{r_1}{r_1 + \lambda}. 
\end{equation} 
We note that if $r$ is sufficiently large, then we can make sure these two parameters smaller than one. Thus, we have the linear convergence.  This completes the proof. 
\end{proof}


We now present the main theorem in this subsection. 
\begin{lemma} 
The GD two step can be shown to be convergent linearly if one chooses $\gamma = \frac{1}{r}$ with $\omega = r$, sufficiently small enough, then it converges linearly.
\end{lemma} 
\begin{proof} 
For GD two step case, we have that with $N = 2$
\begin{eqnarray*}
D_r^*(H_k + rKz_*) = (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}
We also note that  
\begin{equation*}
D_r^*(H_k + rKz_k) = (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k.  
\end{equation*}
Let us define 
\begin{eqnarray*}
D_{H_*,H_k}^{Z_*} &:=& D_r^{*} (H_* + rK z_*) - D_r^*(H_k + rKz_*) \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_* - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma H_k)  \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k) + \gamma (H_* - H_k) \\ 
&=& (I - \gamma A)(\xi_1) ( (I - \gamma A)(X_*) + \gamma H_* - (I - \gamma A)(X_*) - \gamma H_k ) + \gamma (H_* - H_k) \\ 
&=& \gamma (I - \gamma A)(\xi_1)(H_* - H_k) + \gamma (H_* - H_k) = \gamma A_{\gamma_1} (H_* - H_k) + \gamma (H_* - H_k) \\
&=& \gamma (I +  A_\gamma(\xi_1)) (H_* - H_k). 
\end{eqnarray*}
and 
\begin{eqnarray*}
D_{Z_*,Z_k}^{H_k} &=& D_r^*(H_k + rKz_*) - D_r^*(H_k + rKz_k) \\
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) + \gamma H_k) - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) \\ 
&=& (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_k)) - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) \\ 
&=& A_{\eta_1} ((I - \gamma A)(X_*) - (I - \gamma A)(X_k)) = A_{\eta_1} A_{\eta_2} (X_*  - X_k). 
\end{eqnarray*}
We now observe the following estimate holds: 
\begin{eqnarray*}
&& \|H_* - H_{k+1}\|^2 + \omega^2 \|Kz_{*} - Kz_{k+1}\|^2 = \|H_* - H_k - \omega (D_r^{*} (H_* + r K z_*) - D_r^{*} (H_k + r K z_k))\|^2 \\ 
&& \quad = \|H_* - H_k - \omega (D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k})\|^2,  \\
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k} \rangle + \omega^2 \langle  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k},  D_{H_*,H_k}^{Z_*} + D_{Z_*,Z_k}^{H_k}\rangle \\ 
&& \quad = \langle H_* - H_k, H_* - H_k \rangle - 2 \omega \langle H_* - H_k, D_{H_*,H_k}^{Z_*} \rangle + \omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{H_*,H_k}^{Z_*}\rangle \\ 
&& \qquad - 2 \omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2 \omega^2 \langle  
D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k} \rangle \\
&& \qquad + \omega^2 \langle D_{Z_*,Z_k}^{H_k},  D_{Z_*,Z_k}^{H_k} \rangle. 
\end{eqnarray*}
On the other hand, we see that 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad = -2\omega \langle H_* - H_k, A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle \\ 
&& \qquad + 2 \omega^2 \langle \gamma (I + A_\gamma (\xi_1)) (H_* - H_k), A_{\eta_1} A_{\eta_2} (X_*  - X_k)  \rangle \\
&& \qquad = - 2\omega \langle (I - \omega \gamma (I + A_\gamma(\xi_1))(H_* - H_k), A_{\eta_1} A_{\eta_2} (X_*  - X_k) \rangle. 
%&& \qquad \leq 2\omega|\omega\gamma - 1| \sin^2 \theta (1 - \gamma \lambda)^2 \|H_* - H_k\| \|X_* - X_k\| \\
%&& \qquad + 2\omega^2 \gamma (1 - \gamma \lambda)^3 \sin^3 \theta \|H_* - H_k\|\|X_* - X_k\|.   
\end{eqnarray*}
Therefore, we have that with $\alpha + \beta = 1$, 
\begin{eqnarray*}
&& -2\omega \langle H_* - H_k, D_{Z_*,Z_k}^{H_k} \rangle + 2\omega^2 \langle D_{H_*,H_k}^{Z_*}, D_{Z_*,Z_k}^{H_k}  \rangle \\
&& \qquad \leq (1 - \omega \gamma (I + A_\gamma(\xi_1))^{2\alpha} \rho_1^2 \sin 3 \theta \|H_* - H_k\|^2 + (1 - \omega \gamma (I +  A_\gamma(\xi_1))^{2\beta} \rho_1^2 \sin 3 \theta \|X_* - X_k\|_\omega^2. 
\end{eqnarray*}
On the other hand, we have 
\begin{eqnarray*}
\|H_* - H_k - \omega D_{H_*,H_k}^{Z_*} \|^2 &\leq& 
(1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \|H_* - H_k\|^2 \\ 
\|D_{Z_*,Z_k}^{H_k}\|^2 &=& (1 - \gamma \lambda)^4 \|X_* - X_k\|^2.  \\ 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\|^2 + \|Kz_* - Kz_{k+1}\|^2_{\omega} \\
&& \qquad \leq \left ( 1 + (1 - \omega \gamma (I + A_\gamma(\xi_1)))^{2\alpha}\rho^2 \sin 3\theta \right ) (1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \|H_* - H_k\|^2  \\
&& \qquad + \left (1 + (1 - \omega\gamma (I + A_\gamma(\xi_1))^{2\beta} \rho^2 \sin 3\theta \right ) (1 - \gamma \lambda)^4  \|X_* - X_k\|_{\omega}^2. 
%&& + \left ( (1 - \gamma \lambda)^4 + \rho^{2\beta_2} \sin^{2\mu_2} \theta \right ) \|Kz_* - Kz_k\|_\omega^2 \\ 
%&=& \left ( 1 + \sin^{2\mu_1} \right ) (1 - \gamma \lambda)^2  \|H_* - H_k\|^2 \\
%&& + \left ( 1 + \sin^{2\mu_2} \right ) (1 - \gamma \lambda)^4 \|Kz_* - Kz_k\|_\omega^2
\end{eqnarray*}
We note that with $r_1 \rightarrow r - \lambda$, we have that $\rho$ behaves like 
\begin{equation} 
1 - \gamma \lambda = 1 - \frac{\lambda}{r} = \frac{r - \lambda}{r} = \frac{r_1}{r_1 + \lambda}.  
\end{equation} 
On the other hand, with the choice of $\omega = 1/(2\gamma)$, we have that for sufficiently larger $r$, 
\begin{eqnarray*}
\sigma(I - \omega \gamma (I + A_\gamma(\xi_1))) &=& \sigma (I - (I + A_\gamma(\xi_1))/2) \\
&=& \sigma (I - (I + I - \gamma A(\xi_1))/2) = \sigma(\gamma A(\xi_2)/2) = \frac{\gamma}{2} L.    
\end{eqnarray*}
Thus, if we choose $\alpha = 0$ and $\beta = 1$, then we see that 
\begin{eqnarray*}
&& \left ( 1 + (1 - \omega \gamma (I + A_\gamma(\xi_1)))^{2\alpha}\rho^2 \right ) (1 - \omega \gamma (I + A_\gamma(\xi_1)))^2 \\
&& = \left ( 1 + \rho^2 \right ) \frac{\gamma^2 L^2}{4} \\
&& \left (1 + (1 - \omega\gamma (I + A_\gamma(\xi_1))^{2\beta} \rho^2 \right ) (1 - \gamma \lambda)^4 = \left ( 1 + \left ( \frac{\gamma L}{2} \right )^{2\beta}\rho^2 \right ) = \left ( 1 + \frac{c}{r^2} \right ) \left (\frac{r}{r + \lambda} \right )^4 \\ 
&& < 1 \Longleftrightarrow r^4 + c r^2 < (r + \lambda)^4. 
\end{eqnarray*}
This completes the proof. 
\end{proof}

We notice that the following monotonicity holds for $F_N$. 
\begin{lemma}
The function $F_N(X,H) : \Reals{d} \mapsto \Reals{d}$, defined by 
\begin{equation}
F_N(X,H) := \underbrace{W(\cdots (W(W}_{\textsf{N} \, \mbox{times}}(X, \underbrace{H), \cdots H)),H)}_{\textsf{N}\, \mbox{times}}
\end{equation}
is strongly monotone in the second variable. Namely, we have that for some $\lambda_N > 0$, which does not degenerate in $N$,  
\begin{equation}
\langle H_1 - H_2, F_N(X,H_1) - F_N(X,H_2) \rangle \geq \lambda_N \|H_1 - H_2\|^2, \quad \forall H_1, H_2 \in \Reals{d}.
\end{equation}
More precisely, we have that 
\begin{equation} 
\lambda_N = \gamma + (1 - \gamma L) \lambda_{N-1} = \gamma \sum_{\ell=1}^N (1 - \gamma L)^{\ell-1}.  
\end{equation} 
with $\lambda_1 = \gamma$. Furthermore, $F_N$ is uniformly continuous in the second variable in the following sense that
\begin{equation}
\|F_N(X,H_1) - F_N(X,H_2)\| \leq L_{N} \|H_1 - H_2\|,  
\end{equation}
where 
\begin{equation} 
L_{N} = \gamma \sum_{\ell = 1}^{N} (1 - \gamma \lambda)^{\ell-1}. 
\end{equation} 
\end{lemma}
\begin{proof}
We prove it by induction arguement. For $N = 1$, we have that 
\begin{eqnarray*}
\langle H_1 - H_2, F_1(X,H_1) - F_1(X,H_2) \rangle &=& \langle H_1 - H_2, W(X,H_1) - W(X,H_2) \rangle \\ 
&=& \langle H_1 - H_2, [(I - \gamma A)(X)  + \gamma H_1] - [(I - \gamma A)(X) + \gamma H_2] \rangle \\
&=& \langle H_1 - H_2, \gamma (H_1 - H_2) \rangle = \gamma \|H_1 - H_2\|^2. 
\end{eqnarray*}
Here $\lambda_1 = \gamma$. Now, we assume that there exists $\lambda_{N-1}$ such that 
\begin{eqnarray*}
\langle H_1 - H_2, F_{N-1}(X,H_1) - F_{N-1}(X,H_2) \rangle \geq r_{N-1} \|H_1 - H_2\|^2. 
\end{eqnarray*}
We then consider the following: 
\begin{eqnarray*}
&& \langle H_1 - H_2, F_N(X,H_1) - F_N(X,H_2) \rangle \\
&& \qquad = \langle H_1 - H_2, [W(F_{N-1}(X,H_1)) + \gamma H_1] - [W(F_{N-1}(X,H_2)) + \gamma H_2] \rangle = \gamma \|H_1 - H_2\|^2 \\
&& \qquad \geq \gamma \|H_1 - H_2\|^2 + (1 - \gamma L ) \langle H_1 - H_2, F_{N-1}(X,H_1) - F_{N-1}(X,H_2) \rangle \\
&& \qquad = (\gamma + (1 - \gamma L) \lambda_{N-1}) \|H_1 - H_2\|^2 = \lambda_N \|H_1 - H_2\|^2. 
\end{eqnarray*}
This completes the proof. 
\end{proof}


\subsection{Inexact Gauss-Seidel with a fixed number of GD steps for $X$ block} 
For given $(z_k, H_k)$, we consider the solution to the following equation: 
\begin{equation}
\nabla F(X) + r X - H_k - r Kz_k = 0. 
\end{equation}
This has been characterized as $X_{k+1}$ in the block Gauss-Seidel method in the previous section. However, we are interested in solving it using the Gradient Descent method. We notice that the corresponding functional is given as follows: 
\begin{equation} 
G(X) = F(X) - \langle H_k, X \rangle + \frac{r}{2} \|Kz_k - X\|^2. 
\end{equation} 
The optimality condition is that 
\begin{equation} 
0 = \nabla G(X_{k+1}) = \nabla F(X) - H_k -r Kz_k + rX. 
\end{equation}
Namely, we apply the gradient descent by the following procedure: 
\begin{equation} 
X_{k+1} = X_k - \gamma \nabla G(X_k) = X_k - \gamma (\nabla F(X_k) - H_k - rKz_k + rX_k). 
\end{equation} 
If we set $X_k = Kz_k$, then we have that
\begin{eqnarray*} 
X_{k+1} &=& X_k - \gamma \nabla G(X_k) = X_k - \gamma (\nabla F(X_k) - H_k - rKz_k + rX_k) \\
&=& Kz_k - \gamma(\nabla F(X_k) - H_k).   
\end{eqnarray*}
This is the choice of algorithm introduced in \cite{mishchenko2022proxskip}.  
We shall consider to apply the inexact block Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
N = \begin{pmatrix}
D_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad N^{*} = \begin{pmatrix}
D_r^{*} & 0\\
(r K^T K)^{-1} r K^T D_r^{*} & r^{-1} (K^T K)^{-1} 
\end{pmatrix}.  
\end{equation}
The inexact Uzawa iteration can then be defined as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + N^{*} (-B^T H_k - \nabla G(U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that 
\begin{equation}
-B^TH_k - \nabla G(U_k) = \begin{pmatrix} H_k - A_r(X_k) + rK z_k \\ rK^T X_k - r K^T K z_k \end{pmatrix} 
\end{equation}
and thus 
\begin{eqnarray*}
&& N^{*}(-B^TH_k - \nabla G(U_k)) = N^{*} \begin{pmatrix} H_k - A_r(X_k) + rK z_k \\ rK^T X_k - r K^T K z_k \end{pmatrix} \\
&& \quad = 
\begin{pmatrix} D_r^{*} (H_k - A_r(X_k) + rK z_k) \\ (K^TK)^{-1} K^T D_r^{*} (H_k - A_r(X_k) + rK z_k) + (K^T K)^{-1}K^T X_k - z_k \end{pmatrix}
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
X_{k+1} &=& X_k + D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
z_{k+1} &=& (K^TK)^{-1} K^T (X_{k} + D_r^{*} (H_k - A_r(X_k) + r K z_k)) \\ 
H_{k+1} &=& H_k + \omega B U_{k+1} = H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
We shall apply $K$ to $z$ equation to obtain: 
\begin{eqnarray*}
X_{k+1} &=& X_k + D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
Kz_{k+1} &=& P_Z (X_{k} + D_r^{*} (H_k - A_r(X_k) + r K z_k)) \\ 
H_{k+1} &=& H_k + \omega B U_{k+1} = H_k + \omega (-X_{k+1} + Kz_{k+1} )
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
X_{*} &=& X_* + D_r^{*} (H_* - A_r(X_*) + r K z_*) \\
Kz_{*} &=& P_Z (X_* + D_r^{*} (H_* - A_r(X_*) + rK z_*)) \\ 
H_{*} &=& H_* + \omega (-X_{*} + K z_{*}). 
%&=& H_* + \omega \left ( - \left [ X_* + A_r^{-1} (H_* - A_r(X_*)+ r K z_*) \right ] \right .\\
%&& + \left . \left [ Kz_k + K(K^TK)^{-1} K^T A_r^{-1} (H_k - A_r(X_k) + rK z_k) + K(K^T K)^{-1}K^T X_k - K z_k \right ] \right ) \\ 
\end{eqnarray*}
Therefore, we have the following error equation: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_k + D_r^{*} (H_* - A_r(X_*) + r K z_*) - D_r^{*} (H_k - A_r(X_k) + r K z_k) \\
Kz_{*} - Kz_{k+1} &=& P_Z \left \{ X_* - X_k + D_r^{*} (H_* - A_r(X_*) + rK z_*) - D_r^{*} (H_k - A_r(X_k) + rK z_k) \right \} \\
H_{*} - H_{k+1} &=& H_* - H_k + \omega (-(X_{*} - X_{k+1}) + K(z_{*} - z_{k+1})) \\ 
&=& H_* - H_k - \omega (X_{*} - X_{k+1}) + \omega K( z_{*} - z_{k+1}). 
\end{eqnarray*}

\subsubsection{The convergence analysis of a single step GD} In this section, we study the convergence analysis of the single step GD for the first block, i.e., $D_r^{-1} = \gamma$. We note that the main convergence analytical tool introduced in \cite{mishchenko2022proxskip} was the nonexpansiveness of the proximal operator. In this case, the error equation is given as follows: 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_k + \gamma \left [(H_* - A_r(X_*) + r K z_*) - (H_k - A_r(X_k) + r K z_k) \right ] \\
&=& X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \\ 
Kz_{*} - Kz_{k+1} &=& P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \\
H_{*} - H_{k+1} &=& H_* - H_k + \omega (-(X_{*} - X_{k+1}) + K ( z_{*} - z_{k+1})). 
\end{eqnarray*}
The main convergence of interest lies in $z_k$ to $z_*$ and $H_k$ to $H_*$. Therefore, we shall not be much interested in the convergence of $X_k$ to $X_*$. This means that we are more or less interested in the convergence of $X_k$ in $P_Z$ norm, i.e., 
\begin{equation*}
\|E_k^X\|_{P_Z}^2 := \langle P_Z E_k^X, E_k^X \rangle \rightarrow 0 \quad \mbox{ as } \quad k \rightarrow \infty.
\end{equation*}
But, it is nothing else than the convergence of $z_k$ to $z_*$. Our plan is to establish the convergence of $H_k$ to $H_*$ and $z_k$ to $z_*$ and then use it to establish the convergence of $X_k$ to $X_*$. We first note that the following error bound holds. Under the condition on $\gamma$ given as follows: 
\begin{equation}
0 \leq \gamma \leq \frac{1}{L}, 
\end{equation}
Again, the trick is to rewrite $E_{k+1}^Z$ in a somewhat different form as follows: 
\begin{eqnarray*}
-\omega(Kz_{*} - Kz_{k+1}) = -\omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ).
\end{eqnarray*}
we have that 
\begin{eqnarray*}
\omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ) \| \\ 
&=& \|P_Z (H_* - H_k) - \omega \left ( P_Z \left \{X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \right \} \right ) \| \\
&=& \|P_Z (H_* - H_k) - \omega \left ( P_Z \left \{(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k) \right \} \right ) \| \\ 
&=& \|P_Z [(H_* - H_k) - \omega \left \{(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k) \right \}] \|. 
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|H_* - H_k - \omega Q_Z [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
&=& \|Q_Z \left \{ H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \right \} \| \\
&=& \|Q_Z \left \{ H_* - H_k - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma [H_* - H_k]] \right \} \| \\
\end{eqnarray*}
The main idea is not to use $A$, but to use the convexity of the functional for sufficiently small $\gamma > 0$:  
\begin{equation} 
V(x) = \frac{1}{2} \|x\|^2 - \gamma F(x).
\end{equation}
Therefore, we have for $\gamma \leq 1/L$, 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \omega \|Kz_* - Kz_{k+1}\| &\leq& \|H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
&\leq& |1 - \omega\gamma|\|H_* - H_k\| + \omega \|(I - \gamma A)(X_*) - (I - \gamma A)(X_k)\| \\
&\leq& |1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda) \omega \|X_* - X_k\| \\
&=& |1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda)  \|Kz_* - Kz_k\|_\omega. 
\end{eqnarray*}
\begin{remark}
The standard analysis presented in \cite{mishchenko2022proxskip} has used the fact that 
\begin{eqnarray*}
\|X_* - X_k - \gamma (A(X_*) - A(X_k)) \| \leq (1 - \gamma \lambda) \|X_* - X_k\|.
\end{eqnarray*}
This is indeed equivalent to the analysis presented in this paper. However, such a view can not be extended to multiple step GD case as will be discussed below. 
\end{remark}
This concludes that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq 
|1 - \omega\gamma|\|H_* - H_k\| + (1 - \gamma \lambda) \|Kz_* - Kz_k\|_\omega. 
%&\leq& \|H_* - H_k - \omega [X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))]] \| \\
%&\leq& \| (I - \omega\gamma) (H_* - H_k) - \omega \{ X_* - X_k - \gamma (A(X_*) - A(X_k)) \} \| \\
%&\leq& {\rm abs}(1 - \omega \gamma) \|H_* - H_k\| + \omega \|X_* - X_k - \gamma (A(X_*) - A(X_k)) \| \\
%&\leq& {\rm abs}(1 - \omega \gamma) \|H_* - H_k\| + \omega (1 - \gamma \lambda)\|Kz_* - Kz_k\|. 
\end{eqnarray*}
Let us simply choose $r = \omega$ as has been done in \cite{mishchenko2022proxskip}. Then, we choose $r$ and $\gamma$ so that 
\begin{eqnarray*}
|1 - \omega \gamma| < 1 \quad \mbox{ or } \quad -1 < 1 - \omega \gamma < 1.  
\end{eqnarray*}
Furthermore, we can choose $\gamma$ so that 
\begin{equation}
1 - c_0 = \max \{ |1 - \omega \gamma|, 1 - \gamma \lambda \}. 
\end{equation}
Then, we have the linear convergence. Namely, there exists $c_0$ such that 
\begin{equation}
[\|Kz_k - Kz_*\|_\omega + \|H_k - H_*\|] \leq (1 - c_0) [ \|Kz_{k-1} - Kz_*\|_\omega + \|H_{k-1} - H_*\| ], \quad \forall k = 1,\cdots. 
\end{equation}
Lastly, we can obtain the convergence of $X_k$ to $X_*$ from this result: 
\begin{eqnarray*}
\|X_{*} - X_{k+1}\| &=& \|X_* - X_k + \gamma [(H_* - A(X_*)) - (H_k - A(X_k))] \| \\
&\leq& \|X_* - X_k - \gamma (A(X_*) - A(X_k)) + \gamma (H_* - H_k) \| \\ 
&\leq& \|X_* - X_k - \gamma (A(X_*) - A(X_k))\| + \gamma \|H_* - H_k\| \\
&\leq& (1 - \gamma \lambda) \|X_* - X_k\| + \gamma \|H_* - H_k\|.
\end{eqnarray*}
Therefore, we can show that $\|X_* - X_{k}\|$ also converges.  This completes the proof. 

\subsubsection{The convergence analysis for nonlinear solver}
We now look at the general case for $D_r$. As presented in \cite{mishchenko2022proxskip}, we shall set $X_k = Kz_k$ when we begin the GD step for the variable $X$. This includes a total of $N$-step GD for finding zero of the following nonlinear equation for a given $z_k$ and $H_k$:
\begin{equation} 
\nabla F(X) + r X = rKz_k + H_k 
\end{equation} 
reads as follows: with $X_k = Kz_k$ and $\gamma > 0$,   
\begin{eqnarray*} 
X_{k+\frac{1}{N}} &=& X_{k} + \gamma (H_k - A(X_k)) = [(I - \gamma A)(X_k) + \gamma H_k] \\ 
X_{k+\frac{2}{N}} &=& X_{k+\frac{1}{N}} + \gamma (H_k - A(X_{k+\frac{1}{N}})) = [(I - \gamma A)(X_{k+\frac{1}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)(X_k) + \gamma H_k] - \gamma A([(I - \gamma A)(X_k) + \gamma H_k]) + \gamma H_k \\
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] \\
X_{k+\frac{3}{N}} &=& X_{k+\frac{2}{N}} + \gamma (H_k - A(X_{k+\frac{2}{N}})) = [(I - \gamma A)(X_{k+\frac{2}{N}}) + \gamma H_k] \\ 
%&=& [(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k] - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\ 
%&=& (I - \gamma A)(I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k - \gamma A((I - \gamma A)[(I - \gamma A)(X_k) + \gamma H_k] + \gamma H_k) + \gamma H_k \\
X_{k+\frac{4}{N}} &=& X_{k+\frac{3}{N}} + \gamma (H_k - A(X_{k+\frac{3}{N}})) =  [(I - \gamma A)(X_{k+\frac{3}{N}}) + \gamma H_k] \\
&\vdots& \\  
X_{k+\frac{N-1}{N}} &=& X_{k+\frac{N-2}{N}} + \gamma (H_k - A(X_{k+\frac{N-2}{N}})) =  [(I - \gamma A)(X_{k+\frac{N-2}{N}}) + \gamma H_k] \\  \\
X_{k+\frac{N}{N}} &=& X_{k+\frac{N-1}{N}} + \gamma (H_k - A(X_{k+\frac{N-1}{N}})) = [(I - \gamma A)(X_{k+\frac{N-1}{N}}) + \gamma H_k] \\ 
\end{eqnarray*}
Therefore, we see that its action is independent of $Kz_k$ in this setting, but it depends on $H_k$. The action of $D_r^*$ is important for the analysis. We shall interpret it as follows: 
\begin{equation} 
X_{k+1} = X_k + D_r^*(H_k - A(X_k)).  
\end{equation} 
%where 
%\begin{eqnarray*} 
%D_r^* (H_k - A(X_k)) &:=& \gamma (H_k - A(X_k))  \\ 
%&+& \gamma (H_k - A((I - \gamma A)(X_k) + \gamma H_k)) \\ 
%&+& \gamma (H_k - A((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k))) \\
%&+& \gamma (H_k - A((I - \gamma A)^2((I - \gamma A)(X_k) + \gamma H_k))) \\ 
%&+& \cdots \\
%&+& \gamma (H_k - A((I - \gamma A)^{N-2}((I - \gamma A)(X_k) + \gamma H_k))) \\
%&=& \gamma N H_k - \gamma \sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_k )
%\end{eqnarray*} 
For example, if $N = 2$, then 
\begin{equation} 
X_{k+1} = (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k. 
\end{equation}
and for example, if $N = 3$, then 
\begin{eqnarray*} 
X_{k+1} = (I - \gamma A)((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) + \gamma H_k.  
\end{eqnarray*}


We note that $A(X_*) - H_* = \nabla F(X_*) - H_* = 0$ and the 
error equation is given as follows:  
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_{k} + D_r^*(H_* - A(X_*)) - D_r^* (H_k - A(X_k)) \\ 
Kz_* - Kz_{k+1} &=& P_Z (X_{*} - X_{k} + D_r^*(H_* - A(X_*)) - D_r^* (H_k - A(X_k)) \\ 
H_* - H_{k+1} &=& H_* - H_k - \omega (X_* - X_{k+1}) + \omega K(z_* - z_{k+1}). 
\end{eqnarray*}
On the other hand, we have as for the GS or a single step GD case,
\begin{eqnarray*}
Kz_{*} - Kz_{k+1} = P_Z \left ( X_* - X_{k+1} \right ). 
\end{eqnarray*}
and thus
\begin{eqnarray*}
-\omega \left ( Kz_{*} - Kz_{k+1} \right ) = -\omega \left ( P_Z \left ( X_* - X_{k+1} \right ) \right ). 
\end{eqnarray*}
This leads to the following bound:  
\begin{eqnarray*}
\omega \|Kz_{*} - Kz_{k+1}\| &=& \|-\omega \left ( P_Z [X_* - X_k + D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_k) + rKz_k)] \right ) \| \\
%&=& \|-\omega \left ( P_Z [X_* - X_k + D_r^* (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_*) + rKz_*) \right.  \\
%&& + D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*)   \\
%&& \left. + D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) ] \right ) \| \\
&=& \|P_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))].
%&& - \omega (X_* - X_k + \left [ D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*) \right ] ) \\ 
%&& - \omega \left ( D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) \right ) ) \|. 
%
%\left ( (H_* - H_k) -\omega [X_* - X_k + D_r^* (H_k - A_r(X_*) + rKz_k) - D_r^* (H_k - A_r(X_k) + rKz_k) \right.  \\
%&& + \left. D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_*) + rKz_k) \right )  \| \\
%&\leq& \|P_Z [X_* - X_k + D_r^* (H_k - A_r(X_*) + rKz_k) - D_r^* (H_k - A_r(X_k) + rKz_k)] \| \\
%&& + \|P_Z[ D_r^{*} (H_* - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_*) + rKz_k)]\| \\
\end{eqnarray*}
On the other hand, we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|Q_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A_r(X_*) + rKz_*) - D_r^* (H_k - A_r(X_k) + rKz_k)] \| \\  
&=& \|Q_Z (H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k)] \|. 
%&& - \omega (X_* - X_k + \left [ D_r^{*} (H_k - A_r(X_*) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_*) \right ] ) \\ 
%&& - \omega \left ( D_r^{*} (H_k - A_r(X_k) + rKz_*) - D_r^{*} (H_k - A_r(X_k) + rKz_k) \right ) ) \| \\ 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
&& \|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq \\ && \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))] \| \\
&=& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k)) + D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\ 
&=& \|H_* - H_k - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-1}{N}}) + \gamma (H_* - H_k)] \| \\ 
&=& \|(1 - \omega \gamma) (H_* - H_k) - \omega [(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-1}{N}})] \|.  
\\
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|X_* - X_{k+\frac{N-1}{N}}\|  \\ 
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)(X_*) - (I - \gamma A)(X_{k+\frac{N-2}{N}}) + \gamma (H_* - H_k)\|  \\
&\leq& (1 - \gamma \omega) \|H_* - H_k\| + \omega (1 - \gamma \lambda)^2 \|X_* - X_{k+\frac{N-2}{N}} \| + \omega \gamma (1 - \gamma \lambda) \|H_* - H_k\| \\
&\leq& (1 - \gamma\omega) \|H_* - H_k\| + \omega ( 1- \gamma \lambda)^N \|X_* - X_k\| + \omega\gamma ((1 - \gamma \lambda) + \cdots + (1 - \gamma \lambda)^{N-1}) \|H_* - H_k\|. \end{eqnarray*}
We shall choose $\omega = 1/\gamma$. Then, we have that \begin{eqnarray*}
\|H_{*} - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} &\leq& \omega ( 1- \gamma \lambda)^N \|X_* - X_k\| + \omega\gamma \sum_{\mu = 1}^{N-1} (1 - \gamma \lambda)^\mu \|H_* - H_k\|.  
\end{eqnarray*}


Therefore, the convergence boils down to establish that there exists $c_0 > 0$ such that 
\begin{eqnarray*}
&& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_k - A(X_k))] \| \\
&&\qquad \leq (1 - c_0) \left ( \|H_{*} - H_{k}\| + \|Kz_* - Kz_{k}\|_{\omega} \right ). 
\end{eqnarray*}
We observe that for $N = 2$, we have that 
\begin{eqnarray*}
&&  \|H_* - H_k - \omega [X_* - X_k + \{ \gamma ( H_* - A(X_*) ) + \gamma (H_* - A((I - \gamma A)(X_*) + \gamma H_*)) \} \\
&& 
- \{ \gamma ( H_k - A(X_k)) + \gamma (H_k - A((I - \gamma A)(X_k))) \}
\end{eqnarray*}
\textcolor{red}{The trick is to target the second expression of $A$ with input given as $(I - \gamma A)(X)$:} 
\begin{eqnarray*}
&&  \|H_* - H_k - \omega [(I - \gamma A)(X_*) + \gamma H_* +  \gamma (H_* - A((I - \gamma A)(X_*) + \gamma H_*)) \} \\
&& 
- \{(I - \gamma A)(X_k) + \gamma H_k + \gamma (H_k - A((I - \gamma A)(X_k))) \} \| \\ 
&=& \|(I - \omega \gamma) (H_* - H_k) - \omega [[(I - \gamma A)(X_*) + \gamma H_*] - [(I - \gamma A)(X_k) + \gamma H_k] \\
&& - \gamma ( A((I - \gamma A)(X_*) + \gamma H_*) - A((I - \gamma A)(X_k)) + \gamma H_k) \} \\ 
&\leq& |I - \omega \gamma |\|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)(X_*) - (I - \gamma A)(X_k) + \gamma (H_* - H_k)\| \\ 
&\leq& (1 - \omega \gamma) \|H_* - H_k\| + (1 - \gamma \lambda)^2 \|X_* - X_k\|_\omega + \gamma \omega (1 - \gamma \lambda) \|H_* - H_k\| \\ 
&=& (1 - \gamma^2 \lambda) \|H_* - H_k\| + (1 - \gamma \lambda)^2 \|X_* - X_k\|_\omega. 
\end{eqnarray*}

%&& \qquad - \{ \gamma N H_k - \gamma \sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_k ) \}] \| \\
%&&  \|(I - \omega \gamma N ) (H_* - H_k) - \omega [X_* - X_k - \gamma \{\sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_*) + \gamma (I - \gamma A)^{\ell-2} H_* )\} \\ 
%&& \qquad - \{\sum_{\ell = 1}^{N}
%A ((I-\gamma A)^{\ell-1}(X_k) + \gamma (I - \gamma A)^{\ell-2} H_* ) \}] \| \\
%&&\qquad \leq (1 - c_0) \left ( \|H_{*} - H_{k}\| + \|Kz_* - Kz_{k}\|_{\omega} \right ). 
%\end{eqnarray*}



This leads that that there exists $c_0 > 0$ such that
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\|_\omega + \|H_* - H_{k+1}\| \leq (1 - c_0) \left [ \|Kz_* - Kz_k\|_\omega + \|H_* - H_k\| \right ]. 
\end{eqnarray*}
This completes the proof. 

We now consider the case $N = 3$. 
\textcolor{red}{The trick is to target the second expression of $A$ with input given as $(I - \gamma A)(X)$:} 
\begin{eqnarray*}
&&  \|(1 - 2\gamma \omega) (H_* - H_k) - \omega [(I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - \gamma A((I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) + \gamma H_*)] \\
&& - (I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) - \gamma A((I - \gamma A)((I - \gamma A)(X_k) + \gamma H_k) + \gamma H_k) \| \\ 
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda) \|(I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)((I - \gamma A)(X_*) + \gamma H_*) \| \\
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda)^2 \|(I - \gamma A)(X_*) + \gamma H_*) - (I - \gamma A)(X_k) - \gamma H_k) \| \\
&\leq& |1 - 2\gamma \omega| \|H_* - H_k\| + \omega (1 - \gamma \lambda)^3 \|X_* - X_k\| + \gamma \omega (1 - \gamma \lambda)^2 \|H_* - H_k\|. 
\end{eqnarray*}
This gives 
\begin{eqnarray*} 
\|H_* - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} &\leq& (1 - 2\gamma \omega + \gamma \omega (1 - \gamma\lambda)^2) \|H_* - H_k\| \\
&& \quad + (1 - \gamma \lambda)^3 \|Kz_* - Kz_k\|_{\omega}. 
\end{eqnarray*} 
and thus 
\begin{equation} 
\|H_* - H_{k+1}\| + \|Kz_* - Kz_{k+1}\|_{\omega} \leq (1 - \gamma \omega) \|H_* - H_k\| + (1 - \gamma \lambda)^3 \|Kz_* - Kz_k\|_{\omega}. 
\end{equation} 
This completes the proof. 

\begin{eqnarray*}
&& \|H_* - H_k - \omega [X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k)) + D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\ 
&& \|H_* - H_k - \omega [D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| \\
&& + 
\omega \|X_* - X_k + D_r^* (H_* - A(X_*)) - D_r^* (H_* - A(X_k))] \|. 
\end{eqnarray*}
We shall analyze one by one as follows: 
\begin{eqnarray*}
\|H_* - H_k - \omega [D_r^*(H_* - A(X_k)) - D_r^{*}(H_k - A(X_k)) ] \| &=& \\  
\end{eqnarray*}
For $N = 2$, we get for $\gamma \leq 1/L$, 
\begin{equation} 
(1 - 2\gamma \omega) \|H_* - H_k\| + \omega \gamma^2 L \|H^* - H_k\| \leq (1 - 2\gamma \omega) \|H_* - H_k\| + \omega \gamma \|H^* - H_k\| = (1 - \gamma \omega) \|H_* - H_k\|. 
\end{equation} 

\bibliographystyle{plain}
\bibliography{mybib}

%\end{document} 


\subsubsection{The convergence analysis for multiple GD steps}
We now look at the general case for $D_r$. As presented in \cite{mishchenko2022proxskip}, we shall set $X_k = Kz_k$ when we begin the GD step for the variable $X$. A total of $N$-step GD for finding zero of the following nonlinear function: 
\begin{equation} 
\nabla F(X) + r X = rKz_k + H_k 
\end{equation} 
reads as follows: with $\gamma > 0$, 
\begin{eqnarray*} 
X_{k+1} = X_{k} + N \gamma H_k - \gamma \left ( \sum_{\mu = 0}^{N-1} A\left ( X_{k + \mu/N}\right ) \right ).
\end{eqnarray*}
We note that $A(X_*) - H_* = \nabla F(X_*) - H_* = 0$ and the 
error equation is given as follows:  
\begin{eqnarray*}
X_{*} - X_{k+(\ell+1)/N} &=& X_* - X_{k + \ell/N} + \gamma (H_* - A(X_*)) - \gamma(H_{k + \ell/N} - A(X_{k + \ell/N})) \\
&=& X_* - X_{k+\ell/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + \ell/N})),  
\end{eqnarray*}
where $\ell = 0,1,\cdots,N-1$. More precisely, we have that 
\begin{eqnarray*}
X_{*} - X_{k+1} &=& X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) \\ 
X_{*} - X_{k+(N-1)/N} &=& X_* - X_{k + (N-2)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-2)/N})) \\  
X_{*} - X_{k+(N-2)/N} &=& X_* - X_{k + (N-3)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-3)/N})) \\  
&\vdots& \\ 
X_{*} - X_{k+1/N} &=& X_* - X_{k} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k})).  
\end{eqnarray*}
On the other hand, we have as for the GS or a single step GD case,
\begin{eqnarray*}
Kz_{*} - Kz_{k+1} = P_Z \left ( X_* - X_{k+1} \right ). 
\end{eqnarray*}
This means that in order to find out the relationship between $E_{k+1}^Z$ and $E_k^Z$, we must analyze the relationship between $E_{k+1}^X$ and $E_{k}^X$, which boils down to the following estimate under the product of $P_Z$. For all $\ell = 0,1,\cdots,N-1$, we have for $0 < \gamma \leq 1/L$, 
\begin{eqnarray*}
\|X_{*} - X_{k+(\ell+1)/N}\|_{P_Z} &=& \|X_* - X_{k+\ell/N} + \gamma (H_* - A(X_*)) - \gamma(H_{k} - A(X_{k+\ell/N}))\|_{P_Z} \\
&\leq& \|X_* - X_{k+\ell/N} - \gamma (A(X_*) - A(X_{k+\ell/N}))\|_{P_Z} \\ 
&\leq& (1 - \gamma \lambda) \|X_* - X_{k+\ell/N}\|_{P_Z}.  
\end{eqnarray*}
\textcolor{red}{This has to be proven !} 
This gives that 
\begin{eqnarray*}
\|X_{*} - X_{k+1/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k}\|_{P_Z}   \\ 
\|X_{*} - X_{k+2/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k+1/N}\|_{P_Z}   \\ 
\|X_{*} - X_{k+3/N}\|_{P_Z} &\leq& (1 - \gamma \lambda) \|X_* - X_{k+2/N}\|_{P_Z}  \\ 
&\vdots& \\
\|X_{*} - X_{k+1}\|_{P_Z} &\leq& (1 - \gamma \lambda)^N \|X_* - X_{k+(N-1)/N}\|_{P_Z}.  
\end{eqnarray*}
This gives that 
\begin{eqnarray*}
\|X_* - X_{k+1}\|_{P_Z} &\leq& (1 - \gamma \lambda)^N \|X_* - X_k\|_{P_Z} \\ 
&=& (1 - \gamma \lambda)^N \|X_* - X_k\|_{P_Z}.  
\end{eqnarray*}
Therefore, we have 
\begin{eqnarray*}
\|Kz_{*} - Kz_{k+1}\| = \|X_* - X_{k+1}\|_{P_Z} \leq (1 - \gamma \lambda)^N \|Kz_* - Kz_k\| 
\end{eqnarray*}
and we have that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &=& \|H_* - H_k - \omega [ (X_* - X_{k+1}) - K(z_* - z_{k+1}) ] \| \\
&=& \|H_* - H_k - \omega Q_Z (X_* - X_{k+1})\| = \|Q_Z [(H_* - H_k) - \omega (X_* - X_{k+1})]\| \\ 
&=& \|Q_Z [(H_* - H_k) - \omega [ X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) ]] \| \\ 
&=& \|Q_Z [(H_* - H_k) - \omega [ X_* - X_{k + (N-1)/N} + \gamma (H_* - A (X_*)) - \gamma (H_{k} - A(X_{k + (N-1)/N})) ]\}\| \\ 
&\leq& \|Q_Z [(I - \omega \gamma)(H_* - H_k) - \omega (X_* - X_{k + (N-1)/N} - \gamma (A (X_*) - A(X_{k + (N-1)/N}))) ]\| \\ 
&\leq& |1 - \omega \gamma| \|H_* - H_k\| + \omega \|Q_Z[X_* - X_{k + (N-1)/N} - \gamma (A (X_*) - A(X_{k + (N-1)/N}))]\|.
\end{eqnarray*}
Therefore, we have 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\|_{1/r} &\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + (1 - \gamma\lambda) \|X_* - X_{k + (N-1)/N}\| \\
&\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + (1 - \gamma \lambda) \|X_* - X_{k + (N-2)/N} - \gamma (A(X_*) - A(X_{k + (N-2)/N})) + \gamma(H_* - H_k)\| \\
&\leq& |1 - \omega \gamma| \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda) \left \{ (1 - \gamma \lambda) \|X_* - X_{k + (N-2)/N}\| + \gamma\|H_* - H_k\| \right \} \\
&=& [|1 - \omega \gamma| + \gamma (1 - \gamma \lambda) \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^2 \|X_* - X_{k + (N-2)/N}\| \\ 
&=& [|1 - \omega \gamma| + \gamma (1 - \gamma \lambda) + \gamma (1 - \gamma \lambda)^2) \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^3 \|X_* - X_{k + (N-3)/N}\| \\ 
&=& \cdots \\ 
&\leq& [|1 - \omega \gamma| + \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^N \|X_* - X_{k}\| \\ 
&=&  [|1 - \omega \gamma| + \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\|_{1/r} + ( 1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\|.  
\end{eqnarray*}
This gives that 
\begin{eqnarray*}
\|H_{*} - H_{k+1}\| &\leq& [|1 - \omega \gamma| + \omega \gamma \sum_{\ell = 1}^{N-1} (1 - \gamma \lambda)^{\ell}] \|H_* - H_k\| + \omega ( 1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\| \\ 
&=& \left ( |1 - \omega \gamma| + \frac{\omega}{\lambda} ((1 - \gamma \lambda) - (1 - \gamma \lambda)^N) \right ) \|H_* - H_k\| + \omega (1 - \gamma \lambda)^N \|Kz_* - Kz_{k}\|. 
\end{eqnarray*}
Therefore, we arrive at the conclusion that 
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\| + \|H_* - H_{k+1}\| \leq 
\left ( (1 + \omega) (1 - \gamma \lambda)^N \right ) \|Kz_* - Kz_k\| + \left (|1 - \omega \gamma| + \frac{\omega}{\lambda} (1 - (\gamma\lambda)^N) \right ) \|H_* - H_k\|_r. 
\end{eqnarray*}
We note that the coefficient $(1 + \omega) ( 1- \gamma \lambda)^N$ can be made to be smaller than one easily. However, the second term needs a care and our goal is to make 
\begin{equation} 
|1 - \omega\gamma| + \frac{\omega}{\lambda} < 1 
\end{equation} 
Note that if $|1 - \omega\gamma| = 1 - \omega\gamma$, then we do not have a chance. But $|1 - \omega\gamma| = \omega\gamma - 1$ leads that 
\begin{equation}
\omega \gamma - 1 + \frac{\omega}{\lambda} < 1 
\end{equation}
to satisfy that there exists $c_0 > 0$ such that
\begin{eqnarray*}
\|Kz_* - Kz_{k+1}\| + \|H_* - H_{k+1}\|_r \leq (1 - c_0) \left [ \|Kz_* - Kz_k\| + \|H_* - H_k\|_r \right ]. 
\end{eqnarray*}
This completes the proof. 
\begin{remark}
We should use $M_2$ to eliminate $H$ part to make tighter bounds. In fact, we can use the $M_2$ inner product here to eliminate $H$ part.  
\end{remark}

\begin{itemize}
\item Does the convergence rate reduce to exact Gauss-Seidel case when $N\rightarrow \infty$ ? 
\end{itemize}

%\end{comment} 

\bibliographystyle{plain}
\bibliography{mybib}

%\end{document}
\subsection{Linear Case handling $(X,z)$ and $H$ block} 
We now consider the standard approach to handle the following system: 
\begin{equation}
\begin{pmatrix}
A_r & - r K & -I \\
-r K^T& r K^TK & K^T\\
-I& K & 0\\
\end{pmatrix}
\begin{pmatrix}
X_*\\
z_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}. 
\end{equation}
For simplicity, we consider the following block system: 
\begin{equation}
\begin{pmatrix}
\nabla G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0
\end{pmatrix}
\end{equation}
where 
\begin{equation}
\nabla G = \begin{pmatrix}
A_r & -r K\\
-r K^T & rK^T K
\end{pmatrix}, \quad \mbox{ and } \quad 
B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
We note that the Schur complement is given as follows:   
\begin{equation}
S = -B \nabla G^{*} (-B^T) 
\end{equation}
We write an equivalent form of the above equation using the Schur complement system. We shall consider to apply the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
L = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad L^{-1} = \begin{pmatrix}
A_r^{-1} & 0\\
(rK^T K)^{-1} rK^T A_r^{-1} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}

% \begin{equation}
% \begin{aligned}
%       A^{-1} & = \begin{pmatrix}
%      A_r^{-1} + A_r^{-1} rK ( rK^TK - rK^T A_r^{-1} rK)^{-1} rK^T A_r^{-1} & -  A_r^{-1} rK (rK^TK - rK^T A_r^{-1} rK)^{-1}\\
%      (rK^TK - rK^T A_r^{-1} rK)^{-1} rK^T A_r^{-1}  & (rK^TK - rK^T A_r^{-1} rK)^{-1}
%     \end{pmatrix}, \\
%      & = \begin{pmatrix}
%      A_r^{-1} + r A_r^{-1} K (rP)^{-1} K^T rA_r^{-1} & -  r A_r^{-1} K (rP)^{-1} \\
%      (rP)^{-1} rK^T A_r^{-1}  & (rP)^{-1}
%      \end{pmatrix},
% \end{aligned}
% \end{equation}
% where $P = K^TK - K^T rA_r^{-1} K$ is symmetric positive definite and has spectral radius less than 1. 

% \begin{lemma}
% The Schur complement S is symmetric positive definite. 
% \end{lemma}
% \begin{proof}
% It is shown in Lemma \ref{lemma4} that $A^{-1}$ is symmetric positive definite.
% Since $B^T$ is onto, we must have $B A^{-1} B^T$ being symmetric positive definite. 
% \end{proof}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray}
U_{k+1} &=& U_k + L^{-1} (-B^T H_k - \nabla G( U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray}
We note that the exact solutions satiafy \begin{eqnarray}
U_{*} &=& U_{*} + L^{-1} (-B^T H_{*}-\nabla G (U_{*})) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray}
Thus, with the convention that $E_{k}^U = U - U_k$ and $E_{k}^H = H - H_k$, we obtain the error equations: 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + L^{-1} (-\nabla G(E^U_k) -B^T E^H_k) \\
E^H_{k+1} &=& E^H_k + \omega B E^U_{k+1}. 
\end{eqnarray}
\begin{comment} 
In the matrix form, we have 
\begin{equation}
    \begin{pmatrix} 
     I  & 0 \\
      -\omega B & I
    \end{pmatrix} 
        \begin{pmatrix} 
      E^U_{k+1} \\
       E^H_{k+1}
    \end{pmatrix}  = 
        \begin{pmatrix} 
     I - L^{-1} A & -L^{-1} B^T \\
    0 & I
    \end{pmatrix} 
            \begin{pmatrix} 
      E^U_{k} \\
       E^H_{k}
    \end{pmatrix} 
\end{equation}
\end{comment} 
In the other direction, we have that 
\begin{eqnarray}
E^U_{k+1} &=& E^U_k + L^{-1} (- \nabla G(E^U_k) - B^T E^H_k) \\
          &=& E^U_k - L^{-1} \nabla G(E^U_k) - L^{-1} B^T E^H_k \\
E^H_{k+1} &=& E^H_k + \omega \left [ B (E^U_k + L^{-1} (- \nabla G(E_k^U) - B^T E^H_k) \right ] \\
&=& E^H_k - \omega B L^{-1} B^T E_k^H + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
\end{eqnarray}

% Directly taking $A-$norm: 
% \begin{eqnarray}
% \| E^U_{k+1} \|_A &\leq& \| I - L^{-1} \nabla G \| \|E^U_k\|_A  + \| L^{-1} B^T \|_A \|E^H_k\|_A \\
% \| E^H_{k+1}\| &\leq& \|I - \omega B L^{-1} B^T \| \|E_k^H\| + \omega B (E_k^U - L^{-1} \nabla G(E_k^U)). 
% \end{eqnarray}

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

\begin{equation}
    \|M \|_A \leq \sqrt{ \kappa(A)} \| M \|_2
\end{equation}
We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \lambda_{max}(A) \| L^{-1} B^T H\| \\
   & =\lambda_{max}(A) (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}
We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}
\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}
\textcolor{red}{ Here !} 

A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\|_{A} &\leq& \rho_U \|E_k^U\|_{A} + \|L^{-1} B^T E_k^H\|_{A} \\ 
&\leq&  \rho_U \|E_k^U\|_{A} + r \frac{L + r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}


\textbf{Details: }
Denote $A$ as $\nabla G$, which is SPD. 

Note that the equivalence of $A-$norm and the standard $l^2-$norm is given by 
\begin{equation}
  \lambda_{min} (A) \|U  \| \leq    \| U \|_A \leq \lambda_{max} (A )\|U  \|
\end{equation}

We need to analyze the following operators. 
It is known that block Gauss Seidel for SPD system has the following relation under A-norm.
\begin{equation}
    \| I - L^{-1} \nabla G \|_A \leq \rho_U < 1 
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \| & = \| L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix}\|  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
   \| L^{-1} B^T H \|_A & = \langle A L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix},  L^{-1}  \begin{pmatrix}
   -H \\
   0
   \end{pmatrix} \rangle  \\
   & \leq \left(\|A_r^{-1}\| +\| (K^T K)^{-1}K^T \|\right) \|H\| \\
   & = (\frac{1}{r + \lambda } + \frac{1}{\sqrt{n}} ) \|H\| 
\end{aligned}
\end{equation}


We compute 
\begin{equation}
\begin{aligned}
       BL^{-1}B^T & = A_r^{-1} - K (K^T K)^{-1} K^T A_r^{-1} + \frac{1}{r}K (K^T K)^{-1}K^T \\
       & = A_r^{-1} + K (K^T K)^{-1} K^T (\frac{1}{r} I - A_r^{-1})
\end{aligned}
\end{equation}
Therefore, we see $BL^{-1} B^T$ is positive definite, by choosing sufficiently small $\omega$, we have
\begin{equation}
\begin{aligned}
   \rho ( I - \omega B L^{-1} B^T ) < 1 .
\end{aligned}
\end{equation}

\begin{equation}
     \| \omega B (I - L^{-1} \nabla G) \|_A \leq \omega \|B \|_A \rho_U
\end{equation}


\subsubsection{Nonlinear Case} 
For simplicity, we consider the following block system: 
\begin{equation}
\begin{pmatrix}
\nabla G  & B^T \\
B  & 0\\
\end{pmatrix} 
\begin{pmatrix}
U_* \\
H_*
\end{pmatrix} = 
\begin{pmatrix}
0\\
0
\end{pmatrix}
\end{equation}
where 
\begin{equation}
\nabla G = \begin{pmatrix}
A_r & -r K\\
-r K^T & rK^T K
\end{pmatrix}, \quad \mbox{ and } \quad 
B^T = \begin{pmatrix}
-I \\K^T
\end{pmatrix}.
\end{equation}
We shall consider to apply the Gauss-Seidel for the block $\nabla G$, namely, 
\begin{equation}
L = \begin{pmatrix}
A_r & 0\\
-r K^T & rK^T K 
\end{pmatrix} 
\quad \mbox{ and } \quad L^{*} = \begin{pmatrix}
A_r^{*} & 0\\
(rK^T K)^{-1} rK^T A_r^{*} & r^{-1} (K^T K)^{-1} 
\end{pmatrix} 
\end{equation}
We note that the contraction property requires the expansion of $L$, $\nabla G$ and $L^*$. Thus, we list their computation below: 
\begin{eqnarray*}
\nabla G(U) &=& 
\begin{pmatrix}
A_r(X) - r K z \\ 
-r K^T X + r K^T K z 
\end{pmatrix} \\ 
L(U) &=& 
\begin{pmatrix}
A_r(X) \\ 
-r K^T X + r K^T K z 
\end{pmatrix} \\ 
L^*(U) &=& 
\begin{pmatrix}
A_r^*(X) \\ 
(r K^TK)^{-1}r K^T A_r^{*}(X) + r^{-1} (K^T K)^{-1} z 
\end{pmatrix} 
\end{eqnarray*}
% where $P = K^TK - K^T rA_r^{-1} K$ is symmetric positive definite and has spectral radius less than 1. 

% \begin{lemma}
% The Schur complement S is symmetric positive definite. 
% \end{lemma}
% \begin{proof}
% It is shown in Lemma \ref{lemma4} that $A^{-1}$ is symmetric positive definite.
% Since $B^T$ is onto, we must have $B A^{-1} B^T$ being symmetric positive definite. 
% \end{proof}
The inexact Uzawa iteration can then be given as follows: 
\begin{eqnarray*}
U_{k+1} &=& U_k + L^{*} (-B^T H_k - \nabla G(U_k)) \\
H_{k+1} &=& H_k + \omega B U_{k+1}. 
\end{eqnarray*}
We note that the exact solutions satiafy 
\begin{eqnarray*}
U_{*} &=& U_{*} + L^{*} (-B^T H_{*}-\nabla G (U_{*})) \\
H_{*} &=& H_{*} + \omega B U_{*}. 
\end{eqnarray*}
Thus, with the convention that $E_{k}^U = U - U_k$ and $E_{k}^H = H - H_k$, we obtain the error equations: 
\begin{eqnarray*}
U_* - U_{k+1} &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B (U_* - U_{k+1}). 
\end{eqnarray*}
Therefore, we have that 
\begin{eqnarray*}
U_* - U_{k+1} &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B \left [U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) \right. \\
&& \left. - L^{*} (-\nabla G(U_k) -B^T H_k) \right ]. 
\end{eqnarray*}
This gives that 
Therefore, we have that 
\begin{eqnarray*}
\langle U_* - U_{k+1}, \nabla G(U_*) - \nabla G(U_{k+1}) \rangle &=& U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) - L^{*} (-\nabla G(U_k) -B^T H_k)  \\
H_* - H_{k+1} &=& H_* - H_k + \omega B \left [U_* - U_k + L^{*} (-\nabla G(U_*) -B^T H_*) \right. \\
&& \left. - L^{*} (-\nabla G(U_k) -B^T H_k) \right ]. 
\end{eqnarray*}



A simple analysis and crude upper bound would be as follows: 
\begin{eqnarray*}
\|E_{k+1}^U\| &\leq& \rho_U \|E_k^U\| + \|L^{-1} B^T E_k^H\| \\ 
&\leq&  \rho_U \|E_k^U\| + \frac{r}{\lambda + r} \|E_k^H\|_r \\
\|E_{k+1}^H\|_r &\leq& r \rho_H \|E_k^H\|_r + \frac{\omega}{r} \rho_U \|E_k^U\|,  
\end{eqnarray*}
where 
\begin{equation}
\|E_k^H\|_r = \frac{1}{r}\|E_k^H\|.   
\end{equation}
Therefore, by adding two terms, we obtain that 
\begin{eqnarray*}
\|E_{k+1}^U\| + \|E_{k+1}^H\|_r &\leq& \left ( \rho_U + \frac{\omega}{r} \rho_U \right) \|E_k^U\| + \left ( \frac{r}{\lambda + r} + r \rho_H \right ) \|E_k^H\|_r \leq c_0 \left ( \|E_{k}^U\| + \|E_{k}^H\|_r \right ),  
\end{eqnarray*}
where 
\begin{equation}
c_0 = \max \left \{ \rho_U + \frac{\omega}{r} \rho_U, \frac{r}{r+\lambda} + r\rho_H \right \}. 
\end{equation} 
We note that $\rho_U < 1$ and thus, the first term can be made to be small by choosing 
\begin{eqnarray}
\rho_U + \frac{\omega}{r} \rho_U < 1 \quad \mbox{ and } \quad \frac{r}{r+\lambda} + r\rho_H < 1. 
\end{eqnarray}
\begin{remark}
For $L$ being replaced by a number of Gradient descent method, we observe that we can control $\rho_U < 1$ even if it is one step GD. By choosing an appropriate $\omega$, we can satisfy the above inequality. 
\end{remark}

 
\subsection{A New Proof based on Dual operator}

Under the Uzawa framework. 
For simplicity, we denote the nonlinear operator $\nabla F(\cdot) + r I$ as $A_r$, $[z, H]^T$ as $U$,  and rewrite the matrix as follows.
\begin{equation}
\label{optimality condition aug Lag matrix form 2 by 2}
    \begin{pmatrix}
    A_r & B^T \\
    B & C
    \end{pmatrix}
    \begin{pmatrix}
    X\\
    U 
    \end{pmatrix} = 
    \begin{pmatrix}
    0 \\
    0,
    \end{pmatrix}
\end{equation}
where 
\begin{equation}
A_r = \nabla F(\cdot) + r I, \quad B = \begin{pmatrix}
-r K^T\\ -I
\end{pmatrix}, \quad C = \begin{pmatrix}
r K^T K & K^T \\
K & 0
\end{pmatrix}.
\end{equation}
We view the ADMM as a type of Uzawa iteration that solves Schur complement operator with a different iterative method. Namely, we notice that the system can be given as follows: 
\begin{subeqnarray*}
0 &=& A_r X  + B^T U \\
0 &=& \left [ B A_r^* (-B^T) (U) + CU \right ] = S(U). 
\end{subeqnarray*}
Under this setting, we can understand that the Uzawa iteration with an iterative method for the Schur complement is given in the following form: 
\begin{equation}\label{UzawaADMM}
\begin{cases}
A_r X_{k+1} + B^T U_k = 0 \quad \mbox{ or equivalently } \quad X_{k+1} = A_r^*(-B^T)( U_k) \\
U_{k+1} = U_k + N^{-1} \left( - \left [ B A_r^*(-B^T)(U_k) + CU_k \right] \right) = U_k + N^{-1} \left( - (B X_{k+1} + C U_k)\right),
\end{cases}
\end{equation}
where $N$ is some approximation of the nonlinear Schur complement operator. Note that our choice of $N$ will be given as follows: 
\begin{equation}\label{shurH} 
N = \begin{pmatrix}
r K^T K & 0\\
K & - \frac{1}{r} I
\end{pmatrix} \quad \mbox{ and } \quad  N^{-1} = \begin{pmatrix}
(r K^T K)^{-1} &  0 \\
 r K (r K^T K)^{-1} & - r I
\end{pmatrix}. 
\end{equation}
Such an iterative method corresponds to a damped Gauss-Seidel type inexact solver for the Schur complement that solves the first variable $z_{k+1}$ and then using Richardon's iteration with step size $r$ for the second variable $H_{k+1}$. We now show that this is exactly the ADMM method given in Algorithm \ref{algADMM1}.

\begin{proposition}\label{prop: Uzawa iterative solver}
The ADMM method in Algorithm \ref{algADMM1} is equivalent to the Uzawa iterations \eqref{UzawaADMM} with the choice of $N$ defined as in \eqref{shurH}.
\end{proposition}
\begin{proof}
We begin by writing the Uzawa iterations \eqref{UzawaADMM} for each variable update. We note that the second iteration in \eqref{UzawaADMM}  can be expanded as follows.

\begin{equation}
    \begin{pmatrix}
    z_{k+1}\\
    H_{k+1}
    \end{pmatrix} = \begin{pmatrix}
    z_{k}\\H_{k}
    \end{pmatrix} + \begin{pmatrix}
    (r K^T K)^{-1} & 0 \\
   r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
     r K^T X_{k+1} - r K^T K z_k - K^T H_k\\
     X_{k+1} - Kz_k
    \end{pmatrix}
\end{equation}
We can then verify that the iterations indeed are the same as ADMM iterations. Namely, we have for $X_{k+1}$ update, that $A_r X_{k+1} - r Kz_{k} - H_k = 0 $, that is 
\begin{equation}
\nabla F(X_{k+1}) + r X_{k+1} - r K z_k - H_k = 0
\end{equation}
and for $z_{k+1}$ update, that \begin{equation}
\begin{aligned}
z_{k+1} &= z_k + (r K^T K)^{-1} (r K^T X_{k+1} - K^T H_k) - z_k \\
& = (r K^T K)^{-1} (r K^T X_{k+1} - K^T H_k)
\end{aligned}
\end{equation}
and for $H_{k+1}$ update, that  \begin{equation}
\begin{aligned}
H_{k+1} & = H_k +  r K(r K^T K )^{-1}\left( r K X_{k+1} - r K^T K z_k - K^T H_k \right) -r (X_{k+1} - Kz_k) \\
                & = H_k + r K(r K^T K )^{-1} (r K X_{k+1} - K^T H_k) - r K z_k - r (X_{k+1} - Kz_k) \\
                & = H_k +r K z_{k+1} -r X_{k+1}.
\end{aligned}
\end{equation}
Thus, we have 
\begin{equation}
H_{k+1} - H_k -r(Kz_{k+1} - X_{k+1})=0.
\end{equation}
This completes the proof. 
\end{proof}
We shall now set $E_k^U = U - U_k$ and $E_k^X = X - X_k$ and then shall drive the error equation for the above algorithm. First we note that 
\begin{equation}
X = A_r^* (-B^TH) \quad \mbox{ and } \quad X_{k} = A_r^* (-B^T H_{k-1})
\end{equation}
Thus, we have that 
\begin{eqnarray*}
\|X - X_k\|^2 = \|A_r^* (-B^TU) - A_r^* (-B^T U_{k-1})\|^{2} \leq \frac{1}{c_0 + r} \|B^T( U - U_{k-1})\|^2 
\end{eqnarray*}
Therefore, we shall only need to analyze the convergence of $E_k^U$ for the convergence of $E_k^X$. On the other hand, the error propagation operator for $E_k^U$ can be given as follows: 
\begin{equation}
E_{k+1}^U = E_k^U - N^{-1} (S(U) - S(U_k)). 
\end{equation}
We now introduce an inner product defined on the space $U = \{(z_k, H_k)_{k=1,\cdots}, (z_*, H_*) \}$ by 
\begin{equation}
\langle \overline{N} U_k, U_k \rangle = \left \langle 
\begin{pmatrix}
I & 0\\
0 & -I
\end{pmatrix}
\begin{pmatrix}
r K^T K & 0\\
K & -\frac{1}{r} I
\end{pmatrix} \begin{pmatrix}
z_k \\
H_k
\end{pmatrix},  \begin{pmatrix}
z_k \\
H_k
\end{pmatrix} \right \rangle = r \langle Kz, Kz \rangle + \frac{1}{r} \langle H, H \rangle  
\end{equation}
Therefore, it makes a norm. We now notice that
\begin{eqnarray} 
\overline{N} E_{k+1}^U = \overline{N} E_k^U - (\overline{S}(U) - \overline{S}(U_k)), 
\end{eqnarray} 
where 
\begin{equation} 
\overline{S} = \begin{pmatrix}
I & 0\\
0 & -I
\end{pmatrix} S.
\end{equation} 
This indicates that 
\begin{eqnarray*}
\|\overline{N} E_{k+1}^U\|^2 = \overline{N} E_k^U - (\overline{S}(U) - \overline{S}(U_k)), 
\end{eqnarray*}

\begin{comment} 
\begin{lemma}
For the error propagation matrix, we have $\rho(I - N^{-1}S) =\eta < 1$. 
\end{lemma}
\begin{proof}
\textcolor{red}{We found this part is not accurate and need to modify.
First, we observe that $I - N^{-1}S$ is given as follows:


where $E := K^T K = n I$, $Q_A = I - r A_r^{-1}$. We note that 

By $2\times 2$ block matrix decomposition, we have the error propagation matrix is spectrally equivalent to the following matrix
\begin{equation}\label{mat1}
    \begin{pmatrix}
   S1 & 0 \\
 0   &
 S_2 + S_3   \end{pmatrix},
\end{equation}
where $S_1 = I -   E^{-1} K^T Q_A K$, $S_2 = (I - M_2)Q_A $, $S_3 = (I- M_2) Q_A K (I - E^{-1} K^T Q_A K )^{-1} E^{-1} K^T Q_A $.

It is easy to see that $S_1$ is symmetric positive and has $\rho(S_1) < 1$.
We also have $\rho(S_2) \leq \frac{C_0}{r + C_0}$, $\rho(S_3) \leq \frac{C_0}{ r+C_0}\frac{r +C_0}{r} \frac{C_0}{r +C_0 }$. So $\rho(S_2 + S_3) \leq \frac{C_0}{r} \le 1 $ for sufficiently large $r$. 

We can then take $\eta = \min \{ \rho(S_1), \frac{C_0}{r} \}$. This completes the proof.}
\end{proof}
\end{comment} 

\subsubsection{linear case}
To analyze the above Uzawa iteration, the difficulty lies in the nonlinear operation $\nabla F(\cdot)$ since F is assumed to be strongly convex and L-smooth. We first consider a simpler case when $F(X) = \frac{1}{2} X^T A X - b^T X$ is a quadratic function, where $A$ is a symmetric positive definite matrix. In this case, we have a system of linear equations. 

\begin{equation}
\label{optimality condition in linear case}
    \begin{pmatrix}
    A_r & -r K & -I \\
    -r K^T& r K^TK & K^T\\
    -I& K & 0\\
    \end{pmatrix}
    \begin{pmatrix}
    X_*\\
    z_* \\
    H_*
    \end{pmatrix} = 
    \begin{pmatrix}
    f\\
    0\\
    0
    \end{pmatrix}
\end{equation}

\begin{lemma}
$I - rA_r^{-1}$ is symmetric positive definite, and $\rho(I - rA_r^{-1}) < \frac{c_{max}}{r + c_{max}}$, where $c_{max}$ is the largest eigenvalue of $A$. $\rho(A_r^{-1}) \leq \frac{1}{r + c_{min}}$, where $c_{min}$ is the smallest eigenvalue of $A$.
\end{lemma}

\begin{theorem}[Well-posedness]
The matrix \eqref{optimality condition in linear case} is invertible. 
\end{theorem}
\begin{proof}
We know that $A_r$ is invertible since it is positive definite. 
Using the same notation as in \eqref{optimality condition aug Lag matrix form 2 by 2}, it now suffices to prove that the Schur complement $S = C - B A_{r}^{-1} B^T$ is invertible. To see this, we write out the schur complement explicitly.
We have 
\begin{equation}
    B A_r^{-1} B^T = \begin{pmatrix}
    r^2 K^T A_r^{-1} K&  r K^T A_r^{-1}\\
    r A_r^{-1} K & A_r^{-1}
    \end{pmatrix}
\end{equation}
\begin{equation}
    S = \begin{pmatrix}
    r K^T \left(I - r A_{r}^{-1} \right)K  & K^T (I -  r A_{r}^{-1} ) \\
    (I -  r A_{r}^{-1} ) K  & - A_{r}^{-1}
    \end{pmatrix}
\end{equation}
Note that in $S$, the first diagonal block is positive definite because $I - r A_{r}^{-1}$ is positive definite. 

Now it suffices to again consider the Schur complement of $S$. We have 
\begin{equation}
    \tilde{S} = -A_r^{-1} - D \left[ r K^T \left(I -  r A_{r}^{-1} \right)K  \right] D^T  ,
\end{equation}
where $D = (I - (\frac{1}{r} A_r)^{-1}) K$.
Note that $D \left[ r K^T \left(I - (\frac{1}{r} A_{r})^{-1} \right)K  \right] D^T $ is positive semi-definite. Since $A_r$ is positive definite, we have $\tilde{S}$ is negative definite. 
\end{proof}

\subsubsection{Exact Uzawa for linear problems}
We consider the following exact Uzawa iteration,
\begin{equation}
    \begin{cases}
     X_{k+1} = A_r^{-1}f - A_r^{-1}B^T U_k, \quad A_r X_{k+1} +B^T U_k = f \\
     U_{k+1} = U_k + N^{-1} \left( B A_r^{-1} f - (C - B A_r^{-1}B^T)U_k \right) = U_k + N^{-1} \left( - B X_{k+1} - C U_k\right),
    \end{cases}
\end{equation}
where $N$ is some approximation of the Schur complement operator given by 
\begin{equation}
    N = \begin{pmatrix}
    r K^T K & 0\\
     K & - \frac{1}{r} I
    \end{pmatrix} = \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix}^{-1}.
\end{equation}

The exact solution satisfies 
\begin{equation}
    \begin{cases}
     X = A_r^{-1}f - A_r^{-1}B^T U \\
     U = U + N^{-1} \left( B A_r^{-1} f - (C - B A_r^{-1}B^T)U \right) = U + N^{-1} \left( - B X - C U\right),
    \end{cases}
\end{equation}
Denoting $E^X_{k} = X_{k} - X$, $E^U_{k} = U_k - U$, we write out the error equation as follows.
\begin{eqnarray}
E^X_{k+1} &=& - A_r^{-1}B^T (U_k - U) \\
E^U_{k+1} &=& E^U_{k} + N^{-1} \left( -(C - B A_r^{-1}B^T) E^{U}_k \right) = (I - N^{-1}S) E^U_{k}
\end{eqnarray}
With the above error equations, it is not hard to see that the convergence depend on the spectral radius of the error propagation matrix $I - N^{-1}S$. 

\begin{lemma}
The following is our goal: 
$\|I - N^{-1}S\| = \delta < 1$
for some norm. 
\end{lemma}
\begin{proof}
The error transfer operator is given as follows: 
\begin{eqnarray*}
I - N^{-1}S &=& 
\left ( \begin{matrix}
I & 0 \\ 
0 & I 
\end{matrix} \right ) - \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
    r K^T \left(I - r A_{r}^{-1} \right)K  & K^T (I -  r A_{r}^{-1} ) \\
    (I -  r A_{r}^{-1} ) K  & - A_{r}^{-1}
    \end{pmatrix}   \\
&=& \begin{pmatrix}
I - (K^TK)^{-1} K^T (I - r A_r^{-1}) K&  -\frac{1}{r} (K^TK)^{-1} K^T (I - r A_r^{-1}) \\
r(I - K (K^TK)^{-1} K^T) (I - r A_r^{-1}) K  & (I - K (K^TK)^{-1} K^T) (I - r A_r^{-1})
\end{pmatrix},
\end{eqnarray*}
%where $E := K^T K = n I$, $Q_A = I - r A_r^{-1}$. We note that 
%
%By $2\times 2$ block matrix decomposition, we have the error propagation matrix is spectrally equivalent to the following matrix
%\begin{equation}\label{mat1}
%    \begin{pmatrix}
%   S1 & 0 \\
% 0   &
% S_2 + S_3   \end{pmatrix},
%\end{equation}
%where $S_1 = I -   E^{-1} K^T Q_A K$, $S_2 = (I - M_2)Q_A $, $S_3 = (I- M_2) Q_A K (I - E^{-1} K^T Q_A K )^{-1} E^{-1} K^T Q_A $.

%It is easy to see that $S_1$ is symmetric positive and has $\rho(S_1) < 1$.
%We also have $\rho(S_2) \leq \frac{C_0}{r + C_0}$, $\rho(S_3) \leq \frac{C_0}{ r+C_0}\frac{r +C_0}{r} \frac{C_0}{r +C_0 }$. So $\rho(S_2 + S_3) \leq \frac{C_0}{r} \le 1 $ for sufficiently large $r$. 

%We can then take $\eta = \min \{ \rho(S_1), \frac{C_0}{r} \}$. This completes the proof.
\end{proof}

\begin{theorem}
For quadratic objective 
\end{theorem}

Now we extend our result to a nonlinear $\nabla F(X)$. 

\subsubsection{Inexact Uzawa for linear problems}


The inexact Uzawa iteration is given as follows. 

\begin{equation}
    \begin{cases}
     X_{k+1} = X_k + N_1^{-1}(-B^T U_k - A_r X_k), \quad A_r X_{k+1} +B^T U_k = f \\
     U_{k+1} = U_k + N^{-1} \left( - B X_{k+1} - C U_k\right),
    \end{cases}
\end{equation}
where $N_1^{-1}$ is the solver used for the $X$ part, e.g. one step of gradient descent or several steps. $N^{-1}$ is the same linear solver as before. 

We assume the solver $N_1^{-1}$ satisfies  $N_1^{-1}(0) = 0$. This is true for both linear solver and some nonlinear solvers, including several steps of gradient descent.


Under this assumption, the exact solution satisfies the following equations. 

\begin{equation}
    \begin{cases}
     X = X + N_1^{-1}(-B^T U - A_r X)\\
     U = U + N^{-1} \left( - B X - C U\right),
    \end{cases}
\end{equation}
Taking the difference, one can obtain the following error equations. 

\begin{eqnarray}
 E^X_{k+1} & = & E^X_{k} + N_1^{-1} (-B^T U_k - A_r X_k) - N_1^{-1} (-B^T U - A_r X) \\
E_{k+1}^U &=& E_{k}^U + N^{-1} (- B E^X_{k+1} - C E^U_k).
\end{eqnarray}

If $N_1^{-1}$ is a linear solver, we have the follow error equation due to linearity. 
\begin{eqnarray}
 E^X_{k+1} & = & E^X_{k} + N_1^{-1} (-B^T E^U_k - A_r E^X_k) \\
 E_{k+1}^U &=& E_{k}^U + N^{-1} (- B E^X_{k+1} - C E^U_k).
\end{eqnarray}
Writing it in the matrix form, we have 
\begin{equation}
    \begin{pmatrix}
     I & 0\\
     N^{-1} B & I
    \end{pmatrix} 
    \begin{pmatrix}
    E^X_{k+1} \\
    E^U_{k+1}
    \end{pmatrix} 
  = \begin{pmatrix}
    I - N_1^{-1} A_r & - N_1^{-1} B^T \\
    0 & I - N^{-1} C
    \end{pmatrix}
    \begin{pmatrix}
    E^X_{k} \\
    E^U_{k}
    \end{pmatrix}  . 
\end{equation}
Rearranging, we have 
\begin{equation}\begin{pmatrix}
    E^X_{k+1} \\
    E^U_{k+1}
    \end{pmatrix} = 
        \begin{pmatrix}
     I & 0\\
     - N^{-1} B & I
    \end{pmatrix} 
    \begin{pmatrix}
    I - N_1^{-1} A_r & - N_1^{-1} B^T \\
    0 & I - N^{-1} C
    \end{pmatrix}
    \begin{pmatrix}
    E^X_{k} \\
    E^U_{k}
    \end{pmatrix}  
\end{equation}
Now, we analyse the spectral radius of the error propagation matrix. 

We have for an iterative solver like Richardson's iteration (sufficiently small step size), 
$\rho(I - N_1^{-1} A_r) \leq \| I -N_1^{-1} A_r \| \leq 1 $. 

Furthermore, we compute 
\begin{equation}
\begin{aligned}
      I - N^{-1} C & = I - \begin{pmatrix}
     (r K^T K)^{-1} &  0 \\
    r K (r K^T K)^{-1} & -r I
    \end{pmatrix} \begin{pmatrix}
    r K^TK & K^T \\
    K & 0
    \end{pmatrix} \\
    & = I - \begin{pmatrix}
    I &   (r K^T K)^{-1} K^T\\
     0 &  rK(rK^TK)^{-1}K^T 
    \end{pmatrix}\\
    & = \begin{pmatrix}
    0 & - (r K^T K)^{-1} K^T\\
    0 & I - K(K^TK)^{-1}K^T
    \end{pmatrix}
\end{aligned}
\end{equation}



\textcolor{red}{If $N^{-1}$ is a nonlinear solver, e.g. several steps of gradient descent or iterative solvers. 
}
 
\section{Appendix} 



 
\begin{algorithm}\label{alginexact4} \caption{Uzawa for $L_r$ with a single step Gradient Descent}
ADMM updates are as follows. 
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
Apply one step GD to find $X_{k+1}$ for $L_r$: 
\begin{equation}\label{gdADMM}
\nabla F(X_{k}) - H_k - r (Kz_k - X_{k+1}) = 0 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
K^T H_k + r K^T (Kz_{k+1} - X_{k+1}) = 0,         
\end{equation} }
\State{Update the Lagrange multiplier: \begin{equation} 
H_{k+1} - H_k - r (K z_{k+1} - X_{k+1}) = 0. 
\end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Assume that $K^TH_0 = 0$. Then Algorithm produces 
$Y_k = [\overline{z_k}; H_k]$ that is linearly convergent to the optimal solution  $Y_* = [\overline{z_*}; H_*]$ in the $C$-norm, defined by 
\begin{equation}
\|H_{k+1} - H_* \|^2 \leq \frac{1}{1+\delta} \| H_{k} - H_*\|^2,
\end{equation}
where $\delta$ is some positive parameter. Furthermore, $X_k$ is linearly convergent to the optimal solution $X_*$ in the following form
\begin{equation}
\|X_{k+1} - X_* \|^2 \leq \frac{1}{2 \lambda} \|H_{k} - H_* \|^2_C
\end{equation}
\end{theorem}

\begin{proof}
We have the following identity holds: 
\begin{eqnarray*}
\nabla F(X_{k}) - \nabla F(X_*) &=& r( K z_{k} - K {z_{k+1}}) + H_{k+1} - H_* \\
r M_1 (X_{k+1} - X_*) &=& - H_{k+1} + H_k \\
M_2 (X_{k+1} - X_*) &=& K(z_{k+1} - z_*)
\end{eqnarray*}
Let 
\begin{equation}
X = X_{k+1} - \frac{1}{r}H_k \quad \mbox{ and } \quad Y = X_* - \frac{1}{r}H_*. 
\end{equation}
We then see that 
\begin{eqnarray*}
\|Kz_{k+1} - X_* \|^2 &=& \|M_2(X) - M_2(Y)\|^2% \langle X_{k+1} - X_*, X_{k+1} - X_* \rangle \\ 
%&=& \left \langle Kz_k + \frac{1}{r} H_k - \frac{1}{r} \nabla F(X_k) - X_*, Kz_k + \frac{1}{r} H_k - \frac{1}{r} \nabla F(X_k)\right \rangle \\
%&=& \left \langle (X_k - X_*) - \frac{1}{r} (\nabla F(X_k) - \nabla F(X_*)), (X_k - X_*) - \frac{1}{r} (\nabla F(X_k) - \nabla F(X_*))\right \rangle \\
%&\leq& \left ( 1 - \frac{\lambda}{r} \right )  \|X_k - X_*\|^2. 
\end{eqnarray*}
On the other hand, we also observe that
\begin{eqnarray*}
&& \|H_{k+1} - H_* \|^2 = \langle H_{k} + r (Kz_{k+1} - X_{k+1}) - H_*, H_{k} + r (Kz_{k+1} - X_{k+1}) - H_* \rangle \\ 
&& = \left \langle r \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - H_*, r \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - H_* \right \rangle \\
&& = r^2 \left \langle \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \frac{1}{r} H_*, \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \frac{1}{r} H_* \right \rangle \\
&&= r^2 \left \langle \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \left ( X_* - \left (X_* - \frac{1}{r} H_* \right ) \right ), \left( Kz_{k+1} - \left ( X_{k+1} - \frac{1}{r} H_k \right ) \right ) - \left ( X_* - \left (X_* - \frac{1}{r} H_* \right ) \right ) \right \rangle \\
&&= r^2 \left \langle \left( M_2(X) - X \right ) - \left ( M_2(Y) - Y \right ),  \left( M_2(X) - X \right ) - \left ( M_2(Y) - Y \right ) \right \rangle \\
&&= r^2 \|(M_2(X) - X) - (M_2(Y) - Y)\|^2.  
%X_{k+1} - \frac{1}{r}H_k \right ) - \left (  X_* - \frac{1}{r} H_* \right ) \right \|^2 = r^2 \left \| \left ( X_{k+1} - X_* \right ) - \frac{1}{r} \left ( H_k - H_* \right ) \right \|^2 \\
%&&= r^2 \|X_{k+1} - X_*\|^2 - 2r^2 \left \langle X_{k+1} - X_*, \frac{1}{r} \left ( H_k - H_* \right ) \right \rangle + \|H_k - H_*\|^2.
%&&= r^2 \|X_{k+1} - X_*\|^2 - 2 \left \langle W_{k+1} - W_* - , r \left ( H_k - H_* \right ) \right \rangle + \|H_k - H_*\|^2. 
\end{eqnarray*}
Thus, we have that 
\begin{eqnarray*}
\|X_{k+1} - X_*\|^2 + \frac{1}{r^2} \|H_{k+1} - H_*\|^2 &\leq& \|X_{k+1} - \frac{1}{r}H_k - (X_* - \frac{1}{r} H_*))\|^2 \\
&=& \| (X_{k+1} - X_*) - \frac{1}{r}(H_k - H_*))\|^2 \\
&=& \| W_k - W_*\|^2. 
%- 2 \left \langle W_{k} - W_* + \frac{1}{r} \left ( H_k - H_* \right ), \frac{1}{r} \left ( H_k - H_* \right ) \right \rangle + \frac{1}{r^2} \|H_k - H_*\|^2 \\ 
%&=& \|W_{k} - W_*\|^2, 
\end{eqnarray*} 
where $W_k = X_k - \frac{1}{r} \nabla F(X_k)$. 
\end{proof}
\begin{remark}
The use of nonexpansiveness is to obtain $\nabla F(\cdot)$ from two different $\nabla F - H_k$ and $\nabla F - H_*$. 
\end{remark}

\section{$2\times2$ block system}
In this section, we focus on the convergence analysis for the $2\times2$ block system involving the solution of $X, z$ with $H$ being fixed. This is a subproblem from the previous $3\times3$ system.  

We consider convergence analysis of algorithms for the following system.

\begin{equation} \label{2by2}
\begin{aligned}
\nabla F(X_{*}) - H - r (Kz_{*} - X_{*}) &= 0, \\
K^T H + r K^T (Kz_{*} - X_{*}) &= 0.
\end{aligned}
\end{equation}
In operator notation, we have 
\begin{equation}
\begin{pmatrix}\label{2by2abstract}
A_r & B^T\\
B & C
\end{pmatrix} 
\begin{pmatrix}
X_*\\
z_*
\end{pmatrix}= 
\begin{pmatrix}
f  \\
g 
\end{pmatrix},
\end{equation}
where $A_r = \nabla F(\cdot) + r I $, $B = -r K^T$, $C = rK^T K $ $f = H, g = -K^T H$. 


\begin{algorithm}
\caption{Block Gauss Seidel for $2 \times 2$ system \eqref{2by2}}
\label{algexact5}
\begin{algorithmic}
\State{Given initial $X_0, z_0$.}
\For{$k=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
\begin{equation}
\nabla F(X_{k+1}) - H - r (Kz_k - X_{k+1}) = 0 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
K^T H + r K^T (Kz_{k+1} - X_{k+1}) = 0,
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
For general $\lambda-$strongly convex and $L-$smooth objective function $F(X)$, we have for Algorithm \ref{algexact5}, 
\begin{equation}
    \|X_{k+1} - X_{*} \| \leq \frac{r}{r + \lambda} \| \bar{z}_k - \bar{z}_* \|.
\end{equation}
\end{theorem}

\begin{proof}
From the updates in Algorithm \ref{algexact5} and the optimality condition, we obtain the error equation as follows. 
\begin{eqnarray}
 \nabla F(x_{k+1}) - \nabla F(x_*) & = & r (\bar{z}_k - \bar{z}_*) - r(X_{k+1} - X_*)
%  a new line & = & continues
\end{eqnarray}
By $\lambda-$strong convexity, we have 
\begin{equation*}
\begin{aligned}
    \lambda \|X_{k+1} - X_* \|^2 &\leq \langle X_{k+1} - X_*, \nabla F(X_{k+1}) - \nabla F(X_*) \rangle \\
    & = \langle X_{k+1} - X_*, r(\bar{z}_k - \bar{z}_* )\rangle - \langle X_{k+1} - X_*, r (X_{k+1} - X_*) \rangle 
\end{aligned} 
\end{equation*}
Rearranging and applying Cauchy-Schwarz inequality, we can obtain the required inequality. 
\end{proof}

% \begin{lemma}
% For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the block Gauss Seidel iteration in Algorithm \ref{algexact5} corresponds to the exact Uzawa method with the following ierative method for the Schur complement: 
% \begin{equation}
%     z_{k+1} = z_{k} + N^{-1} (g - B X_{k+1} - Cz_{k}),
% \end{equation}
% where $N = C$. 
% \end{lemma}

\begin{theorem}
For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the block Gauss Seidel iteration in Algorithm \ref{algexact5} corresponds to the exact Uzawa method with the following ierative method for the Schur complement: 
\begin{equation}
    z_{k+1} = z_{k} + N^{-1} (g - B X_{k+1} - Cz_{k}),
\end{equation}
where $N = C$. 

Furthremore, we have the following convergence result
\begin{eqnarray}
     \| z_{k+1} - z_*\| &\leq& \left( \frac{r}{r +c_0}\right)^{k+1 } \| z_0 -z_*\|, \\
    \|X_{k+1} -X_* \| &\leq& \frac{r}{r + \lambda} \left( \frac{r}{r + c_0}\right)^k \|z_0 - z_* \|,
\end{eqnarray}
where $c_0$ is the smallest eigenvalue of $A$. 
\end{theorem}
\begin{proof}
Note that $C = rK^TK$ is invertible. A direct calculation can show that it is indeed same as the update in $z_{k+1}$ in Algorithm \ref{algexact5}. 

For simplicity, we absorb $b$ into the right hand side $f$. 

Konwing that 
\begin{equation}
    X_{k+1} = A_r^{-1} f - A_r^{-1} B^T z_{k},  
\end{equation}
we eliminate $X_{k+1}$ from the construction of $z_{k+1}$ to have the iteration 
\begin{equation}
    z_{k+1} = z_{k} + C^{-1} \left (g - B A_r^{-1}f - (C - B {A_r}^{-1} B^T) z_k \right),
\end{equation}
which is the iteration applied to the Schur complement system: 
\begin{equation}
    (C - B {A_r}^{-1} B^T) z = g - B A^{-1} f.
\end{equation}
The convergence in error $\| z_{k+1}-z_* \|$ depend on  $\rho( I - C^{-1} (C - B {A_r}^{-1} B^T))$.
\begin{equation*}
\begin{split}
\rho( I - C^{-1} (C - B {A_r}^{-1} B^T)) &=  \rho ( C^{-1} B {A_r}^{-1} B^T) \\
&\leq \rho( (rK^T K)^{-1} rK^T (A + rI)^{-1} rK )   \\ 
& \leq  \frac{r}{r + c_0}.
\end{split}
\end{equation*}
Therefore, we have 
\begin{equation}
    \| z_{k+1} - z_*\| \leq \left( \frac{r}{r +c_0}\right)^{k+1 } \| z_0 -z_*\|. 
\end{equation}

\end{proof}

\begin{algorithm}
\caption{Inexact Uzawa for $2 \times 2$ system \eqref{2by2abstract} }
\label{algexact6}
\begin{algorithmic}
\State{Given initial $X_0, z_0$.}
\For{$k=0, 1,2,\cdots,T-1$}
\State{Update of $X_{k+1}$: 
\begin{equation}
X_{k+1} = X_k + \omega I (f - A_r X_k - B^T z_k), 
\end{equation}}
\State{Update of $z_{k+1}$: 
\begin{equation} 
z_{k+1} = C^{-1} (g - B X_{k+1} - Cz_k)
\end{equation} }
\EndFor
\end{algorithmic}
\end{algorithm}

For Algorthm \ref{algexact6}, we define $E^X_{k} = X_k - X_*$, $e^z_{k} = z_k - z_*$ the error equations are given by 
\begin{eqnarray}
E^X_{k+1} &=& E^X_{k} + wI\left( -A_rE^X_{k} - B^Te^z_k \right) \label{erroreqn1}\\
e^z_{k+1} &=& -C^{-1}B E^X_{k+1} \label{erroreqn2}
\end{eqnarray}
    
\begin{theorem}
For a special quadratic objective $F(x) = \frac{1}{2} X^T A X - b^TX$, with $A$ being SPD, the Algorithm \ref{algexact5} has the following convergence results for some $\eta<1$ if $\omega$ is sufficiently small

\begin{equation}
    \|X_{k+1} - X_*\| \leq \eta^{k+1} \|X_{0} - X_* \| 
\end{equation}

\begin{equation}
    \|Kz_{k+1} - K z_*\| \leq \eta^{k+1} \|X_{0} - X_* \| 
\end{equation}
\end{theorem}

\begin{proof}
Substituting \eqref{erroreqn2} into \eqref{erroreqn1}, we have 
\begin{equation}
    E^X_{k+1} = E^X_{k} + wI\left( -A_rE^X_{k} + B^T C^{-1} B E^X_{k} \right)
\end{equation}
Then convergence of $\|E^X_{k+1} \|$ depends on $\rho(I  - w ( A_r - B^T C^{-1}B) )$. 

Note that $\lambda_{\text{min}}(A_r) = c_0 + r$, and $B^T C^{-1} B = r K (K^TK)^{-1} K^T$, which is symmetric and has spectral radius $r$. We have $A_r - B^T C^{-1} B$ is symmetric positive definite. Therefore, by choosing sufficiently small step size $\omega$ for the Richardson's iteration, we have for some $\eta < 1$
\begin{equation}
    \|E^X_{k+1}\| \leq \eta^{k+1} \|E^X_{0} \| 
\end{equation}

Furthermore, multiplying \eqref{erroreqn2} by $K$ and considering the norm, we have 
\begin{equation}
    \begin{aligned}
       \|Ke^z_{k+1} \| &= \|KC^{-1}B E^X_{k+1}\| \\
        & = \| K(K^T K)^{-1} K^T  E^X_{k+1}\| \\
        & \leq \|E^X_{k+1}\|  \leq \eta^{k+1} \|E^X_{0} \| 
    \end{aligned}
\end{equation}
\end{proof}




\begin{algorithm}\label{alg:inexactADMM2}
\caption{ADMM for $L_r$ with GD with for the first step}
ADMM updates are as follows. 
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_{t+1}$ update: 
    solve $\nabla F(X_{t+1}) - H_t - r (Kz_t - X_{t+1}) = 0$ using gradient descent as follows.
    \For{$ k = 0,..., K-1$}
      \begin{equation}\label{gd for ADMM}
        \begin{split}
            X_{t+\frac{k+1}{K}} & = X_{t+\frac{k}{K}} - \lambda \left(\nabla F(X_{t+\frac{k}{K}})  - H_k - r(K z_k - X_{t + \frac{k}{K}}) \right) \\
            & = (1 - \lambda r)X_{t+\frac{k}{K}} + \lambda r K z_k - \lambda \left(\nabla F(X_{t+\frac{k}{K}})  - H_k \right)
        \end{split}
    \end{equation}
    \EndFor
    }
    \State{$z_{t+1}$ update: 
        \begin{equation} 
        K^T H_k + r K^T (Kz_{t+1} - X_{t+1}) = 0,         
    \end{equation} }
    \State{Update the Lagrange multiplier:    
    \begin{equation} 
        H_{t+1} - H_t - r (K z_{t+1} - X_{t+1}) = 0. 
    \end{equation}}
\EndFor
\end{algorithmic}
\end{algorithm}



\textcolor{red}{How to analyze the convergence rate in the following two cases?}

\textcolor{red}{If we consider $X$ part, and consider $z$, $H$ part together, then this is an exact Uzawa, with an iterative method for the Schur complement. See Propostition \ref{prop: Uzawa iterative solver}. }

\textcolor{red}{If we consider the $X$, $z$ together, then it is an inexact Uzawa with the first block solved by one step of block GS and the second part solved by Richardson iteration.}


\newpage 

\begin{section}{Uzawa iterations}
This section contains mathematical theory on Uzawa iterations that are related to our problem. 
\end{section}

\section{Introduction}

Data becomes increasingly decentralized and the privacy of individual data is an utmost importance in the digital age \cite{house2012consumer, cai2021deepstroke,chen2020ai,luo2020arbee,wang2020panel}. Unlike standard machine learning approaches, \textit{Federated learning} (FL) encourages each client to have a local training data set, which will not be stored to the server and  to update the local correction of the current global model maintained by the main server via the local data and local gradient descent method. {FL} has been used successfully in many different areas, which include Internet of Things (IoT) applications \cite{hwang2015iot, ferrag2021federated}. {FL} can be modeled as the optimization problem given as 
\begin{equation}\label{FL}
\min_{x \in X} \left \{ E(x) = \sum_{k=1}^N f_k (x) \right \},
\end{equation} 
where $X$ is a parameter space, $N$ is the number of clients or devices, and $f_k \colon X \rightarrow \mathbb{R}$, $1 \leq k \leq N$, is a local objective function for the $k^{\rm th}$ worker. The local objective function $f_k$ depends on the data of the $k^{\rm th}$ worker, but not on those of the other clients. The standard Federated Avgerage ({\textit{FebAvg}}) algorithm consists of three steps
\begin{enumerate}
\item  the central server broadcasts the latest model $x_t$, to all the clients;
\item every worker, say $k^{\rm th}$ worker, lets $x_t^k = x_t$ and then performs one or few local updates with learning rate $\gamma$ 
  $$x_{t+1}^k \leftarrow x_t^k - \gamma \nabla f_k(x_t^k)$$
\item the server then aggregates the local models, $x_{t+1}^1, \cdots x_{t+1}^N$, to produce the new global model $x_{t+1}$ \cite{konevcny2016federated}.
  \end{enumerate}

Figure \ref{fig:scheme} illustrates a simple single local gradient descent algorithm (Local GD) and the arrows indicate the communications, which poses a bottleneck of the algorithm because it is generally orders of magnitude more expensive than the local computations and more communications make the algorithm more vulnerable for cybersecurity. Recent methods, therefore, aim at enhancing the privacy of FL by using reduced model or even sacrificing system efficiency. However, {\textbf{providing privacy has to be carefully balanced with system efficiency}} \cite{li2020federated}. Figure \ref{fig:scheme2} shows that the Local GD without applying the shifted gradient can reach the lower accuracy faster, but it does not converge eventually. One recent algorithm, called Scaffnew or ProxSkip, is shown to achieve the best communication efficiency, without sacrificing the convergence property, until today \cite{mishchenko2022proxskip}. Scaffnew reformulates ~\cref{FL} as follows:
\begin{equation}\label{cp}
\min_{x_1, \cdots, x_N \in X} \left \{ \frac{1}{N} \sum_{i=1}^N f_i(x) + \psi(x_1,\cdots,x_N) \right \},
\end{equation} 
where $\psi$ is a proper closed convex function introduced for a consensus reformulation, which can be interpreted as some average of parameters $x_1,\cdots,x_N$, from each client. Then it applies the proximal gradient descent method. The novelty of Scaffnew is at the introduction of certain shift, denoted by $h_t$, leading to the modified grandient, i.e., $\widetilde{\nabla}f(x_t) = \nabla f(x_t) - h_t$. This can be considered as a type of the preconditioner, which leads to the solution for the consensus reformulation even with applying the proxy operator once in a while. 

%The focus of our tasks is to develop mathematical foundation of FL and to design effective training algorithms for FL using ideas from numerical analysis and mathematical optimization so as to push the boundaries of the current state of the art in FL \cite{li2019convergence,zhou2022convergence,haddadpour2019convergence,mitra2021linear}. 

The focus of this proposal is to push the boundary of theoretical convergence analysis for the currently available for {\textit{FebAvg}}. The convergence theory of Scaffnew was established for strongly convex problems \cite{mishchenko2022proxskip}. We also present improved FL models that can provide better privacy without sacrificing convergence property of scheme. These two goals will be achieved by viewing \textit{FebAvg} within the framework of subspace correction methods.  

To expedite the success of the proposed studies, we will team up with people of expertises in numerical analysis, mathematical optimization, and machine learning. The project leader Jinchao Xu will closely collaborate with Young Ju Lee (Texas State) and other close collaborators such as Qingguo Hong (Penn State), Xiaofeng Xu (Penn State) and Jongho Park (KAIST).  J. Xu's research interests include mathematical foundation of machine learning,  approximation theory for deep neural networks, design of convolutional neural networks from multigrid viewpoint, and development of training algorithms based on subspace correction --- a general framework containing a large class of optimization algorithms such as coordinate descent method and federated learning. Lee has worked on successive subspace corrections for nontrivial problem, such as nearly or singular problem, which fits in the current project since objective functionals are typically nearly singular \cite{chen2020robust,lee2009robust,LWXZ:2007}. Hong has an expertise in analysis of preconditioners for coupled nonlinear systems \cite{hong2016uniformly,hong2016robust,chen2020robust}. 
Park has several interesting results on parallel subspace correction methods for mathematical optimization problems~\cite{Park:2020,Park:2021,Park:2022}.



We consider to solve 
\begin{equation} 
\min_{x \in \Reals{d}} f(x) + \psi(x), 
\end{equation}
where $f : \Reals{d} \rightarrow \Reals{}$ is a smooth function and $\psi : \Reals{d} \rightarrow \Reals{} + \{+\infty\}$ is a proper, closed and convex regularizer. 

We consider the constrained optimization. Namely, for some $\mathcal{C} \subset \Reals{d}$, we consider to minimize  
\begin{equation}\label{main:eq}  
\min_{x \in \mathcal{C}} f(x) \quad \mbox{ or equivalently, } \quad \min_{x \in \Reals{d}} f(x) \quad \mbox{ subject to } x \in \mathcal{C}.  
\end{equation}
By defining $\psi : \Reals{d} \mapsto \Reals{}$ by 
\begin{equation}
\psi(x) :=  \left \{ \begin{array}{cc} 
0, & \mbox{ if } x \in \mathcal{C} \\
+\infty, & \mbox{ otherwise } 
\end{array} \right . 
\end{equation}
the problem \eqref{main:eq} can be formulated into 
\begin{equation} 
\min_{x \in \Reals{d}} f(x) + \psi(x).  
\end{equation}

\newpage 


\section{Federated learning and ADMM}
Notation:

$x \in \mathbb{R}^d$, $X = (x_1, x_2 ,\dots,x_n) \in \mathbb{R}^{d\times n}$, where $x_i \in \mathbb{R}^d$.

In federated learning, the objective function is $f(x) = \frac{1}{n} \sum_{i = 1}^n f_i(x)$,where each $f_i$ is the local objective function for each client $i$. 

We have the following formulations of the minimization problem in federated learning. 

\begin{itemize} 
    \item Original formulation \begin{equation}\label{Fed: original}
        \min_{x \in \mathbb{R}^d} f(x) = \frac{1}{n} \sum_{i = 1}^n f_i(x)
    \end{equation} 
    \item 
    \begin{equation}
        \min_{\substack{X \in \mathbb{R}^{dn} \\x1 = x2 = \cdots = x_n }} F(X) = \frac{1}{n} \sum_{i = 1}^n f_i(x_i)
    \end{equation}
    \item Consensus formulation
        \begin{equation}  \label{Fed: concensus}
        \min_{X \in \mathbb{R}^{dn}} F(X) + \psi(X),
    \end{equation}
    where 
    \begin{equation}\label{psi}
    \psi(x) =  \left \{ \begin{array}{cc} 
    0, & \mbox{ if }  x_1 = x_2 = \cdots = x_n \\
    +\infty, & \mbox{ otherwise } 
    \end{array} \right . 
    \end{equation}
\item 
    \begin{equation}
    \min_{ \substack{X,Z \in \mathbb{R^{dn}} \\ X = Z}} F(X) + \psi(Z)
    \end{equation}
\item Lagrange multiplier method
    \begin{equation} \label{Fed: Lagrange}
        \min_{X,Z \in \mathbb{R}^{dn}} \max_{h \in \mathbb{R}^{dn} } F(X) + \psi(Z) + \langle H, Z- X \rangle
    \end{equation}
\item Introduce the $K = [I,...,I]^T$, where $I \in \mathbb{R}^{d\times d}$ is the identity matrix.
\begin{equation}
    \min_{ \substack{ X \in \mathbb{R}^{dn}, z \in \mathbb{R}^d \\X = Kz }} F(X)
\end{equation}
\item Using tensor notation, $\mathds{1} = [1,1,\dots, 1]^T $
\begin{equation}
    \min_{ \substack{X \in \mathbb{R}^{dn}, z \in \mathbb{R}^d \\ X = \mathds{1} \otimes z  } } F(X)
\end{equation}
\item Lagrange multiplier method 
\begin{equation}
    \min_{X \in \mathbb{R}^{dn},z \in \mathbb{R}^d} \max_{H \in \mathbb{R}^{dn}} F(X) + \langle H,Kz - X \rangle 
\end{equation}
\end{itemize}
\newpage

\begin{algorithm}
\caption{Federated Learning for $f(x)$}\label{alg:FedLearing}
Given a stepsize $\gamma > 0$, initial iterate $x_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following ($x_i,t$ is the local copy of the parameter for client i):  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{For each client i: $x^0_{i,t} = x_t$}
    \For{$k = 0, 1,2, \cdots, K-1$}
    \State{$x_{i,t +\frac{k+1}{K}} = x_{i,t + \frac{k}{K}} - \gamma \nabla f_i(x_{i,t+\frac{k}{K}})$}
    \EndFor
    \State{$x_{t+1} =\frac{1}{n} \sum_{i = 1}^n x_{i,t+1} $} 
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{remark}
For K = 1, this simply reduces to the usual GD for f(x).
\end{remark}

\begin{lemma}\label{lemma: prox psi}
Given $X = (x_1, x_2, \dots, x_n)$, we have 
\begin{equation}
    {\rm prox}_{a\psi}(X) = (\bar{x},\dots, \bar{x} ),
\end{equation}
where $\bar{x} =\frac{1}{n} \sum_{i = 1}^n x_i$, $\psi$ is given in \eqref{psi}, $a$ is a nonzero constant. 
\end{lemma}
\begin{proof}
By definition, we have 
\begin{equation*}
\begin{split}
        \text{prox}_{a\psi} (X) &= \mathop{\arg \min}_{Y} \psi(Y) + \frac{1}{2a} \| X - Y \|^2 \\
         & = \mathop{\arg \min}_{Y : y_1 = \cdots =y_n = \alpha } \sum_{i = 1}^n \| x_i - \alpha\|^2 = (\bar{x},\dots , \bar{x}),
\end{split}
\end{equation*}
where $\bar{x} =\frac{1}{n}\sum_{i=1}^n x_i$. 
\end{proof}

Thanks to Lemma \ref{lemma: prox psi}, we have the following consensus form of federated learning.

\begin{algorithm}
\caption{Consensus Federated Learning for $F(X) + \psi(X)$}\label{alg:FedLearing consensus}
Given a stepsize $\gamma > 0$, initial iterate $X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    % \State{$X^0_t = X_t$}
    \For{$k = 0, 1,2, \cdots, K-1$}
    \State{$X_{t +\frac{k+1}{K}} = X_{t + \frac{k}{K}} - \gamma \nabla F(X_{t + \frac{k}{K}})$}
    \EndFor
    \State{$X_{t+1} = \text{prox}_{\gamma \psi} (X_{t+1})$} 
\EndFor
\end{algorithmic}
\end{algorithm}

Using our notation, we now introduce the ProxSkip algorithm. See algorithm~\ref{alg:ProxSkip}. 

\begin{algorithm}
\caption{ProxSkip}\label{alg:ProxSkip}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, $h_0$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{$X_{t+1} = Z_{t} - \gamma (\nabla F(Z_{t}) -h_t ) $}
    \State{Flip a coin $\theta_t$, $P(\theta_t = 1) = p $}
    \If{$\theta_t = 1$} 
    \State{$Z_{t+1} = {\rm prox}_{\frac{\gamma}{p}\psi } 
    \left ( X_{t+1} - \frac{\gamma}{p} h_t \right )$} 
    \Else
        \State{$Z_{t+1} = X_{t+1}$}
    \EndIf 
    \State{$h_{t+1} = h_t + \frac{p}{\gamma} (Z_{t+1} - X_{t+1})$} 
\EndFor
\end{algorithmic}
\end{algorithm}

A deterministic version of ProxSkip is given in algorithm \ref{alg:ProxSkip deterministic}. 

\begin{algorithm}
\caption{ProxSkip Deterministic (SCAFFOLD)}\label{alg:ProxSkip deterministic}
Given a stepsize $\gamma > 0$, initial iterate $Z_0 = X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \For{$k = 0, 1,\dots, K-1$}
    \State{$X_{t + \frac{k+1}{K}} = Z_{t + \frac{k}{K}} - \gamma ( \nabla F(Z_{t + \frac{k}{K}} ) - h_t)$ }
    \State{$Z_{t + \frac{k+1}{K}} =X_{t + \frac{k+1}{K}} $}
    \EndFor    
    \State{$Z_{t+1} = \text{prox}_{K\gamma \psi}(X_{t+1} - K\gamma h_t)$}
    \State{$H_{t+1} = H_t + \frac{\gamma}{K} (Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}

Now we look at the Lagrange multiplier formulation~\eqref{Fed: Lagrange}. The Lagrangian is given by 
\begin{equation}\Label{lagrangian}
    L(X,Z,h) = F(X) +\psi(Z) + \langle H,Z-X \rangle
\end{equation}
The augmented Lagrangian is given by 
\begin{equation}\Label{Augmented lagrangian}
    L_r(X,Z,h) = F(X) +\psi(Z) + \langle H,Z-X \rangle + \frac{r}{2} \|Z-X \|^2
\end{equation}

The famous ADMM method performs the following three steps
\begin{itemize} 
    \item ADMM step 1
    \begin{equation}
        X_{t+1} = \mathop{ \arg \min }_{X} L_r({X,Z_{t}, H_t})
    \end{equation} 
    
    \begin{equation}
    L_r(X,Z_t,H_t) = F(X) +\psi(Z_t) + \langle H_t,Z_t-X \rangle + \frac{r}{2} \|Z_t-X \|^2
    \end{equation}
    
    \begin{equation}
        \nabla F(X_{t+1}) - H_t + r (X_{t+1} - Z_t)
    \end{equation}

    \item ADMM step 2
    \begin{equation}
        Z_{t+1} = \mathop{ \arg \min }_{Z} L_r({X_{t+1},Z, H_t})
    \end{equation}
    
    \begin{equation}
    L_r(X,Z,H) = F(X_{t+1}) +\psi(Z) + \langle H_t,Z-X_{t+1} \rangle + \frac{r}{2} \|Z-X_{t+1} \|^2
    \end{equation}
    
    \item ADMM step 3
    \begin{equation}
        H_{t+1} = H_t + r (Z_{t+1} - X_{t+1})
    \end{equation}
    
    \begin{equation}
    L_r(X,Z,H) = F(X) +\psi(Z) + \langle H,Z-X \rangle + \frac{r}{2} \|Z-X \|^2
\end{equation}
\end{itemize}

From this viewpoint, we can derive various algorithms for the Federated learning. 

\begin{algorithm}
\caption{Exact ADMM for $L_r$}\label{alg:ADMM exact}
Given a stepsize $\gamma > 0$, initial iterate $X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{Solve using exact solver $X_{t+1} = \mathop{ \arg \min }_{X} L_r({X,Z_{t}, H_t})$, namely $X_{t+1}$ satisfies\\
    \begin{equation}
        \nabla F (X_{t+1}) - H_t +r(X_{t+1} - Z_t) = 0
    \end{equation} }
    \State{Solve $Z_{t+1} = \mathop{ \arg \min }_{Z} L_r({X_{t+1},Z, H_t})$ using exact solver , namely\\
    \begin{equation}
        Z_{t+1} = \text{prox}_{\frac{\psi}{r}} (X_{t+1} - \frac{1}{r} h_t)
    \end{equation} }
    
    \State{Update the Lagrange multiplier: $H_{t+1} = H_t + r (Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{lemma}\label{lemma: equivalence of admm step 2 and prox}
The second minimization problem in the ADMM method 
\begin{equation}
    Z_{t+1} = \mathop{\arg \min}_Z L_r (X_{t+1})
\end{equation}
is equivalent to the following proximal map 
\begin{equation}
    Z_{t+1} = \text{prox}_{\frac{\psi}{r}}(X_{t+1} - \frac{1}{r} h_t) 
\end{equation}
\end{lemma}
\begin{proof}
\begin{equation}
    \begin{split}
         \mathop{\arg \min}_Z L_r (X_{t+1},Z,H_t)
         &=  \mathop{\arg \min}_Z \psi(Z) + \langle H_t, Z - X_{t+1}\rangle+ \frac{r}{2} \| Z- X_{t+1} \|^2 \\
         & =  \mathop{\arg \min}_Z  \psi(Z) + \frac{r}{2} \| Z - X_{t+1} + \frac{1}{r}H_t  \|^2 \\
         & = \text{prox}_{\frac{\psi}{r}} (X_{t+1} - \frac{1}{r} H_t)
    \end{split}
\end{equation}
\end{proof}

\begin{algorithm}
\caption{Inexact ADMM for $L_r$}\label{alg:ADMM inexact}
Given a stepsize $\gamma > 0$, initial iterate $X_0 = (x_0, \dots, x_0) \in \mathbb{R}^{dn}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=0, 1,2,\cdots,T-1$}
    \State{Solve $X_{t+1} = \mathop{ \arg \min }_{X} L_r({X,Z_{t}, h_t})$ approximated, e.g. using  K iterations of GD: }
    \For{$k = 0,1,\dots, K-1$}
    \State{$X_{t + \frac{k+1}{K}} = X_{t+\frac{k}{K}} - \gamma \bigg( \nabla F(X_{t+\frac{k}{K}}) -H_t - r(Z_{t} -X_{t+\frac{k}{K}}) \bigg)$}
    \State{\textcolor{blue}{$X_{t + \frac{k+1}{K}} = Z_t - \frac{1}{r} \bigg( \nabla F(X_{t+\frac{k}{K}}) -H_t \bigg)$ ~~(if we a stepsize $\gamma = \frac{1}{r}$)} }
    \EndFor
    \State{Solve $Z_{t+1} = \mathop{ \arg \min }_{Z} L_r({X_{t+1},Z, h_t})$ using exact solver, namely 
    \begin{equation}
        Z_{t+1} = \text{prox}_{\frac{\psi}{r}} (X_{t+1} - \frac{1}{r} h_t).
    \end{equation}
    }
    \State{Update the Lagrange multiplier: $h_{t+1} = h_t + r (Z_{t+1} - X_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}




\newpage
\subsection{Proximal gradient descent: ProxGD} 

ProxGD, known as forward-backward algorithm is the basis. For the forward step, we apply the gradient descent algorithm for the objective functional $f(x)$, i.e., 
\begin{equation}
\widehat{x}_{t+1} = x_t - \gamma_t \nabla f(x_t). 
\end{equation}
Here $\gamma_t > 0$ is a suitably chosen stepsize at time $t$, while $s_t = -\nabla f(x_t)$ is the search direction. 
On the other hand, due to the other functional $\psi(x)$, we need to adjust it. Namely, we need to apply certain projection. This generates the following:
\begin{equation}
x_{t+1} = {\rm prox}_{\textcolor{red}{\gamma_t}  \psi} (\widehat{x}_{t+1}) = {\rm prox}_{\textcolor{red}{\gamma_t} \psi} (x_t - \gamma_t \nabla f(x_t)). 
\end{equation}
 and 
\begin{equation}
{\rm prox}_{\gamma \psi(\cdot)} : \Reals{d} \rightarrow \Reals{d}    
\end{equation}
is the proximity operator of $\psi$, defined via 
\begin{equation}
{\rm prox}_{\gamma \psi}(x) = {\rm arg}\min_{y \in \Reals{d}} \left [ \frac{1}{2} \|y - x\|^2 + \gamma \psi(y) \right ]. 
\end{equation}
In summary, we have write the ProxGD algorithm as follows: 

\begin{algorithm}
\caption{ProxGD}\label{alg:maingd}
Given a stepsize $\gamma > 0$, initial iterate $x_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=1,2,\cdots,T-1$}

\State{$\widehat{x}_{t+1} = x_t - \gamma (\nabla f(x_t))$} 

%\State{Flip a coin $\theta_t \in \{0,1\}$ where ${\rm Prob}(\theta_t = 1) = p$}
%
%\If{$\theta_t = 1$} 
\State{$x_{t+1} = {\rm prox}_{\gamma \psi}  
\left ( \widehat{x}_{t+1} \right )$} %
%\Else
%    \State{$x_{t+1} = \widehat{x}_{t+1}$}
%    
%\EndIf 
%
%\State{$h_{t+1} = h_t + \frac{p}{\gamma}(x_{t+1} - \widehat{x}_{t+1})$}
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{remark}
Typically, the proximity operator is assumed to be evaluated in closed form. ProxGD is most suited to situations when the proximity operator is relatively cheap to evaluate. Therefore, the bottleneck is at the computation of $\nabla f$, than at the computation of the proximity operator. 
\end{remark}


\section{The update of the Lagrange multiplier for the federated learning algorithm} 

We denote the vector $X \in \mathbb{R}^{nd}$ as follows: 
\begin{equation}
X = \left ( \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_d \end{array} \right ) \in \mathbb{R}^{nd}, \quad \mbox{ where } x_j \in \mathbb{R}^{n}, \mbox{ and } \forall i=1,\cdots,d. 
\end{equation}
We now introduce a matrix $K \in \mathbb{R}^{d} \rightarrow \mathbb{R}^{nd \times d}$ defined by the following relation: 
\begin{equation} 
K = \left ( \begin{array}{c} I_d \\ I_d \\ \vdots \\ I_d \end{array} \right ), 
\end{equation}
where $I_d \in \mathbb{R}^{d\times d}$ is the identity matrix. Therefore, we see that for $z \in \mathbb{R}^{d}$, we have 
\begin{equation} 
K z = \left ( \begin{array}{c} z \\ z \\ \vdots \\ z \end{array} \right ) \in \mathbb{R}^{nd}. 
\end{equation} 
With $V = \mathbb{R}^{nd} \times \mathbb{R}^d$, we now consider to solve 
\begin{equation} 
\min_{X \in V } F(X) + H^T(Kz - X) + \frac{r}{2} \|Kz - X\|^2. 
\end{equation} 
This problem can be formulated as a saddle problem as follows: 
\begin{equation}\label{main:eq} 
\max_{H \in \mathbb{R}^{nd}}  \min_{X \in V} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}
\end{equation}
We define $g(H)$ as follows: 
\begin{equation}\label{gfunction}
g(H) = \min_{X} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}. 
\end{equation}
We shall now show how the Lagrange multiplier should be updated in the following Lemma. 
\begin{lemma}
Let $F$ be closed and convex and for a fixed $H_{k-1}$ and $z = z_k$, let 
\begin{equation} 
X_k = {\rm arg}\min_{X} ( F(X) + H_{k-1}^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2).  
\end{equation} 
Then, it holds that 
\begin{equation} 
\partial g(H_{k-1}) = Kz_k - X_k,  
\end{equation}
where $g$ is defined in \eqref{gfunction}. 
Furthermore, we have that
\begin{equation} 
\partial F(X_k) - (H_{k-1} + r (K z_k - X_k)) = 0. 
\end{equation} 
Therefore, the Lagrange multiplier has to be updated via the following formular: 
\begin{equation}
H_k = H_{k-1} + r (Kz_k - X_k). 
\end{equation}
\end{lemma}
\begin{proof} 
We define $G(X) = F(x) + \frac{r}{2} \|Kz - X\|^2$ and recall that the conjugate of $G$ denoted by $G^*$ is defined as follows: 
\begin{equation}
G^*(Y) := \max_{X} Y^T X - G(X). 
\end{equation}
Therefore, we see that 
\begin{eqnarray*} 
g(H) &=& -\max_X \left \{ -G(X) - H^T (Kz - X)  \right \} \\ 
&=&  -\max_X \left \{ -G(X) - (K^T H)^T z + H^TX \right \} \\
&=&  - \max_X \left \{ H^T X - G(X) \right \} + H^T K z \\
&=& - G^*(H) + H^TKz. 
\end{eqnarray*} 
The dual variable is then to satisfy the following maximum optimization: 
\begin{equation} 
\max_H g(H). 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
H_k = H_{k-1} + t_k \partial g(H_{k-1}), 
\end{equation} 
where $t_k \geq 0$. We note that $H_{k-1}$ produces $X_k$ as the minimizer given as follows: 
\begin{equation} 
X_k = {\rm arg}\min_Y \left ( G(Y) + H_{k-1}^T (K z_k - X) \right ).  
\end{equation} 
We now recall the well-known fact that if $G$ is closed and convex, then 
\begin{equation}
Y \in \partial G(X) \quad \Leftrightarrow \quad X \in \partial G^*(Y) \quad \Leftrightarrow \quad X \in {\rm arg}\min_Z G(Z) - Y^T Z.
\end{equation}
Using this fact, we shall show that $\partial g(H_{k-1}) = Kz_k - X_k$. First, we note that 
\begin{equation}
\partial g(H) = \partial G^*(H) + Kz. 
\end{equation}
Therefore,  if $X \in \partial G^*(H)$, then $X = {\rm arg}\min_Z G(Z) - H^TZ$. Namely, we have that 
\begin{eqnarray*}
H_{k-1}^T Kz_k + {\rm arg}\min_Z \left \{ G(Z) + H_{k-1}^T Z \right \} &=& G(X_k) + H_{k-1}^T (Kz_k - X_k) \\
&=& F(X_k) + (H_{k-1}^T (Kz_k - X_k) + \frac{r}{2}\|Kz_k - X_k\|^2. 
\end{eqnarray*}
Thus, we have that
\begin{equation}
\partial g(H_{k-1}) = Kz_k - X_k. 
\end{equation}
This completes the proof. 
\end{proof} 

\newpage 
\subsection{ProxSkip} 
Generally, the proximity operator is difficult to apply and thus, we want to skip this procedure and apply it only every once in a while. Therefore, the convergence of the ProxSkip requires some additional trick, which is basically the modification of the $\nabla f$ term. 

\begin{algorithm}
\caption{ProxSkip}\label{alg:mainskip}
Given a stepsize $\gamma > 0$, probability $p > 0$, initial iterate $x_0 \in \Reals{d}$, initial control variate $h_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=1,2,\cdots,T-1$}

\State{$\widehat{x}_{t+1} = x_t - \gamma (\nabla f(x_t) \textcolor{red}{- h_t})$} 

\State{Flip a coin $\theta_t \in \{0,1\}$ where ${\rm Prob}(\theta_t = 1) = p$}

\If{$\theta_t = 1$} 
    \State{$x_{t+1} = {\rm prox}_{\frac{\gamma}{p}} \psi 
\left ( \widehat{x}_{t+1} \textcolor{red}{- \frac{\gamma}{p} h_t} \right )$} 
\Else
    \State{$x_{t+1} = \widehat{x}_{t+1}$}
    
\EndIf 

\State{$\textcolor{red}{h_{t+1} = \textcolor{blue}{h_t} + \frac{p}{\gamma}(x_{t+1} - \widehat{x}_{t+1})}$}
\EndFor
\end{algorithmic}
\end{algorithm}
 

\begin{remark}
The term $h_t$ is called the control variate and it plays a role of shiting the gradient $\nabla f(x_t)$ when the forward step is performed. We note that at the convergence, we have that $x_* = \widehat{x}_*$ and thus, the consensus is achieved and it holds that 
\begin{equation}
x_* = x_* - \gamma (\nabla f(x_*) - h_*).
\end{equation}
Therefore, $h_* = \nabla f(x_*).$
\end{remark}

\subsection{ProxSkip vs ADMM for $p=1$} 

For $p = 1$, we shall now prove that this is nothing else than ADMM. We consider the following minimization problem: 
\begin{equation} 
\min_{x,z} f(x) + \psi(z) \quad \mbox{ subject to } \quad z - x = 0. 
\end{equation}
This problem is equivalent to the original problem. The augmented Lagrangian can be defined as follows for a given parameter $\delta > 0$: 
\begin{equation} 
L_\rho(x,z,h) = f(x) + \psi(z) + h (z - x) + \frac{\rho}{2} \|x-z\|^2. 
\end{equation}
The alternating direction method of multiplier reads as follows: 
\begin{algorithm}
\caption{ADMMSkip}\label{alg:ADMMSkip}
Given a stepsize $\gamma > 0$, probability $p > 0$, initial iterate $x_0 = z_0 \in \Reals{d}$, initial control variate $h_0 \in \Reals{d}$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}
\For{$t=1,2,\cdots,T-1$}

\State{$x_{t+1} = {\rm arg}\min_{x} L_\rho(x,z_t,h_t)$} 

\State{Flip a coin $\theta_t \in \{0,1\}$ where ${\rm Prob}(\theta_t = 1) = p$}

\If{$\theta_t = 1$} 
    \State{$z_{t+1} = {\rm arg}\min_z L_\rho(x_t, z, h_t) = {\rm Prox}_{\rho \Psi}(x_{t+1})$} 
\Else
    \State{$z_{t+1} = x_{t+1}$}
    
\EndIf 

\State{$\textcolor{red}{h_{t+1} = \textcolor{blue}{h_t} + \frac{p}{\gamma}(z_{t+1} - x_{t+1})}$}
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{remark} 
For $p = 1$, we see that {\rm ADMMSkip} is {\rm ADMM}. 
\end{remark} 

\subsection{Linear Convergence of ProxSkip} 

We define 
\begin{equation}
x_* = {\rm arg}\min_x \left ( f(x) + \psi(x) \right ).
\end{equation}

For the convergence measure, we introduce the so-called Lyapunov function:  
\begin{equation} 
\Psi_t := \|x_t - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2.
\end{equation} 
We further define
\begin{subeqnarray}
w_t &=& x_t - \nabla f(x_t) \\ 
w_* &=& x_* - \nabla f(x_*). 
\end{subeqnarray}

Under these settings, we shall then show that ProxSkip generates iterates 
$\{x_t\}_{t = 1,\cdots}$ converges linearly in the sense that 
\begin{equation}
\mathbb{E}(\Psi_T) \leq (1 - \zeta)^T \Psi_0. 
\end{equation}
To achieve this theorem, we state a couple of assumptions on $f$ and $\psi$. 
\begin{assump}\label{ass1} 
$f$ is $L-$smooth and $r-$strongly convex. 
\end{assump}
By $L-$smoothness, we mean that 
\begin{equation}
\|\nabla f(x) - \nabla f(y) \| \leq L \|x - y\|, \quad \forall x, y \in \Reals{d}. 
\end{equation}
By $r-$strongly convex, we mean that 
\begin{equation}
f(y) \geq f(x) + \nabla f(x) ( y - x) +\frac{1}{2} r \|y - x\|^2, \quad \forall x, y \in \Reals{d}.  
\end{equation}
The regularizer $\psi$ satisfies
\begin{assump}\label{ass2} 
$\psi$ is proper, closed, and convex. 
\end{assump}
\begin{lemma} 
Under the above two Assumptions \ref{ass1} and \ref{ass2}, there exists a unique minimizer for the following problem: 
\begin{equation}
\min_{x} f(x) + \psi(x). 
\end{equation}
\end{lemma}
We also note that the Bregman divergence of a differentiable function $f: \Reals{d} \mapsto \Reals{}$ is defined by
\begin{equation} 
D_f(x,y) := f(x) - f(y) - \langle \nabla f(y), x - y \rangle. 
\end{equation} 
and the symmetrized Bregman divergence is given as 
\begin{equation} 
D_f(x,y) + D_f(y,x) = \langle \nabla f(x)  - \nabla f(y), x - y \rangle.  
\end{equation} 
For an $L-$smooth and $r-$strongly convex function, we have 
\begin{equation} 
\frac{r}{2} \|x - y\|^2 \leq D_f(x,y) \leq \frac{L}{2} \|x - y\|^2.
\end{equation} 
Furthermore, we have 
\begin{equation} 
\frac{1}{2L} \|\nabla f(x) - \nabla f(y)\|^2 \leq D_f(x,y) \leq \frac{1}{2r} \|\nabla f(X) - \nabla f(y)\|^2. 
\end{equation} 
%
%Given $\psi : \Reals{d} \mapsto \Reals{}$, we define $\psi^*(y) := \sup_{x \in \Reals{d}} \{ \langle x, y \rangle - \psi(x)\}$ to be its Fenchel conjugate.  
%
%The proximity operator of $\psi^*$ satisfies for any $\tau > 0$. 
%\begin{equation} 
%u = {\rm prox}_{\tau \psi^*} (y) \mbox{ implies } u \in y - \tau %\partial \psi^*(u). 
%\end{equation} 
We now present three technical lemmas, which will be needed to establish the main theorem. First of all, we shall need the following firm non-expansiveness of the proximity operator. The proof can be found at \cite{xxx}.  
\begin{lemma}
Let $\psi$ be proper, closed and convex and let $P(x):= {\rm prox}_{\frac{\gamma}{p}\psi}(x)$ and $Q(x) = x - P(x)$. Then, it holds that
\begin{equation}
\|P(x) - P(y)\|^2 + \|Q(x) - Q(y)\|^2 \leq \|x - y\|^2.    
\end{equation}
\end{lemma}

\begin{lemma}\label{3.3}
Under the assumptions \ref{ass1} and \ref{ass2}, for $\gamma > 0$ and $0 < p \leq 1$, we have 
\begin{equation} 
\mathbb{E}(\Psi_{t+1}) \leq \|w_t - w_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|h_t - h_*\|^2, 
\end{equation} 
where the expectation is taken over the $\theta_t$ in the Algorithm 1. 
\end{lemma} 
\begin{proof} 
We let $P(x) = {\rm prox}_{\frac{\gamma}{p}\psi}(x)$ and 
\begin{equation}
x := \widehat{x}_{t+1} - \frac{\gamma}{p} h_t \quad y = x_* - \frac{\gamma}{p} h_*. 
\end{equation}
The optimality conditin is that 
\begin{equation} 
x_* = P(y). 
\end{equation} 
The method reads as follows: 
\begin{equation}
x_{t+1} = \left \{ \begin{array}{ll} P(x) & \mbox{ with probability } p \\ \widehat{x}_{t+1} & \mbox{ with probability } 1 - p \end{array}  \right. 
\end{equation}
Furthermore, we have that 
\begin{equation}
h_{t+1} = h_t + \frac{p}{\gamma} (x_{t+1} - \widehat{x}_{t+1}) = \left \{ \begin{array}{ll} h_t + \frac{p}{\gamma} \left ( P(x) - \widehat{x}_{t+1} \right ) & \mbox{ with probability } p \\ h_t & \mbox{ with probability } 1 - p \end{array}  \right. 
\end{equation}
Now, we compute the expected value of the Lyapunov function 
\begin{equation}
\Psi_t := \|x_t - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 
\end{equation}
at the time step $t+1$, with respect to the coin toss at iteration $t$, which is 
\begin{eqnarray}
\mathbb{E}(\Psi_{t+1}) &=& p \left ( \|P(x) - x_*\|^2 + \frac{\gamma^2}{p^2} \left \|h_t + \frac{p}{\gamma} (P(x) - \widehat{x}_{t+1})  - h_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&=&   p \left ( \|P(x) - P(y) \|^2 + \left \| \frac{\gamma}{p} h_t + (P(x) - \widehat{x}_{t+1})  - \frac{\gamma}{p} h_* \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&=& p \left ( \|P(x) - P(y) \|^2 + \left \| P(x) - (\widehat{x}_{t+1} - \frac{\gamma}{p}h_t )  + ( y - x_*) \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&=& p \left ( \|P(x) - P(y) \|^2 + \left \| (P(x) - x) + ( y - P(y) ) \right \|^2 \right ) \\
&& + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ) \\
&\leq& p \left \|\left ( \widehat{x}_{t+1} - \frac{\gamma}{p} h_t \right ) - \left ( x_* - \frac{\gamma}{p} h_* \right ) \right \|^2 + (1-p) \left ( \|\widehat{x}_{t+1} - x_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2  \right ). 
\end{eqnarray}
A simple algebra shows that 
\begin{eqnarray}
\mathbb{E}(\Psi_{t+1}) &\leq& \|w_t - w_*\|^2 -\gamma^2\|h_t - h_*\|^2 + \frac{\gamma^2}{p^2} \|h_t - h_*\|^2. 
\end{eqnarray}
This completes the proof. 
\end{proof} 
We shall now need the last lemma: 
\begin{lemma}\label{3.4} 
Let Assumption \ref{ass1} hold with any $r \geq 0$. If $0 < \gamma \leq \frac{1}{L}$, then 
\begin{equation} 
\|w_t - w_*\|^2 \leq (1 - \gamma r) \|x_t - x_*\|^2. 
\end{equation} 
\end{lemma}
\begin{proof}
Recall the definition of $w_t$ and $w_*$ and thus, 
\begin{eqnarray}
\|w_t - w_*\|^2 &=& \|x_t - x_* - \gamma (\nabla f(x_t) - \nabla f(x_*))\|^2 \\
&=& \|x_t - x_*\|^2 + \gamma^2\|\nabla f(x_t) - \nabla f(x_*)\|^2 - 2\gamma \langle \nabla f(x_t) - \nabla f(x_*), x_t - x_* \rangle \\ 
&\leq& (1 - \gamma r) \|x_t - x_*\|^2  - 2\gamma D_f(x_t,x_*) + \gamma^2 \|\nabla f(x_t) - \nabla f(x_*)\|^2 \\ 
&=& (1 - \gamma r) \|x_t - x_*\|^2  - 2\gamma \left ( D_f(x_t,x_*) - \frac{\gamma}{2} \|\nabla f(x_t) - \nabla f(x_*)\|^2 \right ) \\ 
&\leq& (1 - \gamma r) \|x_t - x_*\|^2,  
\end{eqnarray}
where the last inequality is due to $0 \leq \gamma \leq 1/L.$ This completes the proof. 
\end{proof}
Using the above lemmas, we can conclude that 
\begin{theorem}
Let Assumptions \ref{ass1} and \ref{ass2} hold. Let $0 < \gamma \leq 1/L$ and 
$0 < p \leq 1$. Then the ProxSkip converges with the following property: 
\begin{equation} 
\mathbb{E}(\Psi_T) \leq (1 - \zeta)^T \Psi_0, 
\end{equation} 
where $\zeta:= \min\{\gamma r, p^2\}$. 
\end{theorem}
\begin{proof} 
By Lemma \ref{3.3} and Lemma \ref{3.4}, we have that 
\begin{eqnarray}
\mathbb{E}(\Psi_{t+1}) &\leq& \|w_t - w_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 \\ 
&\leq& (1 - \gamma r) \|x_t - x_*\|^2 + (1 - p^2) \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 \\
&\leq& (1 - \zeta) \left ( \|x_t - x_*\|^2 +  \frac{\gamma^2}{p^2} \|h_t - h_*\|^2 \right ) \\
&\leq& (1 - \zeta) \Psi_t. 
\end{eqnarray}
By the recurrence relation, we arrive at the conclusion. This completes the proof.  
\end{proof}

\newpage 

\section{Optimization of the average of $n-$functions} 

We consider to minimize the average of $n$ functions using a cluster of $n$ compute nodes
\begin{equation}
\min_{x \in \Reals{d}} \left \{ f(x) := \frac{1}{n} \sum_{i=1}^n f_i(x) \right \}, 
\end{equation}
where $f_i : \Reals{d} \mapsto \Reals{}$ and it is owned by and stored by client $i \in [n] := \{1,\cdots,n\}.$ This problem is currently the dominant paradigm for training supervised machine learning models. 

This problem can be formulated in a somewhat different form given as follows. By cloning the model $x$ into $n$ independent copies $x_1,\cdots,x_n$ and casting them into the consensus form, we have  
\begin{equation}
\min_{x_1,\cdots,x_n \in \Reals{d}} \frac{1}{n} \sum_{i=1}^n f_i(x_i) + \psi (x_1,\cdots,x_n), 
\end{equation}
where the regualizer $\psi : \Reals{nd} \mapsto \Reals{}$, 
\begin{equation}
\psi(x_1,\cdots,x_n) :=  \left \{ \begin{array}{cc} 
0, & \mbox{ if } x_1 = x_2 = \cdots = x_n \\
+\infty, & \mbox{ otherwise }. 
\end{array} \right . 
\end{equation}
The evaluation of the proximity operator of $\psi$ given by, is 
\begin{equation}
{\rm arg}\min_{y \in C} \|y - x\|. 
\end{equation}

\begin{equation}
C := \{(x_1,\cdots,x_n) \in \Reals{nd} : x_1 = x_2 = \cdots = x_n\}.     
\end{equation}
We note that with $\overline{x} = \frac{1}{n} \sum_{i=1} x_i,$
\begin{equation}
{\rm prox}_{\gamma \psi}(x_1, \cdots, x_n) = (\overline{x},\cdots,\overline{x}) \in \Reals{nd}. 
\end{equation}
Basically, we observe that the proximity operator is to achieve the consensus. 

%\subsection{Decentralized training} 
%Given a graph $G = (V,E)$ with nodes $V$ and edges $E$, we assume that every communication node $i$ receives a weighted average of its neighbors' vectors with weights $W_{i,1},\cdots,W_{in} \in [0,1]$. We also assume that nodes $i$ and $j$ communicate if and only if $W_{ij} \neq 0$. This is basically equivalent to say that $(i,j) \in E$. %
%Therefore, the weights $W_{ij}$ define the mixing matrix $\tenq[2]{W}$. 
%
%The optimization problem with decentralized communication %can be cast into 
%\begin{equation} 
%\min_{x \in \Reals{d}} f(x) \quad \mbox{ subject to } %(\tenq[2]{I} - \tenq[2]{W}) x = 0. 
%\end{equation} 
%We can reformulate it into 
%\begin{equation} 
%\min_{x \in \Reals{d}} f(x) + \psi(Lx),  \quad \mbox{ %subject to } (\tenq[2]{I} - \tenq[2]{W}) x = 0. 
%\end{equation} 
%where 
\subsection{Federated Learning (FL) : } 

Federated learning is basically an optimization for some functional $f(x)$. However, data are assigned to each client and using such data set can enhance the accuracy of the constructed parameters. Due to the use of different data sets for any client, all the local functions $f_i$ are typically different. Therefore, the local steps can introduce a drift in the updates of each client. This leads to convergence issues. The main task for the FL community is to propose algorithm that could mitigate the client drift issue. 
 
The ProxSkip applied to FL can be summarized as follows: 
\begin{algorithm}
\caption{Scaffnew}\label{alg:mainscaff}
Given a stepsize $\gamma > 0$, probability $p > 0$, initial iterate $\{x_{i0}\}_{i=1,\cdots,n} \in \Reals{nd}$, initial control variate $\{h_{i0}\}_{i=1,\cdots,n} \in \Reals{nd}$ such that $\sum_{i=1}^n h_{i0} = 0$, number of iterations $T \geq 1$, we perform the following:  
\begin{algorithmic}

\State{Server: flip a coin, $\theta_t \in \{0,1\}$, $T$ times, where ${\rm Prob}(\theta_t = 1) = p$ and send the sequence $\theta_0, \cdots, \theta_{T-1}$ to all workers} 

\For{$t=1,2,\cdots,T-1$, \mbox{ \textbf{in parallel, all workers $i \in [n]$}} }

\State{$\widehat{x}_{i,t+1} = x_{i,t} - \gamma (\nabla f_i(x_{i,t}) \textcolor{red}{- h_{i,t}})$} 

\If{$\theta_t = 1$} 
    \State{$x_{i,t+1} = {\rm prox}_{\frac{\gamma}{p}} \psi 
\left ( \widehat{x}_{i,t+1} \textcolor{red}{- \frac{\gamma}{p} h_{i,t}} \right ) = \frac{1}{n} \sum_{j=1}^n \widehat{x}_{j,t+1}$} 
\Else
    \State{$x_{i,t+1} = \widehat{x}_{i,t+1}$}
    
\EndIf 

\State{$\textcolor{red}{h_{i,t+1} = \textcolor{blue}{h_{i,t}} + \frac{p}{\gamma}(x_{i,t+1} - \widehat{x}_{i,t+1})} = \textcolor{blue}{h_{i,t}} + \frac{p}{\gamma}(\sum_{j=1}^n \widehat{x}_{j,t+1}- \widehat{x}_{i,t+1}) $}
\EndFor
\end{algorithmic}
\end{algorithm}
\begin{remark}
I consider that we may remove $h_{i,t}$ for the update of $h_{i,t+1}$. 
\end{remark}

We can mimick the convergence proof of ProxSkip algorithm to achieve the convergence of the Scaffnew algorithm. We now provide an assumption for each $f_i$ for all $i=1:n$ as follows: 
\begin{assump}\label{scaffnewass1} 
Each $f_i$ is $L-$smooth and $r-$strictly convex. 
\end{assump}
\begin{corollary}[Federated Learning]
Let Assumption \ref{scaffnewass1} hold and let $\gamma = 1/L$, $p = 1/\sqrt{\kappa}$, and $g_{i,t}(x_{i,t}) = \nabla f_i(x_{i,t})$. Then the iteration complexity of Scaffnew is $O(\kappa \log 1/\epsilon)$ and its communication complexity is $O(\sqrt{\kappa} \log 1/\epsilon)$.   
\end{corollary}

\newpage 

\section{Interpretation of Scaffnew Algorithm} 
We consider the total of $n-$particle whose location is given as 
\begin{equation}
\partial_t \tenq[2]{x} = \tenq[2]{v}, 
\end{equation}
We consider the following time dependent system of $n$ equations:  
\begin{equation} 
\partial_t \tenq[2]{v} = \Delta_p \tenq[2]{v} + \tenq[2]{h}(\tenq[2]{v}), 
\end{equation} 
subject to $\tenq[2]{v}(x,0) = \tenq[2]{v_0}$, $\tenq[2]{x_0} = \tenq[2]{x_0}$, where 
\begin{equation}
\tenq[2]{h}(\tenq[2]{s}) = \left ( \begin{array}{c} 
(\overline{s} - s_1 ) |\overline{s} - s_1|^{q-2} \\ (\overline{s} - s_2 ) |\overline{s} - s_2|^{q-2} \\ \vdots \\ ( \overline{s} - s_n ) |\overline{s} - s_n|^{q-2} \end{array} \right )
\end{equation} 
% |\tenq{s}|^{q-2}$
In a recent work, it was proved that 
\begin{theorem}
If $q = 2$, then the exponential decay to consensus. If $1 < q < 2$, then the finite time arrival to consensus, depending on the initial condition.  
\end{theorem}
I am currently working to extend it to replace $p-$Laplacian to obtain similar results. If we view the GD as the time integration to reach the steady state solution to $-\nabla f(v) + h(v) = 0$ or the time evolution or Jacobi method to solve the time dependent problem: 
\begin{equation}
\partial_t v = -\nabla f(v) + h(v), 
\end{equation}
as the iteration is stated as 
\begin{equation}
v_{t+1} = v_t + \gamma(-\nabla f(v_t) + h_t). 
\end{equation}
Now, we observe the time discrete form of $h_t$ given as follows: 
\begin{equation}
h_{t+1} - h_t = \frac{p}{\gamma}(\overline{v} - v). 
\end{equation}
This can be interpreted as 
\begin{equation}
\partial_t h = \overline{v} - v. 
\end{equation}
If we view $f(v) = \frac{1}{p} \int_\Omega |\nabla v|^p \, dx$, then we have that 
\begin{equation}
-\nabla f(v) = \Delta_p v.     
\end{equation}
Therefore, 
\begin{equation}
h_t = \frac{p}{\gamma} \int_0^t \overline{v} - v \, dt.  
\end{equation}
This can be interpreted as to the iterate we see in Algorithm 1, ProxSkip. 
This analoguous relation can be used to understand why $h$ is introduced to shift the gradient of $f$. 
\begin{remark}
We can speed up further the consensus, then the communication can be lowered.
\end{remark}

\section{Proposed Projects} 

In this section, we shall discuss two main subtasks. 



\subsection{Convergence analysis of Scaffnew-type methods in a framework of composite optimization} 

The theory of Scaffnew was originally developed for strongly convex problems \cite{mishchenko2022proxskip}. Our first target goal is, therefore, to extend such a theory to general convex loss functions. We will then proceed to extend Scaffnew algorithm to non-convex loss functions and perform numerical investigation for deep learning problems such as convolutional neural networks. 

As discussed in \S \ref{intro}, Scaffnew~\cite{mishchenko2022proxskip} constructs the modified, but equivalent formulation \cref{cp} of \cref{FL} as a block-separable structure of~\cref{FL} via introducing auxiliary variables. In \cref{FL}, each $f_k$ possesses its own parameter $x_k$ and the equalities among $x_k$'s are imposed by the constraint function $\psi$. This is in fact a very special case of a large class of mathematical optimization problems called composite optimization~\cite{Nesterov:2013}:
\begin{equation}
    \label{composite}
    \min_{x \in X} \left\{ F(x) + G(x) \right\},
\end{equation}
where $X$ is a solution space, $F \colon X \rightarrow \mathbb{R}$ is a smooth function, and $G \colon X \rightarrow \overline{\mathbb{R}}$ is a proper, convex, and lower semicontinuous function that is possibly nonsmooth.
Indeed, setting
\begin{equation*}
    x = (x_1, \dots, x_N), \quad
    F(x) = \sum_{k} f_k (x_k), \quad
    G(x) = \chi (x_1, \dots, x_N)
\end{equation*}
in~\cref{composite} yields~\cref{cp}. In the past decade, there have been numerous literature on efficient numerical solvers for composite optimization~\cref{cp}; see~\cite{CP:2016,Nesterov:2013,Teboulle:2018} and references cited therein. In particular, there have been several notable results on efficient subspace correction methods for composite optimization~\cite{Park:2021,Park:2022}. Exploiting the composite optimization structure of~\cref{cp}, we design an efficient subspace correction solver for~\cref{cp} that can effectively reduce the amount of communications. In~\cref{cp}, the constraint function $\psi$ was used to enforce the equalities among $x_k$'s. Meanwhile, there are various mathematical tools to synchronize the parameters in the clients, such as penalty methods and/or Lagrange multipliers. Using these tools, numerous variational problems equivalent to~\cref{FL} or relaxed version of models, which might be more relevant can be generated. Each of these variational problems has its own properties and there lies a possibility of designing efficient numerical solvers based on such properties. For example, in~\cite{GM:2012}, a multiple splitting algorithm to solve problems of the form~\cref{FL} was proposed based on an equivalent variational problem
\begin{equation*}
    \min_{\substack{x_k \in X,\\ 1 \leq k \leq N}} \sum_k f_k (x_k) \quad
    \textrm{ subject to } x_k = x_{k+1}, \textrm{ } 1 \leq k \leq N-1.
\end{equation*} 
By carefully studying the properties of variational formulations, we will develop efficient training algorithms or build a new shift operator in a framework of subspace correction strategies~\cite{Park:2020,Park:2021,Park:2022,Xu:1992,LWXZ:2007,LWXZ:2008}. On a separate issue, objective functionals that arise in FL are semi-coersive or nearly semi-coersive convex, \cite{lee2009robust,LWXZ:2007,hong2016uniformly,hong2016robust,chen2020robust}. In particular, Xu, Lee, and Park have collaborative research topics in progress on designing efficient subspace correction methods for various convex optimization problems~\cite{LPX:un2,LPX:un1}. These works are critical to design fast optimization algorithms for machine learning, such as logistic equation and they can be applied directly for FL.  

\subsection{Design and analysis of improved FL models using duality} 

%Our second proposed project is to apply the method of subspace correction for numerical solutions of partial differential equations \cite{Xu:1992}. 
A challenging aspect of solving~\cref{FL} is that every local objective function $f_k$ shares the same parameter $x$. Since there does not exist a choice for $x$ that minimizes every $f_k$ simultaneously in general, it is not very straightforward to design a numerical solver for~\cref{FL} that deals with each $f_k$ in parallel. On the other hand, we can consider various variational formulations equivalent to~\cref{FL} by using well-established tools in mathematical optimization, suitable for applying the idea of subspace correction, and design efficient subspace correction-based training algorithm for FL. Namely, FL can be connected to the framework of subspace correction method, based on the dual formulation of the objective functional \cite{zhang2002dual}. 

We assume that each $f_k$ is convex and that $E$ is strongly convex with parameter $r > 0$. Reallocating the strongly convex term in $E$ appropriately,~\cref{FL} can be rewritten as follows:
\begin{equation}\label{primal}
\min_{x \in X} \left\{ E(x) = \sum_k \tilde{f}_k (x) + \frac{r}{2} \|x\|^2 \right\},
\end{equation}
where each $\tilde{f}_k \colon X \rightarrow \mathbb{R}$, $1 \leq k \leq N$, is a convex function. Invoking Fenchel-Rockafellar duality~\cite{CP:2016}, one can obtain the following dual formulation of~\cref{primal}:
\begin{equation}\label{dual}
\min_{\substack{y_k \in X, \\ 1\leq k\leq N}} \left\{ \sum_k \tilde{f}_k^* (y_k) + \frac{1}{2r} \left\| \sum_k y_k \right\|^2 \right\},
\end{equation}
where $\tilde{f}_k^*$ is the convex conjugate of $\tilde{f}_k$ defined by
\begin{equation*}
    \tilde{f}_k^* (y) = \sup_{x \in X} \left\{ \left< y, x \right> - \tilde{f}_k (x) \right\}.
\end{equation*}
For example, if $\tilde{f}_k$ is given by the logistic loss function, $\tilde{f}_k (x) = \log (1 + e^{ax})$, then we have $\tilde{f}_k^* (y) = (y/a) \log (y/a) + (1 - y/a) \log (1 - y/a)$. We note that the cost function of~\cref{dual} has a block-separable structure. Therefore, it is possible to apply block coordinate descent methods~\cite{CP:2015}, or more generally, subspace correction methods. The convergence of subspace correction is well-known to generally accelerate when the local correction can be done in a more accurate manner unlike the Local DG, based on the operator splitting techniques. Such a formulation has been applied in limited cases in machine learning, such as SVM \cite{hsieh2008dual} and LR \cite{yu2011dual}. 

%
%The general guideline we propose is to consider the dual formulation of the objective functional \cite{zhang2002dual} via Fenchel-Rockafella Duality\cite{nachum2020reinforcement}. The advantage of this approach is that the unknown is not the parameter space, rather its dual variable is the unknown, which resides in the data points and thus, the large scale subspace correction framework can be naturally applied within FL framework. Furthermore, the convergence of subspace correction is well-known to generally accelerate when the local correction can be done in a more accurate manner unlike the Local DG which is based on the operator splitting techniques. Such a formulation has been applied in limited cases in machine learning, such as SVM \cite{hsieh2008dual} and LR \cite{yu2011dual}. In our approach, we first present various variational formulations that are equivalent to the original FL formulation such as the one considered in \cite{mishchenko2022proxskip} or modified FL models. By carefully studying the properties of variational formulations, we will develop efficient training algorithms or new shift operator, using subspace correction strategies in both the data space and the parameter space~\cite{Park:2020,Park:2021,Park:2022,Xu:1992,LWXZ:2007,LWXZ:2008}. Lastly, we note that generally, objective functionals that arise in FL are semi-coersive or nearly semi-coersive convex, \cite{lee2009robust,LWXZ:2007,hong2016uniformly,hong2016robust,chen2020robust}. The current ongoing collaborative effort in designing efficient subspace correction methods for various convex optimization problems \cite{LPX:un2,LPX:un1} will be applied to understand these open issues. These works are critical to design fast optimization algorithms for machine learning, such as logistic equation. %Xu is well-known for his studies in developing, designing, and analyzing fast methods for the solution of large-scale systems of equations, which include the BPX-preconditioner \cite{bramble1990parallel} and the HX-preconditioner \cite{hiptmair2007nodal}. Among others, his work on subspace correction methods in his SIAM Review paper \cite{xu1992iterative} is mostly relevant to the current proposal.


%The current major research effort on FL is devoted to the following three issues; (1) Convergence study of {\textit{FebAvg}} or its variants, (2) Improvement of Communication speeds between local and global servers, and (3) Development of models that take into account heterogeneity or unbalanced and non-i.i.d. data. Thus far, there have been a number of convergence theories that justify the use of {\textit{FebAvg}} as presented in \cite{li2019convergence,zhou2022convergence,haddadpour2019convergence,mitra2021linear}. Clearly, the main motivation of the FL is to encourage the local computations by each worker. Such local update for global model improvement has to be transferred to the server and thus algorithm developments are more or less motivated to improve communications between local and global server. 

%The most recent algorithm called ProxSkip, is shown to achieve the best communication efficiency until today \cite{mishchenko2022proxskip}. Figure \ref{xxx} shows the algorithm. Basically, this introduces proximal gradient with specially designed constraint operator for averaging or communication step. 

%This is randomized method, which apply the global communication with some probability. The communication rounds is not too much needed for the convergence. 

%The main idea of ProxSkip is to use the communication between local and global communication only once in a while, still achieving the convergence. However, we note that all the convergence theory rely on the assumption that the functional is strictly convex. Therefore, the state of the art theory lacks in handling realistic objective functionals, i.e., convex functional or non-convex functional \cite{haddadpour2019convergence}.
%
%
%However, we note that all the convergence theory rely on the assumption that the functional is strictly convex. Therefore, the state of the art theory lacks in handling realistic objective functionals, i.e., convex functional or non-convex functional \cite{haddadpour2019convergence}.




%We have expertises in subspace correction methods, randomized solvers and convex optimization.

%
%Specifically, the following two tasks will be accomplished in this project. %Michalis Kalitsis (Merit Network, Inc at Univ of Michigan)'s expertise is at algorithm design, ML, and security and privacy, especially for IoT applications. Prasenjit Mitra (Penn State)'s interest lies in AI, ML on Web and Social media, Hadi Hosseini (Penn State)'s research expertise is AI and Multiagent Systems.
 
%{\textbf{In this proposal, we shall establish the convergence for {\textit{FebAvg}} only under $F$ being convex, which is an open issue, and also suggest strategies to design efficient numerical solvers for~\cref{FL}, which include sampling and averaging techniques. We shall also present an improved model that can adequately deal with unbalanced and non-i.i.d. clients' data.}}

%\begin{description}
%\item[Task 1:] To design client-level parallel subspace correction methods for Federated Learning by identifying block-separable structures of the problem.
%\item[Task 2:] To design efficient numerical methods for FL via constructing equivalent %reformulation or relaxed variational formulations of FL. % which are derived by using the %optimization theory 
%\end{description}
 

%Specifically, the following two tasks will be accomplished in this project. %Michalis Kalitsis (Merit Network, Inc at Univ of Michigan)'s expertise is at algorithm design, ML, and security and privacy, especially for IoT applications. Prasenjit Mitra (Penn State)'s interest lies in AI, ML on Web and Social media, Hadi Hosseini (Penn State)'s research expertise is AI and Multiagent Systems.
 
%{\textbf{In this proposal, we shall establish the convergence for {\textit{FebAvg}} only under $F$ being convex, which is an open issue, and also suggest strategies to design efficient numerical solvers for~\cref{FL}, which include sampling and averaging techniques. We shall also present an improved model that can adequately deal with unbalanced and non-i.i.d. clients' data.}}

%\begin{description}
%\item[Task 1:] To design client-level parallel subspace correction methods for Federated Learning by identifying block-separable structures of the problem.
%\item[Task 2:] To design efficient numerical methods for FL via constructing equivalent reformulation or relaxed variational formulations of FL. % which are derived by using the optimization theory 
%\end{description}

%\section{Proposed Projects}
%In this section, we present some description of the proposed projects. 
\subsection{Subspace correction methods}
Subspace correction methods are prominent numerical solvers for large-scale problems because they can efficiently utilize massively parallel computer architectures. More generally, various iterative methods such as multigrid methods and domain decomposition methods can be interpreted as subspace correction methods~\cite{Xu:1992}. Algebraic multigrid methods~\cite{XZ:2017} provides a general framework to design a multilevel training algorithm without the limitation of geometric grids. There have been many important developments on subspace correction methods and algebraic multigrid methods for linear and nonlinear problems, but nonlinear preconditioning is much less explored than linear preconditioning. In the context of domain decomposition, a seminal contribution for nonlinear preconditioning was made by~\cite{CKY:2001}, namely the Additive Schwarz Preconditioned Inexact Newton method~(ASPIN), see also~\cite{CK:2002}. The idea is: “The nonlinear system is transformed into a new nonlinear system, which has the same solution as the original system. For certain applications the nonlinearities of the new function are more balanced and, as a result, the inexact Newton method converges more rapidly.” For solving nonlinear equations and optimization problems, a good nonlinear preconditioner can sometimes drastically improve the robustness of the nonlinear convergence by reducing the impact of certain parameters. 
Recently, some acceleration schemes that can be applied to subspace correction methods for general convex optimization problems were proposed~\cite{Park:2021,Park:2022}. We also mention that the speed up has been observed with specially designed randomized choices for mini batch \cite{li2019convergence,zhou2022convergence}, which is expected to be analyzed and understood in the framework of randomized subspace correction method studied in \cite{hu2019randomized} as well. We expect that novel training algorithms for FL with improved efficiency in the senses of both communication and computation can be designed using subspace correction strategies explained above. We propose subspace correction-based training algorithm for FL that can can significantly reduce the amount of communications between the server and clients by reducing the number of required epochs.

% Section: Variational formulations for federated learning
\subsection{Variational formulations for federated learning}
A challenging aspect of solving~\cref{FL} is that every local objective function $f_k$ shares the same parameter $x$. Since there does not exist a choice for $x$ that minimizes every $f_k$ simultaneously in general, it is not very straightforward to design a numerical solver for~\cref{FL} that deals with each $f_k$ in parallel.

Meanwhile, various variational formulations equivalent to~\cref{FL} can be obtained by using well-established tools in mathematical optimization. We find variational formulations equivalent to~\cref{FL} that are suitable for applying the idea of subspace correction, and design efficient subspace correction-based training algorithm for FL.

A possible formulation to apply subspace correction strategies is a dual formulation of~\cref{FL}, which is more suitable for designing a client-wise parallel algorithm than the original formulation~\cref{FL}. We assume that each $f_k$ is convex and that $E$ is strongly convex with parameter $r > 0$. Reallocating the strongly convex term in $E$ appropriately,~\cref{FL} can be rewritten as follows:
\begin{equation}
    \label{primal}
    \min_{x \in X} \left\{ E(x) = \sum_k \tilde{f}_k (x) + \frac{r}{2} \|x\|^2 \right\},
\end{equation}
where each $\tilde{f}_k \colon X \rightarrow \mathbb{R}$, $1 \leq k \leq N$, is a convex function. Invoking Fenchel--Rockafellar duality~\cite{CP:2016}, one can obtain the following dual formulation of~\cref{primal}:
\begin{equation}
    \label{dual}
    \min_{\substack{y_k \in X, \\ 1\leq k\leq N}} \left\{ \sum_k \tilde{f}_k^* (y_k) + \frac{1}{2r} \left\| \sum_k y_k \right\|^2 \right\},
\end{equation}
where $\tilde{f}_k^*$ is the convex conjugate of $\tilde{f}_k$ defined by
\begin{equation*}
    \tilde{f}_k^* (y) = \sup_{x \in X} \left\{ \left< y, x \right> - \tilde{f}_k (x) \right\}.
\end{equation*}
For example, if $\tilde{f}_k$ is given by the logistic loss $\tilde{f}_k (x) = \log ( 1 + e^{ax})$, then we have $\tilde{f}_k^* (y) = (y/a) \log (y/a) + (1 - y/a) \log (1 - y/a)$.
As the cost function of~\cref{dual} has a block-separable structure, it is suitable to apply block coordinate descent methods~\cite{CP:2015}, or more generally, subspace correction methods.

An alternative way to create a block-separable structure in~\cref{FL} is to introduce some auxiliary variable. More precisely, we introduce $N$ copies $x_1$, \dots, $x_N$ of $x$ that play roles of the parameters of $f_1$, \dots, $f_N$, respectively, as follows:
\begin{equation}
    \label{constrained}
    \min_{\substack{x_k \in X,\\1\leq k\leq N}} \left\{ \sum_k f_k (x_k) + \chi (x_1, \dots, x_N) \right\},
\end{equation}
where
\begin{equation*}
    \chi(x_1, \dots, x_N) = \begin{cases}
    0, & \textrm{ if } x_1 = \dots = x_N, \\
    \infty, & \textrm{ otherwise.}
    \end{cases}
\end{equation*}
That is, each $f_k$ possesses its own parameter $x_k$ and the equalities among $x_k$'s are imposed by the constraint function $\chi$. The formulation~\cref{constrained} was considered in several existing works, e.g., ProxSkip~\cite{mishchenko2022proxskip}. Here, we focus on the fact that~\cref{constrained} is an instance of a large class of mathematical optimization problems called composite optimization~\cite{Nesterov:2013}:
\begin{equation}
    \label{composite}
    \min_{x \in X} \left\{ F(x) + G(x) \right\},
\end{equation}
where $X$ is a solution space, $F \colon X \rightarrow \mathbb{R}$ is a smooth function, and $G \colon X \rightarrow \overline{\mathbb{R}}$ is a proper, convex, and lower semicontinuous function that is possibly nonsmooth.
Indeed, setting
\begin{equation*}
    x = (x_1, \dots, x_N), \quad
    F(x) = \sum_k f_k (x_k), \quad
    G(x) = \chi (x_1, \dots, x_N)
\end{equation*}
in~\cref{composite} yields~\cref{constrained}. In the past decade, there have been numerous literature on efficient numerical solvers for composite optimization~\cref{composite}; see~\cite{CP:2016,Nesterov:2013,Teboulle:2018} and references therein. In particular, there have been several notable results on efficient subspace correction methods for composite optimization~\cite{Park:2021,Park:2022}. Exploiting the composite optimization structure of~\cref{constrained}, we design an efficient subspace correction solver for~\cref{constrained} that can effectively reduce the amount of communications. In~\cref{constrained}, the constraint function $\chi$ was used to enforce the equalities among $x_k$'s. Meanwhile, there are various mathematical tools to synchronize the parameters in the clients, such as penalty methods and Lagrange multipliers. Using these tools, numerous variational problems equivalent to~\cref{FL} or relaxed version of models, which might be more relevant can be generated. Each of these variational problems has its own properties and there lies a possibility of designing efficient numerical solvers based on such properties. For example, in~\cite{GM:2012}, a multiple splitting algorithm to solve problems of the form~\cref{FL} was proposed based on an equivalent variational problem
\begin{equation*}
    \min_{\substack{x_k \in X,\\ 1 \leq k \leq N}} \sum_k f_k (x_k) \quad
    \textrm{ subject to } x_k = x_{k+1}, \textrm{ } 1 \leq k \leq N-1.
\end{equation*}
We shall investigate various variational models equivalent to the original formulation~\cref{FL} for FL, and then try to find efficient numerical strategies to solve the models. 

\section{The update of the Lagrange multiplier} 
We consider to solve
\begin{equation} 
\min_{\{x \in \Reals{n} : Ax = b\}} f(x) 
\end{equation} 
This problem can be formulated as a saddle problem as 
\begin{equation}
\max_u \min_{x} \left \{ f(x) + u^T (Ax - b) \right \}
\end{equation}
We define $g(u)$ by 
\begin{equation} 
g(u) = \min_x \left \{ f(x) + u^T (Ax - b) \right \}. 
\end{equation} 
We recall that the conjugate of $f$ denoted by $f^*$ is defined as follows: 
\begin{equation}
f^*(y) := \max_{x} y^T x - f(x). 
\end{equation}
Therefore, we see that 
\begin{eqnarray*} 
g(u) &=& - \max_x \left \{ -f(x) - u^T (Ax - b)  \right \} \\ 
&=&  - \max_x \left \{ -f(x) - (A^T u)^T x + u^Tb \right \} \\
&=&  - \max_x \left \{ - (A^T u)^T x - f(x) \right \} - u^Tb = - f^*(-A^Tu) - u^Tb. 
\end{eqnarray*} 
The dual variable is then to satisfy the following maximum optimization: 
\begin{equation} 
\max_u g(u). 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
u_k = u_{k-1} + t_k \partial g(u_{k-1}), 
\end{equation} 
where $t_k \geq 0$. We note that $u_{k-1}$ produces $x_k$ as the minimizer given as follows: 
\begin{equation} 
x_k = {\rm arg}\min_x \left ( f(x) + u_{k-1}^T (Ax - b)\right ) 
\end{equation} 
We recall the well-known fact that if $f$ is closed and convex, then 
\begin{equation}
y \in \partial f(x) \quad \Leftrightarrow \quad x \in \partial f^*(y) \quad \Leftrightarrow \quad x \in {\rm arg}\min_z f(z) - y^T z.
\end{equation}
Using this fact, we shall show that $\partial g(u_{k-1}) = Ax_k - b$. First, we note that 
\begin{equation}
\partial g(u) = A \partial f^*(-A^T u) - b. 
\end{equation}
Therefore,  if $x \in \partial f^*(-A^Tu)$, then $x = {\rm arg}\min_z f(z) - (-A^Tu)^T z$. Namely, we have that 
\begin{equation}
{\rm arg}\min_z f(z) - (-A^Tu)^T z = f(x) + u^T A x. 
\end{equation}
Thus, we have that
\begin{equation}
\partial g(u_{k-1}) = A \partial f^*(-A^T u_{k-1}) - b = Ax_{k} - b.  
\end{equation}

\subsection{Augmented Lagrangian Method} 

We note that the choice of the step size $t_k$ in the ADMM is unclear. To remedy this issue, we consider the augmented Lagrangian method. This reads as follows: 
\begin{equation} 
\min_{\{x \in \Reals{n} : Ax = b\}} f(x) + \frac{\rho}{2} \|Ax - b\|^2. 
\end{equation} 
This problem can be formulated as a saddle problem as 
\begin{equation}
\max_u \min_{x} \left \{ f(x) + u^T (Ax - b) + \frac{\rho}{2} \|Ax - b\|^2 \right \}
\end{equation}
We define $g(u)$ by 
\begin{equation} 
g(u) = \min_x \left \{ f(x) + u^T (Ax - b) + \frac{\rho}{2} \|Ax - b\|^2 \right \}. 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
u_k = u_{k-1} + \rho \partial g(u_{k-1}), 
\end{equation} 
where $t_k = \rho$ is chosen. The reason why it makes sense is that 
\begin{equation} 
x_k = {\rm arg}\min_x \left ( f(x) + u_{k-1}^T (Ax - b) + \frac{\rho}{2} \|Ax - b\|^2 \right ). 
\end{equation} 
Namely, we have that
\begin{equation} 
\partial f(x_k) + A^T(u_{k-1} + \rho (A x_k - b)) = 0. 
\end{equation} 
This is the stationary condition for original primal problem if $u_k = u_{k-1} + \rho (A x_k - b)$.   



\section{The update of the Lagrange multiplier for the federated learning algorithm} 

We denote the vector $X \in \mathbb{R}^{nd}$ as follows: 
\begin{equation}
X = \left ( \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_d \end{array} \right ) \in \mathbb{R}^{nd}, \quad \mbox{ where } x_j \in \mathbb{R}^{n}, \mbox{ and } \forall i=1,\cdots,d. 
\end{equation}
We now introduce a matrix $K \in \mathbb{R}^{d} \rightarrow \mathbb{R}^{nd \times d}$ defined by the following relation: 
\begin{equation} 
K = \left ( \begin{array}{c} I_d \\ I_d \\ \vdots \\ I_d \end{array} \right ), 
\end{equation}
where $I_d \in \mathbb{R}^{d\times d}$ is the identity matrix. Therefore, we see that for $z \in \mathbb{R}^{d}$, we have 
\begin{equation} 
K z = \left ( \begin{array}{c} z \\ z \\ \vdots \\ z \end{array} \right ) \in \mathbb{R}^{nd}. 
\end{equation} 
With $V = \mathbb{R}^{nd} \times \mathbb{R}^d$, we now consider to solve 
\begin{equation} 
\min_{X \in V } F(X) + H^T(Kz - X) + \frac{r}{2} \|Kz - X\|^2. 
\end{equation} 
This problem can be formulated as a saddle problem as follows: 
\begin{equation}\label{main:eq} 
\max_{H \in \mathbb{R}^{nd}}  \min_{X \in V} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}
\end{equation}
We define $g(H)$ as follows: 
\begin{equation}\label{gfunction}
g(H) = \min_{X} \left \{ F(X) + H^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2 \right \}. 
\end{equation}
We shall now show how the Lagrange multiplier should be updated in the following Lemma. 
\begin{lemma}
Let $F$ be closed and convex and for a fixed $H_{k-1}$ and $z = z_k$, let 
\begin{equation} 
X_k = {\rm arg}\min_{X} ( F(X) + H_{k-1}^T (Kz - X) + \frac{r}{2} \|Kz - X\|^2).  
\end{equation} 
Then, it holds that 
\begin{equation} 
\partial g(H_{k-1}) = Kz_k - X_k,  
\end{equation}
where $g$ is defined in \eqref{gfunction}. 
Furthermore, we have that
\begin{equation} 
\partial F(X_k) - (H_{k-1} + r (K z_k - X_k)) = 0. 
\end{equation} 
Therefore, the Lagrange multiplier has to be updated via the following formular: 
\begin{equation}
H_k = H_{k-1} + r (Kz_k - X_k). 
\end{equation}
\end{lemma}
\begin{proof} 
We define $G(X) = F(x) + \frac{r}{2} \|Kz - X\|^2$ and recall that the conjugate of $G$ denoted by $G^*$ is defined as follows: 
\begin{equation}
G^*(Y) := \max_{X} Y^T X - G(X). 
\end{equation}
Therefore, we see that 
\begin{eqnarray*} 
g(H) &=& -\max_X \left \{ -G(X) - H^T (Kz - X)  \right \} \\ 
&=&  -\max_X \left \{ -G(X) - (K^T H)^T z + H^TX \right \} \\
&=&  - \max_X \left \{ H^T X - G(X) \right \} + H^T K z \\
&=& - G^*(H) + H^TKz. 
\end{eqnarray*} 
The dual variable is then to satisfy the following maximum optimization: 
\begin{equation} 
\max_H g(H). 
\end{equation} 
The gradient ascent method is then given by 
\begin{equation} 
H_k = H_{k-1} + t_k \partial g(H_{k-1}), 
\end{equation} 
where $t_k \geq 0$. We note that $H_{k-1}$ produces $X_k$ as the minimizer given as follows: 
\begin{equation} 
X_k = {\rm arg}\min_Y \left ( G(Y) + H_{k-1}^T (K z_k - X) \right ).  
\end{equation} 
We now recall the well-known fact that if $G$ is closed and convex, then 
\begin{equation}
Y \in \partial G(X) \quad \Leftrightarrow \quad X \in \partial G^*(Y) \quad \Leftrightarrow \quad X \in {\rm arg}\min_Z G(Z) - Y^T Z.
\end{equation}
Using this fact, we shall show that $\partial g(H_{k-1}) = Kz_k - X_k$. First, we note that 
\begin{equation}
\partial g(H) = \partial G^*(H) + Kz. 
\end{equation}
Therefore,  if $X \in \partial G^*(H)$, then $X = {\rm arg}\min_Z G(Z) - H^TZ$. Namely, we have that 
\begin{eqnarray*}
H_{k-1}^T Kz_k + {\rm arg}\min_Z \left \{ G(Z) + H_{k-1}^T Z \right \} &=& G(X_k) + H_{k-1}^T (Kz_k - X_k) \\
&=& F(X_k) + (H_{k-1}^T (Kz_k - X_k) + \frac{r}{2}\|Kz_k - X_k\|^2. 
\end{eqnarray*}
Thus, we have that
\begin{equation}
\partial g(H_{k-1}) = Kz_k - X_k. 
\end{equation}
This completes the proof. 
\end{proof} 



\bibliographystyle{plain}
\bibliography{mybib}

\end{document} 

